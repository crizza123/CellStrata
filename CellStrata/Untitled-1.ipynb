{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24184ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellxgene_census as cc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class adata:\n",
    "    def__init__(self, X=None, obs=None, var=None, uns=None):\n",
    "        self.X = X\n",
    "        self.obs = obs\n",
    "        self.var = var\n",
    "        self.uns = uns  = uns if uns is not None else {}\n",
    "\n",
    "\n",
    "function cellxgene_query(dataset_id, filters=None):\n",
    "\n",
    "    # Simulated function to query cellxgene census datasets\n",
    "\n",
    "    return adata()\n",
    "\n",
    "# Load dataset\n",
    "dataset_id = \"example_dataset_id\"\n",
    "adata = cc.load_dataset(dataset_id)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(adata)\n",
    "print(f\"Number of cells: {adata.X.shape[0]}\")\n",
    "print(f\"Number of genes: {adata.X.shape[1]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6150c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Failed to open local file 'census_hsapiens_healthy_adult_obs.parquet'. Detail: [errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, tbl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(read_iter):\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         writer = \u001b[43mpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     writer.write_table(tbl)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:1076\u001b[39m, in \u001b[36mParquetWriter.__init__\u001b[39m\u001b[34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, write_time_adjusted_to_utc, max_rows_per_page, **options)\u001b[39m\n\u001b[32m   1071\u001b[39m filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1073\u001b[39m     \u001b[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m     sink = \u001b[38;5;28mself\u001b[39m.file_handle = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_output_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1079\u001b[39m     sink = where\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/_fs.pyx:913\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_output_stream\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Failed to open local file 'census_hsapiens_healthy_adult_obs.parquet'. Detail: [errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cellxgene_census as cxc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "OUT = Path(\"census_hsapiens_healthy_adult_obs.parquet\")\n",
    "\n",
    "# Replace these with values discovered from summary_cell_counts\n",
    "HEALTHY_DISEASE_OTID = \"REPLACE_ME\"\n",
    "ADULT_STAGE_OTID = \"REPLACE_ME\"\n",
    "\n",
    "# Keep columns minimal for speed and file size\n",
    "OBS_COLS = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"sex\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"disease\",\n",
    "    \"development_stage\",\n",
    "    \"is_primary_data\",\n",
    "    # optionally include ontology IDs if you want them in the output\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"tissue_general_ontology_term_id\",\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "]\n",
    "\n",
    "VALUE_FILTER = (\n",
    "    \"is_primary_data == True and \"\n",
    "    f\"disease_ontology_term_id == '{HEALTHY_DISEASE_OTID}' and \"\n",
    "    f\"development_stage_ontology_term_id == '{ADULT_STAGE_OTID}'\"\n",
    ")\n",
    "\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    human = census[\"census_data\"][\"homo_sapiens\"]\n",
    "\n",
    "    read_iter = human.obs.read(\n",
    "        value_filter=VALUE_FILTER,\n",
    "        column_names=OBS_COLS,\n",
    "        # batch_size is supported by SOMA DataFrame reads (iterator over Arrow tables)\n",
    "        batch_size=200_000,\n",
    "    )\n",
    "\n",
    "    writer = None\n",
    "    try:\n",
    "        for i, tbl in enumerate(read_iter):\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(OUT.as_posix(), tbl.schema)\n",
    "            writer.write_table(tbl)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Wrote {i+1} batches...\")\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "print(f\"Done: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a08f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "HEALTHY_DISEASE_OTID is not set. Replace the placeholder with the ontology_term_id you discovered from census_info/summary_cell_counts.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone. Wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows across \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m batches to:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43mvalidate_placeholders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHEALTHY_DISEASE_OTID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mADULT_STAGE_OTID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     outpath = resolve_output_path(OUTDIR, OUTFILE, OVERWRITE)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCWD:\u001b[39m\u001b[33m\"\u001b[39m, Path.cwd())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mvalidate_placeholders\u001b[39m\u001b[34m(healthy_disease_otid, adult_stage_otid)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m [(\u001b[33m\"\u001b[39m\u001b[33mHEALTHY_DISEASE_OTID\u001b[39m\u001b[33m\"\u001b[39m, healthy_disease_otid), (\u001b[33m\"\u001b[39m\u001b[33mADULT_STAGE_OTID\u001b[39m\u001b[33m\"\u001b[39m, adult_stage_otid)]:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mstr\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m (val.strip() == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mREPLACE_ME\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m val):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    168\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not set. Replace the placeholder with the ontology_term_id you discovered \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfrom census_info/summary_cell_counts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: HEALTHY_DISEASE_OTID is not set. Replace the placeholder with the ontology_term_id you discovered from census_info/summary_cell_counts."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export CELLxGENE Census human (homo_sapiens) OBS metadata for healthy adults\n",
    "across all tissues to a Parquet file, streaming in batches to avoid OOM.\n",
    "\n",
    "Key features:\n",
    "- Writes to a user-writable output directory (HOME/census_exports by default)\n",
    "- Validates writability before starting\n",
    "- Streams Arrow record batches -> Parquet (single file)\n",
    "- Minimal, explicit column selection\n",
    "- Clear logging and failure modes\n",
    "\n",
    "Prereqs:\n",
    "  pip install -U cellxgene-census pyarrow pandas\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, List, Tuple\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# User-configurable parameters\n",
    "# ----------------------------\n",
    "\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# Replace these two with the ontology IDs you identified from summary_cell_counts\n",
    "HEALTHY_DISEASE_OTID = \"REPLACE_ME\"  # e.g., \"PATO:0000461\" (example only; use your discovered value)\n",
    "ADULT_STAGE_OTID = \"REPLACE_ME\"      # e.g., \"HsapDv:0000087\" (example only; use your discovered value)\n",
    "\n",
    "# Output settings\n",
    "OUTDIR = Path(os.environ.get(\"CENSUS_OUTDIR\", str(Path.home() / \"census_exports\")))\n",
    "OUTFILE = \"census_hsapiens_healthy_adult_obs.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Streaming settings\n",
    "BATCH_SIZE = 200_000  # tune up/down depending on RAM and IO\n",
    "LOG_EVERY_N_BATCHES = 10\n",
    "\n",
    "# Minimal useful OBS columns (add/remove as needed)\n",
    "OBS_COLS: List[str] = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"sex\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"disease\",\n",
    "    \"development_stage\",\n",
    "    \"is_primary_data\",\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"tissue_general_ontology_term_id\",\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions\n",
    "# ----------------------------\n",
    "\n",
    "def ensure_writable_dir(outdir: Path) -> None:\n",
    "    \"\"\"Create outdir (if needed) and ensure we can write to it.\"\"\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(outdir, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {outdir}\")\n",
    "\n",
    "\n",
    "def resolve_output_path(outdir: Path, filename: str, overwrite: bool) -> Path:\n",
    "    \"\"\"Resolve output path and handle overwrite behavior.\"\"\"\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / filename\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            # Ensure we can remove the existing file\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file (not writable): {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output file already exists: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def build_value_filter(\n",
    "    healthy_disease_otid: str,\n",
    "    adult_stage_otid: str,\n",
    "    require_primary: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Build SOMA value_filter for OBS selection.\"\"\"\n",
    "    parts = []\n",
    "    if require_primary:\n",
    "        parts.append(\"is_primary_data == True\")\n",
    "    parts.append(f\"disease_ontology_term_id == '{healthy_disease_otid}'\")\n",
    "    parts.append(f\"development_stage_ontology_term_id == '{adult_stage_otid}'\")\n",
    "    return \" and \".join(parts)\n",
    "\n",
    "\n",
    "def stream_obs_arrow_tables(\n",
    "    census: cxc.SOMACollection,\n",
    "    organism: str,\n",
    "    value_filter: str,\n",
    "    columns: List[str],\n",
    "    batch_size: int,\n",
    ") -> Iterable[pa.Table]:\n",
    "    \"\"\"Yield Arrow tables from Census OBS with a filter, streaming by batch_size.\"\"\"\n",
    "    exp = census[\"census_data\"][organism]\n",
    "    reader = exp.obs.read(\n",
    "        value_filter=value_filter,\n",
    "        column_names=columns,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for tbl in reader:\n",
    "        # Defensive: ensure it's a Table (some Arrow flows can return RecordBatch)\n",
    "        yield tbl if isinstance(tbl, pa.Table) else pa.Table.from_batches([tbl])\n",
    "\n",
    "\n",
    "def write_parquet_stream(\n",
    "    tables: Iterable[pa.Table],\n",
    "    outpath: Path,\n",
    "    log_every: int = 10,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Write an iterable of Arrow tables to a single Parquet file.\n",
    "\n",
    "    Returns:\n",
    "      (num_batches_written, total_rows_written)\n",
    "    \"\"\"\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    n_batches = 0\n",
    "    n_rows = 0\n",
    "\n",
    "    try:\n",
    "        for i, tbl in enumerate(tables, start=1):\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema)\n",
    "\n",
    "            writer.write_table(tbl)\n",
    "            n_batches += 1\n",
    "            n_rows += tbl.num_rows\n",
    "\n",
    "            if log_every and (i % log_every == 0):\n",
    "                print(f\"[write] batches seen={i:,}  batches_written={n_batches:,}  rows_written={n_rows:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    return n_batches, n_rows\n",
    "\n",
    "\n",
    "def validate_placeholders(healthy_disease_otid: str, adult_stage_otid: str) -> None:\n",
    "    \"\"\"Fail fast if the user forgot to replace placeholder ontology IDs.\"\"\"\n",
    "    for name, val in [(\"HEALTHY_DISEASE_OTID\", healthy_disease_otid), (\"ADULT_STAGE_OTID\", adult_stage_otid)]:\n",
    "        if (val is None) or (not isinstance(val, str)) or (val.strip() == \"\") or (\"REPLACE_ME\" in val):\n",
    "            raise ValueError(\n",
    "                f\"{name} is not set. Replace the placeholder with the ontology_term_id you discovered \"\n",
    "                f\"from census_info/summary_cell_counts.\"\n",
    "            )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    validate_placeholders(HEALTHY_DISEASE_OTID, ADULT_STAGE_OTID)\n",
    "\n",
    "    outpath = resolve_output_path(OUTDIR, OUTFILE, OVERWRITE)\n",
    "    print(\"CWD:\", Path.cwd())\n",
    "    print(\"Output:\", outpath)\n",
    "\n",
    "    value_filter = build_value_filter(\n",
    "        healthy_disease_otid=HEALTHY_DISEASE_OTID,\n",
    "        adult_stage_otid=ADULT_STAGE_OTID,\n",
    "        require_primary=True,\n",
    "    )\n",
    "    print(\"OBS filter:\", value_filter)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        tables = stream_obs_arrow_tables(\n",
    "            census=census,\n",
    "            organism=ORGANISM,\n",
    "            value_filter=value_filter,\n",
    "            columns=OBS_COLS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "        n_batches, n_rows = write_parquet_stream(\n",
    "            tables=tables,\n",
    "            outpath=outpath,\n",
    "            log_every=LOG_EVERY_N_BATCHES,\n",
    "        )\n",
    "\n",
    "    print(f\"Done. Wrote {n_rows:,} rows across {n_batches:,} batches to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae02334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category                                              label  \\\n",
      "1284  disease                                             normal   \n",
      "1171  disease                          cytomegalovirus infection   \n",
      "1274  disease                                           COVID-19   \n",
      "1177  disease                                  Parkinson disease   \n",
      "1041  disease                      dementia || Alzheimer disease   \n",
      "1030  disease                                           dementia   \n",
      "1249  disease                                       glioblastoma   \n",
      "1214  disease                                      breast cancer   \n",
      "1270  disease                     malignant ovarian serous tumor   \n",
      "1145  disease                                lung adenocarcinoma   \n",
      "1219  disease                       systemic lupus erythematosus   \n",
      "1241  disease                        B-cell non-Hodgkin lymphoma   \n",
      "1122  disease                                  Alzheimer disease   \n",
      "1138  disease                             dilated cardiomyopathy   \n",
      "1186  disease                                    atherosclerosis   \n",
      "1148  disease                                      neuroblastoma   \n",
      "1242  disease                          interstitial lung disease   \n",
      "1147  disease                              myocardial infarction   \n",
      "1049  disease  dementia || Alzheimer disease || Lewy body dem...   \n",
      "1218  disease                  nonpapillary renal cell carcinoma   \n",
      "1097  disease                                 pulmonary fibrosis   \n",
      "1184  disease                             chronic kidney disease   \n",
      "1090  disease                               colon adenocarcinoma   \n",
      "1055  disease                          dementia || schizophrenia   \n",
      "1152  disease                                      schizophrenia   \n",
      "1150  disease                               renal cell carcinoma   \n",
      "1162  disease                       squamous cell lung carcinoma   \n",
      "1178  disease                                metastatic melanoma   \n",
      "1029  disease                                     gastric cancer   \n",
      "1194  disease                   triple-negative breast carcinoma   \n",
      "1118  disease                   invasive ductal breast carcinoma   \n",
      "1132  disease                         clear cell renal carcinoma   \n",
      "1098  disease                               heart valve disorder   \n",
      "1134  disease                                      Crohn disease   \n",
      "1251  disease                           myelodysplastic syndrome   \n",
      "1123  disease                      amyotrophic lateral sclerosis   \n",
      "1259  disease                               basal cell carcinoma   \n",
      "1072  disease                                 bipolar I disorder   \n",
      "1255  disease                         bronchopulmonary dysplasia   \n",
      "1117  disease                B-cell acute lymphoblastic leukemia   \n",
      "1199  disease                                  colorectal cancer   \n",
      "1033  disease  dementia || vascular dementia || Alzheimer dis...   \n",
      "1189  disease                                open-angle glaucoma   \n",
      "1258  disease         myelodysplastic/myeloproliferative disease   \n",
      "1139  disease                                           epilepsy   \n",
      "1254  disease                     progressive supranuclear palsy   \n",
      "1091  disease                               acute kidney failure   \n",
      "1198  disease                                          tauopathy   \n",
      "1238  disease                     primary sclerosing cholangitis   \n",
      "1253  disease                                follicular lymphoma   \n",
      "1217  disease                                 Lewy body dementia   \n",
      "1245  disease                            frontotemporal dementia   \n",
      "1131  disease              chronic obstructive pulmonary disease   \n",
      "1175  disease                           type 2 diabetes mellitus   \n",
      "1243  disease    arrhythmogenic right ventricular cardiomyopathy   \n",
      "1197  disease                               hematologic disorder   \n",
      "1222  disease                                       Pick disease   \n",
      "1277  disease                             post-COVID-19 disorder   \n",
      "1232  disease                                   Sjogren syndrome   \n",
      "1151  disease                        respiratory system disorder   \n",
      "\n",
      "                                     ontology_term_id  unique_cell_count  \n",
      "1284                                     PATO:0000461           64381160  \n",
      "1171                                    MONDO:0005132            6415859  \n",
      "1274                                    MONDO:0100096            4688586  \n",
      "1177                                    MONDO:0005180            1810127  \n",
      "1041                   MONDO:0001627 || MONDO:0004975            1486303  \n",
      "1030                                    MONDO:0001627            1394874  \n",
      "1249                                    MONDO:0018177            1363983  \n",
      "1214                                    MONDO:0007254            1286123  \n",
      "1270                                    MONDO:0024885             927205  \n",
      "1145                                    MONDO:0005061             831387  \n",
      "1219                                    MONDO:0007915             777258  \n",
      "1241                                    MONDO:0015759             642851  \n",
      "1122                                    MONDO:0004975             608235  \n",
      "1138                                    MONDO:0005021             482581  \n",
      "1186                                    MONDO:0005311             400631  \n",
      "1148                                    MONDO:0005072             372619  \n",
      "1242                                    MONDO:0015925             316801  \n",
      "1147                                    MONDO:0005068             309951  \n",
      "1049  MONDO:0001627 || MONDO:0004975 || MONDO:0007488             304296  \n",
      "1218                                    MONDO:0007763             287368  \n",
      "1097                                    MONDO:0002771             268932  \n",
      "1184                                    MONDO:0005300             266542  \n",
      "1090                                    MONDO:0002271             257251  \n",
      "1055                   MONDO:0001627 || MONDO:0005090             247512  \n",
      "1152                                    MONDO:0005090             240069  \n",
      "1150                                    MONDO:0005086             238027  \n",
      "1162                                    MONDO:0005097             235853  \n",
      "1178                                    MONDO:0005191             230924  \n",
      "1029                                    MONDO:0001056             217923  \n",
      "1194                                    MONDO:0005494             215960  \n",
      "1118                                    MONDO:0004953             208638  \n",
      "1132                                    MONDO:0005005             187792  \n",
      "1098                                    MONDO:0002869             183616  \n",
      "1134                                    MONDO:0005011             179393  \n",
      "1251                                    MONDO:0018881             176174  \n",
      "1123                                    MONDO:0004976             171539  \n",
      "1259                                    MONDO:0020804             157034  \n",
      "1072                                    MONDO:0001866             152195  \n",
      "1255                                    MONDO:0019091             150671  \n",
      "1117                                    MONDO:0004947             146710  \n",
      "1199                                    MONDO:0005575             146575  \n",
      "1033  MONDO:0001627 || MONDO:0004648 || MONDO:0004975             140940  \n",
      "1189                                    MONDO:0005338             139633  \n",
      "1258                                    MONDO:0020077             130268  \n",
      "1139                                    MONDO:0005027             129905  \n",
      "1254                                    MONDO:0019037             127542  \n",
      "1091                                    MONDO:0002492             125291  \n",
      "1198                                    MONDO:0005574             124789  \n",
      "1238                                    MONDO:0013433             124635  \n",
      "1253                                    MONDO:0018906             122702  \n",
      "1217                                    MONDO:0007488             120232  \n",
      "1245                                    MONDO:0017276             118536  \n",
      "1131                                    MONDO:0005002             110630  \n",
      "1175                                    MONDO:0005148             108640  \n",
      "1243                                    MONDO:0016587             104496  \n",
      "1197                                    MONDO:0005570             100206  \n",
      "1222                                    MONDO:0008243              98043  \n",
      "1277                                    MONDO:0100320              97224  \n",
      "1232                                    MONDO:0010030              95254  \n",
      "1151                                    MONDO:0005087              94987  \n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census as cxc\n",
    "\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    scc = census[\"census_info\"][\"summary_cell_counts\"].read().concat().to_pandas()\n",
    "\n",
    "hs = scc[scc[\"organism\"].astype(str).str.contains(\"sapiens\", case=False, na=False)]\n",
    "subset = hs[hs[\"category\"].isin([\"disease\", \"development_stage\"])].copy()\n",
    "\n",
    "# Show the most common labels to help you choose the correct IDs\n",
    "print(\n",
    "    subset.sort_values([\"category\", \"unique_cell_count\"], ascending=[True, False])\n",
    "          .loc[:, [\"category\", \"label\", \"ontology_term_id\", \"unique_cell_count\"]]\n",
    "          .head(60)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490d1ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home\n",
      "Output: /home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No development_stage labels containing 'adult' were found in summary_cell_counts.\n",
      "         The export will proceed WITHOUT a development_stage filter (healthy only).\n",
      "\n",
      "OBS value_filter:\n",
      "is_primary_data == True and disease_ontology_term_id == 'PATO:0000461'\n",
      "\n",
      "Done. Wrote 62,464,283 rows across 1 batches to:\n",
      "  /home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# From your output: disease \"normal\" => PATO:0000461\n",
    "HEALTHY_DISEASE_OTID = \"PATO:0000461\"\n",
    "\n",
    "# Output location (override with env var if desired)\n",
    "OUTDIR = Path(os.environ.get(\"CENSUS_OUTDIR\", str(Path.home() / \"census_exports\")))\n",
    "OUTFILE = \"census_hsapiens_healthy_adult_obs.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Streaming parameters\n",
    "BATCH_SIZE = 200_000\n",
    "LOG_EVERY_N = 10\n",
    "\n",
    "# Keep columns minimal to reduce output size\n",
    "OBS_COLS: List[str] = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"sex\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"disease\",\n",
    "    \"development_stage\",\n",
    "    \"is_primary_data\",\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"tissue_general_ontology_term_id\",\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite (no write permission): {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and OVERWRITE=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def discover_adult_stage_otids(census: cxc.SOMACollection) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Returns list of (label, ontology_term_id, unique_cell_count) for human development_stage\n",
    "    entries whose label contains 'adult' (case-insensitive).\n",
    "    \"\"\"\n",
    "    scc = census[\"census_info\"][\"summary_cell_counts\"].read().concat().to_pandas()\n",
    "\n",
    "    hs = scc[\n",
    "        scc[\"organism\"].astype(str).str.contains(\"sapiens\", case=False, na=False)\n",
    "        & (scc[\"category\"] == \"development_stage\")\n",
    "    ].copy()\n",
    "\n",
    "    hs[\"label_str\"] = hs[\"label\"].astype(str)\n",
    "    adult = hs[hs[\"label_str\"].str.contains(\"adult\", case=False, na=False)].copy()\n",
    "\n",
    "    adult = adult.sort_values(\"unique_cell_count\", ascending=False)\n",
    "    results = list(\n",
    "        zip(\n",
    "            adult[\"label\"].astype(str).tolist(),\n",
    "            adult[\"ontology_term_id\"].astype(str).tolist(),\n",
    "            adult[\"unique_cell_count\"].astype(int).tolist(),\n",
    "        )\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def or_chain_equals(field: str, values: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a safe OR-chain expression:\n",
    "      (field == 'v1' or field == 'v2' or ...)\n",
    "    \"\"\"\n",
    "    if not values:\n",
    "        return \"\"\n",
    "    parts = [f\"{field} == '{v}'\" for v in values]\n",
    "    return \"(\" + \" or \".join(parts) + \")\"\n",
    "\n",
    "\n",
    "def build_value_filter(healthy_disease_otid: str, adult_stage_otids: List[str]) -> str:\n",
    "    base = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{healthy_disease_otid}'\",\n",
    "    ]\n",
    "\n",
    "    adult_expr = or_chain_equals(\"development_stage_ontology_term_id\", adult_stage_otids)\n",
    "    if adult_expr:\n",
    "        base.append(adult_expr)\n",
    "\n",
    "    return \" and \".join(base)\n",
    "\n",
    "\n",
    "def export_obs_to_parquet(\n",
    "    census: cxc.SOMACollection,\n",
    "    organism: str,\n",
    "    value_filter: str,\n",
    "    obs_cols: List[str],\n",
    "    outpath: Path,\n",
    "    batch_size: int,\n",
    "    log_every_n: int,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Streams OBS rows matching value_filter to a single Parquet file.\n",
    "    Returns (batches_written, rows_written).\n",
    "    \"\"\"\n",
    "    exp = census[\"census_data\"][organism]\n",
    "    reader = exp.obs.read(\n",
    "        value_filter=value_filter,\n",
    "        column_names=obs_cols,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    batches_written = 0\n",
    "    rows_written = 0\n",
    "\n",
    "    try:\n",
    "        for i, tbl in enumerate(reader, start=1):\n",
    "            if not isinstance(tbl, pa.Table):\n",
    "                tbl = pa.Table.from_batches([tbl])\n",
    "\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema)\n",
    "\n",
    "            writer.write_table(tbl)\n",
    "            batches_written += 1\n",
    "            rows_written += tbl.num_rows\n",
    "\n",
    "            if log_every_n and (i % log_every_n == 0):\n",
    "                print(f\"[export] batches_seen={i:,} batches_written={batches_written:,} rows_written={rows_written:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    return batches_written, rows_written\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main execution\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    outpath = resolve_outpath(OUTDIR, OUTFILE, OVERWRITE)\n",
    "    print(\"CWD:\", Path.cwd())\n",
    "    print(\"Output:\", outpath)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        adult_entries = discover_adult_stage_otids(census)\n",
    "\n",
    "        if not adult_entries:\n",
    "            print(\"WARNING: No development_stage labels containing 'adult' were found in summary_cell_counts.\")\n",
    "            print(\"         The export will proceed WITHOUT a development_stage filter (healthy only).\")\n",
    "            adult_stage_otids = []\n",
    "        else:\n",
    "            print(\"Discovered adult development_stage candidates (label, ontology_term_id, unique_cell_count):\")\n",
    "            for label, otid, n in adult_entries[:25]:\n",
    "                print(f\"  - {label} | {otid} | {n:,}\")\n",
    "            adult_stage_otids = [otid for _, otid, _ in adult_entries]\n",
    "\n",
    "        value_filter = build_value_filter(HEALTHY_DISEASE_OTID, adult_stage_otids)\n",
    "        print(\"\\nOBS value_filter:\")\n",
    "        print(value_filter)\n",
    "        print()\n",
    "\n",
    "        batches, rows = export_obs_to_parquet(\n",
    "            census=census,\n",
    "            organism=ORGANISM,\n",
    "            value_filter=value_filter,\n",
    "            obs_cols=OBS_COLS,\n",
    "            outpath=outpath,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            log_every_n=LOG_EVERY_N,\n",
    "        )\n",
    "\n",
    "    print(f\"Done. Wrote {rows:,} rows across {batches:,} batches to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dbb858a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BatchSize' from 'tiledbsoma' (/home/crizza/miniconda3/lib/python3.12/site-packages/tiledbsoma/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtiledbsoma\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSize\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BatchSize' from 'tiledbsoma.options' (/home/crizza/miniconda3/lib/python3.12/site-packages/tiledbsoma/options/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtiledbsoma\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSize\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtiledbsoma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSize\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# User-configurable parameters\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     25\u001b[39m CENSUS_VERSION = \u001b[33m\"\u001b[39m\u001b[33mstable\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# or a pinned date like \"2025-11-08\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BatchSize' from 'tiledbsoma' (/home/crizza/miniconda3/lib/python3.12/site-packages/tiledbsoma/__init__.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census\n",
    "import tiledbsoma\n",
    "\n",
    "# --- BatchSize import (location varies by tiledbsoma version) ---\n",
    "try:\n",
    "    from tiledbsoma.options import BatchSize\n",
    "except ImportError:\n",
    "    from tiledbsoma import BatchSize\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# User-configurable parameters\n",
    "# ----------------------------\n",
    "CENSUS_VERSION = \"stable\"  # or a pinned date like \"2025-11-08\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "\n",
    "# ontology ID for \"normal\" disease in the Census (you already used this successfully)\n",
    "NORMAL_DISEASE_ONTOLOGY_ID = \"PATO:0000461\"\n",
    "\n",
    "SUSPENSION_TYPE = \"cell\"\n",
    "SEXES = [\"male\", \"female\"]\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# output\n",
    "OUT_DIR = Path.home() / \"census_exports\" / \"hsapiens_normal_10x_adult15_cell_obs_parts\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# how many rows per parquet part (target; exact size may vary)\n",
    "ROWS_PER_BATCH = 250_000\n",
    "\n",
    "\n",
    "def _format_in_list(values: list[str]) -> str:\n",
    "    \"\"\"Return a SOMA value_filter-compatible list literal: ['a','b']\"\"\"\n",
    "    escaped = [v.replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in escaped]) + \"]\"\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns mapping: assay_label -> assay_ontology_term_id\n",
    "    using census_info/summary_cell_counts.\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(TARGET_ASSAY_LABELS))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in TARGET_ASSAY_LABELS if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Did not find ontology_term_id for assay label(s): {missing}. \"\n",
    "            f\"Found={list(found.keys())}\"\n",
    "        )\n",
    "    return found\n",
    "\n",
    "\n",
    "def infer_age_filter_expr(obs_schema: pa.Schema) -> str | None:\n",
    "    \"\"\"\n",
    "    Prefer in-query filter if donor_age is numeric and (optionally) donor_age_unit exists.\n",
    "    Otherwise return None (caller can post-filter).\n",
    "    \"\"\"\n",
    "    fields = {f.name: f.type for f in obs_schema}\n",
    "\n",
    "    if \"donor_age\" not in fields:\n",
    "        return None\n",
    "\n",
    "    donor_age_type = fields[\"donor_age\"]\n",
    "    donor_age_is_numeric = pa.types.is_integer(donor_age_type) or pa.types.is_floating(donor_age_type)\n",
    "    if not donor_age_is_numeric:\n",
    "        return None\n",
    "\n",
    "    if \"donor_age_unit\" in fields:\n",
    "        # Most robust: enforce years\n",
    "        return f\"(donor_age >= {MIN_AGE_YEARS} and donor_age_unit == 'year')\"\n",
    "    else:\n",
    "        return f\"(donor_age >= {MIN_AGE_YEARS})\"\n",
    "\n",
    "\n",
    "_age_re = re.compile(r\"(\\d+(?:\\.\\d+)?)\")\n",
    "\n",
    "\n",
    "def postfilter_age_if_needed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If the query couldn't filter by age in the backend, do a best-effort â‰¥15y filter here.\n",
    "    Handles:\n",
    "      - numeric donor_age (+ optional donor_age_unit)\n",
    "      - string donor_age like '34 years'\n",
    "    Rows with unknown/invalid age are dropped.\n",
    "    \"\"\"\n",
    "    if \"donor_age\" not in df.columns:\n",
    "        # Nothing we can do; return unchanged.\n",
    "        return df\n",
    "\n",
    "    # If donor_age is numeric already:\n",
    "    if pd.api.types.is_numeric_dtype(df[\"donor_age\"]):\n",
    "        if \"donor_age_unit\" in df.columns:\n",
    "            df = df[df[\"donor_age_unit\"] == \"year\"]\n",
    "        return df[df[\"donor_age\"] >= MIN_AGE_YEARS]\n",
    "\n",
    "    # Otherwise try parsing from strings\n",
    "    donor_age_str = df[\"donor_age\"].astype(\"string\")\n",
    "    parsed = donor_age_str.str.extract(_age_re, expand=False)\n",
    "    parsed_num = pd.to_numeric(parsed, errors=\"coerce\")\n",
    "\n",
    "    # enforce years if unit exists OR if string contains 'year'\n",
    "    if \"donor_age_unit\" in df.columns:\n",
    "        unit_ok = df[\"donor_age_unit\"].astype(\"string\") == \"year\"\n",
    "    else:\n",
    "        unit_ok = donor_age_str.str.contains(\"year\", case=False, na=False)\n",
    "\n",
    "    keep = unit_ok & (parsed_num >= MIN_AGE_YEARS)\n",
    "    return df.loc[keep].copy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"Writing Parquet parts to:\\n  {OUT_DIR}\")\n",
    "\n",
    "    with cellxgene_census.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        human = census[\"census_data\"][ORGANISM]\n",
    "\n",
    "        # 1) Resolve assay ontology IDs (avoids apostrophe issues in assay labels)\n",
    "        assay_map = lookup_assay_term_ids(census)\n",
    "        assay_term_ids = [assay_map[x] for x in TARGET_ASSAY_LABELS]\n",
    "        print(\"Assay ontology_term_id mapping:\")\n",
    "        for k, v in assay_map.items():\n",
    "            print(f\"  {k:12s} -> {v}\")\n",
    "\n",
    "        # 2) Build the base filter\n",
    "        base_filter_parts = [\n",
    "            \"is_primary_data == True\",\n",
    "            f\"disease_ontology_term_id == '{NORMAL_DISEASE_ONTOLOGY_ID}'\",\n",
    "            f\"sex in {_format_in_list(SEXES)}\",\n",
    "            f\"suspension_type == '{SUSPENSION_TYPE}'\",\n",
    "            f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "        ]\n",
    "\n",
    "        # 3) Add in-backend age filter if possible\n",
    "        obs_schema = human.obs.schema\n",
    "        age_expr = infer_age_filter_expr(obs_schema)\n",
    "        if age_expr is None:\n",
    "            print(\n",
    "                \"WARNING: Could not apply an in-query age filter (>=15y). \"\n",
    "                \"Will post-filter age per batch after reading.\"\n",
    "            )\n",
    "        else:\n",
    "            base_filter_parts.append(age_expr)\n",
    "\n",
    "        obs_value_filter = \" and \".join(base_filter_parts)\n",
    "        print(\"\\nOBS value_filter:\\n\", obs_value_filter)\n",
    "\n",
    "        # 4) Optional: count how many cells match without reading all metadata\n",
    "        # (axis_query + n_obs is the standard way to get the filtered size) :contentReference[oaicite:5]{index=5}\n",
    "        query = human.axis_query(\n",
    "            measurement_name=\"RNA\",\n",
    "            obs_query=tiledbsoma.AxisQuery(value_filter=obs_value_filter),\n",
    "        )\n",
    "        try:\n",
    "            print(f\"\\nFiltered cell count (query.n_obs): {query.n_obs:,}\")\n",
    "        finally:\n",
    "            query.close()\n",
    "\n",
    "        # 5) Stream obs rows and write parquet parts\n",
    "        batch_size = BatchSize(count=ROWS_PER_BATCH)\n",
    "        read_iter = human.obs.read(value_filter=obs_value_filter, batch_size=batch_size)\n",
    "\n",
    "        total_rows_written = 0\n",
    "        part_idx = 0\n",
    "\n",
    "        for tbl in read_iter:\n",
    "            # If we couldn't do in-query age filter, apply post-filter here\n",
    "            if age_expr is None:\n",
    "                df = tbl.to_pandas()\n",
    "                df = postfilter_age_if_needed(df)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                tbl = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "            out_path = OUT_DIR / f\"part-{part_idx:05d}.parquet\"\n",
    "            pq.write_table(tbl, out_path, compression=\"zstd\")\n",
    "\n",
    "            n = tbl.num_rows\n",
    "            total_rows_written += n\n",
    "            if part_idx % 10 == 0:\n",
    "                print(f\"  wrote {out_path.name}: {n:,} rows (running total {total_rows_written:,})\")\n",
    "\n",
    "            part_idx += 1\n",
    "\n",
    "        print(f\"\\nDone. Wrote {total_rows_written:,} rows across {part_idx} part files.\")\n",
    "        print(f\"Output directory:\\n  {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d511d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: /home/crizza/census_exports/census_hsapiens_normal_10x_adult15_cell_obs.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assay term IDs:\n",
      "  10x 3' v3 -> EFO:0009922\n",
      "  10x 3' v2 -> EFO:0009899\n",
      "  10x multiome -> EFO:0030059\n",
      "\n",
      "OBS value_filter:\n",
      " is_primary_data == True and disease_ontology_term_id == 'PATO:0000461' and sex in ['male', 'female'] and suspension_type == 'cell' and assay_ontology_term_id in ['EFO:0009899', 'EFO:0009922', 'EFO:0030059']\n",
      "NOTE: donor_age is not a numeric field in this obs schema; applying adult>=15y as a post-filter.\n",
      "\n"
     ]
    },
    {
     "ename": "SOMAError",
     "evalue": "[SOMAArray] internal coding error: No column named donor_age found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSOMAError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 254\u001b[39m\n\u001b[32m    250\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone. Wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrows_written\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows to:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNOTE: donor_age is not a numeric field in this obs schema; applying adult>=15y as a post-filter.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# Read iterator (no BatchSize required)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m read_iter = \u001b[43mhuman\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOBS_COLS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m writer: Optional[pq.ParquetWriter] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    218\u001b[39m rows_written = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tiledbsoma/_dataframe.py:789\u001b[39m, in \u001b[36mDataFrame.read\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28mself\u001b[39m._verify_open_for_reading()\n\u001b[32m    788\u001b[39m \u001b[38;5;66;03m# TODO: batch_size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTableReadIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43marray\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_clib_result_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_order\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplatform_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplatform_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tiledbsoma/_read_iters.py:98\u001b[39m, in \u001b[36mTableReadIter.__init__\u001b[39m\u001b[34m(self, array, coords, column_names, result_order, value_filter, platform_config, coord_space)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     62\u001b[39m     array: SOMAArray,\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     coord_space: CoordinateSpace | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     70\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initializes a new TableReadIter for SOMAArrays.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m \n\u001b[32m     97\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mArrowTableRead\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplatform_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoord_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoord_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tiledbsoma/_read_iters.py:543\u001b[39m, in \u001b[36mArrowTableRead.__init__\u001b[39m\u001b[34m(self, array, coords, column_names, result_order, value_filter, platform_config, coord_space)\u001b[39m\n\u001b[32m    541\u001b[39m column_names = column_names \u001b[38;5;129;01mor\u001b[39;00m array.schema.names\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[43mclib_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m.select_columns(\u001b[38;5;28mself\u001b[39m.mq._handle)\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value_filter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    546\u001b[39m     \u001b[38;5;28mself\u001b[39m.mq._handle.set_condition(QueryCondition(value_filter), clib_handle.schema)\n",
      "\u001b[31mSOMAError\u001b[39m: [SOMAArray] internal coding error: No column named donor_age found"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# User configuration\n",
    "# -----------------------------\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# Filters requested\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"  # \"normal\"\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "TARGET_SEXES = [\"male\", \"female\"]\n",
    "TARGET_SUSPENSION_TYPE = \"cell\"\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# Output\n",
    "OUTDIR = Path(os.environ.get(\"CENSUS_OUTDIR\", str(Path.home() / \"census_exports\")))\n",
    "OUTFILE = \"census_hsapiens_normal_10x_adult15_cell_obs.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Keep columns tight to reduce file size\n",
    "OBS_COLS: List[str] = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    \"sex\",\n",
    "    \"suspension_type\",\n",
    "    \"disease\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "    \"donor_age\",\n",
    "    \"donor_age_unit\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"is_primary_data\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and OVERWRITE=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def _format_in_list(values: List[str]) -> str:\n",
    "    # SOMA value_filter list literal: ['a','b']\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id using census_info/summary_cell_counts.\n",
    "    This avoids quoting problems with \"10x 3' v3\".\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(TARGET_ASSAY_LABELS))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in TARGET_ASSAY_LABELS if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Did not find ontology_term_id for assay label(s): {missing}. \"\n",
    "            f\"Found: {list(found.keys())}\"\n",
    "        )\n",
    "    return found\n",
    "\n",
    "\n",
    "def has_field(schema: pa.Schema, name: str) -> bool:\n",
    "    return name in schema.names\n",
    "\n",
    "\n",
    "def build_obs_value_filter(\n",
    "    assay_term_ids: Optional[List[str]],\n",
    "    obs_schema: pa.Schema,\n",
    ") -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Returns (value_filter, needs_post_age_filter).\n",
    "    We try to express age>=15 years in the backend if donor_age is numeric.\n",
    "    Otherwise we will post-filter by age on Arrow tables.\n",
    "    \"\"\"\n",
    "    parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{NORMAL_DISEASE_OTID}'\",\n",
    "        f\"sex in {_format_in_list(TARGET_SEXES)}\",\n",
    "        f\"suspension_type == '{TARGET_SUSPENSION_TYPE}'\",\n",
    "    ]\n",
    "\n",
    "    if assay_term_ids is not None and has_field(obs_schema, \"assay_ontology_term_id\"):\n",
    "        parts.append(f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\")\n",
    "    else:\n",
    "        # Fallback to label filter if ontology field is absent\n",
    "        parts.append(f\"assay in {_format_in_list(TARGET_ASSAY_LABELS)}\")\n",
    "\n",
    "    # Age handling\n",
    "    needs_post_age = True\n",
    "    if has_field(obs_schema, \"donor_age\") and pa.types.is_numeric(obs_schema.field(\"donor_age\").type):\n",
    "        if has_field(obs_schema, \"donor_age_unit\"):\n",
    "            parts.append(f\"(donor_age >= {MIN_AGE_YEARS} and donor_age_unit == 'year')\")\n",
    "        else:\n",
    "            # If no unit, assume years (best-effort; verify if needed)\n",
    "            parts.append(f\"(donor_age >= {MIN_AGE_YEARS})\")\n",
    "        needs_post_age = False\n",
    "\n",
    "    return \" and \".join(parts), needs_post_age\n",
    "\n",
    "\n",
    "def post_filter_adult_15_plus(tbl: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Post-filter to adults (>=15y) when we could not express age filter in value_filter.\n",
    "    Priority:\n",
    "      1) donor_age numeric (+ donor_age_unit == 'year' when present)\n",
    "      2) development_stage_ontology_term_id adult-stage heuristic (coarser)\n",
    "    \"\"\"\n",
    "    schema = tbl.schema\n",
    "\n",
    "    # 1) donor_age path\n",
    "    if \"donor_age\" in schema.names:\n",
    "        col = tbl[\"donor_age\"]\n",
    "        # numeric donor_age\n",
    "        if pa.types.is_numeric(col.type):\n",
    "            mask = pc.greater_equal(col, pa.scalar(MIN_AGE_YEARS, type=col.type))\n",
    "            if \"donor_age_unit\" in schema.names:\n",
    "                mask = pc.and_(mask, pc.equal(tbl[\"donor_age_unit\"], pa.scalar(\"year\")))\n",
    "            return tbl.filter(mask)\n",
    "\n",
    "    # 2) fallback: development_stage ontology heuristic (coarser than real age)\n",
    "    # Adult stage root in HsapDv is commonly HsapDv:0000087 (and possible descendants).\n",
    "    # This is a best-effort fallback ONLY if donor_age is not usable.\n",
    "    if \"development_stage_ontology_term_id\" in schema.names:\n",
    "        stage = tbl[\"development_stage_ontology_term_id\"]\n",
    "        # startswith(\"HsapDv:0000087\") OR equals common adult terms\n",
    "        # (Some datasets use the root directly, others use children; this remains heuristic.)\n",
    "        adult_terms = pa.array(\n",
    "            [\"HsapDv:0000087\", \"HsapDv:0000088\", \"HsapDv:0000089\"],\n",
    "            type=pa.string(),\n",
    "        )\n",
    "        mask = pc.is_in(stage, value_set=adult_terms)\n",
    "        return tbl.filter(mask)\n",
    "\n",
    "    # If neither exists, return unchanged (cannot enforce adult criterion)\n",
    "    return tbl\n",
    "\n",
    "\n",
    "def enforce_columns(tbl: pa.Table, desired: List[str]) -> pa.Table:\n",
    "    cols = [c for c in desired if c in tbl.schema.names]\n",
    "    return tbl.select(cols)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    outpath = resolve_outpath(OUTDIR, OUTFILE, OVERWRITE)\n",
    "    print(\"Output:\", outpath)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        human = census[\"census_data\"][ORGANISM]\n",
    "        obs_schema = human.obs.schema\n",
    "\n",
    "        # Resolve assay ontology IDs (preferred)\n",
    "        assay_term_ids: Optional[List[str]] = None\n",
    "        try:\n",
    "            assay_map = lookup_assay_term_ids(census)\n",
    "            assay_term_ids = [assay_map[x] for x in TARGET_ASSAY_LABELS]\n",
    "            print(\"Assay term IDs:\")\n",
    "            for k, v in assay_map.items():\n",
    "                print(f\"  {k} -> {v}\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Could not resolve assay_ontology_term_id mapping; falling back to label filter. Reason: {e}\")\n",
    "\n",
    "        value_filter, needs_post_age = build_obs_value_filter(assay_term_ids, obs_schema)\n",
    "        print(\"\\nOBS value_filter:\\n\", value_filter)\n",
    "        if needs_post_age:\n",
    "            print(\"NOTE: donor_age is not a numeric field in this obs schema; applying adult>=15y as a post-filter.\\n\")\n",
    "\n",
    "        # Read iterator (no BatchSize required)\n",
    "        read_iter = human.obs.read(value_filter=value_filter, column_names=OBS_COLS)\n",
    "\n",
    "        writer: Optional[pq.ParquetWriter] = None\n",
    "        rows_written = 0\n",
    "        batches_seen = 0\n",
    "\n",
    "        try:\n",
    "            for tbl in read_iter:\n",
    "                batches_seen += 1\n",
    "                if not isinstance(tbl, pa.Table):\n",
    "                    tbl = pa.Table.from_batches([tbl])\n",
    "\n",
    "                # Apply post-filter for age if needed\n",
    "                if needs_post_age:\n",
    "                    tbl = post_filter_adult_15_plus(tbl)\n",
    "\n",
    "                # Ensure consistent column set (in case some are missing)\n",
    "                tbl = enforce_columns(tbl, OBS_COLS)\n",
    "\n",
    "                if tbl.num_rows == 0:\n",
    "                    continue\n",
    "\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=\"zstd\")\n",
    "\n",
    "                writer.write_table(tbl)\n",
    "                rows_written += tbl.num_rows\n",
    "\n",
    "                if batches_seen % 5 == 0:\n",
    "                    print(f\"[progress] batches_seen={batches_seen:,} rows_written={rows_written:,}\")\n",
    "\n",
    "        finally:\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "\n",
    "        print(f\"\\nDone. Wrote {rows_written:,} rows to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128109bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/home/crizza/census_exports/census_hsapiens_normal_10x_adult15_cell_obs.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m p = \u001b[33m\"\u001b[39m\u001b[33m/home/crizza/census_exports/census_hsapiens_normal_10x_adult15_cell_obs.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m t = \u001b[43mpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdonor_age\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdonor_age_unit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdevelopment_stage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df = t.to_pandas()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[33m\"\u001b[39m\u001b[33mdonor_age\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdonor_age_unit\u001b[39m\u001b[33m\"\u001b[39m]].dropna().describe(include=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:1858\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1847\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1848\u001b[39m                binary_type=\u001b[38;5;28;01mNone\u001b[39;00m, list_type=\u001b[38;5;28;01mNone\u001b[39;00m, memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1854\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1855\u001b[39m                arrow_extensions_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1857\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1858\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1879\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1880\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1881\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:1438\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1435\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1436\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/dataset.py:790\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    779\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    780\u001b[39m     schema=schema,\n\u001b[32m    781\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    787\u001b[39m )\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/dataset.py:472\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    470\u001b[39m         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    475\u001b[39m     partitioning=partitioning,\n\u001b[32m    476\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    477\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    478\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    479\u001b[39m )\n\u001b[32m    480\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pyarrow/dataset.py:437\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    435\u001b[39m     paths_or_selector = [path]\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: /home/crizza/census_exports/census_hsapiens_normal_10x_adult15_cell_obs.parquet"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "p = \"/home/crizza/census_exports/census_hsapiens_normal_10x_adult15_cell_obs.parquet\"\n",
    "t = pq.read_table(p, columns=[c for c in [\"donor_age\", \"donor_age_unit\", \"development_stage\"] if c is not None])\n",
    "df = t.to_pandas()\n",
    "\n",
    "print(df[[\"donor_age\",\"donor_age_unit\"]].dropna().describe(include=\"all\"))\n",
    "print(df[\"development_stage\"].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6e9040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present: ['development_stage', 'development_stage_ontology_term_id']\n",
      "Rows scanned: 2,000,000\n",
      "Pct development_stage that looks like ontology ID: 0.00%\n",
      "\n",
      "Top development_stage:\n",
      "  50-year-old stage -> 304,688\n",
      "  42-year-old stage -> 196,204\n",
      "  29-year-old stage -> 165,224\n",
      "  22nd week post-fertilization stage -> 118,647\n",
      "  adult stage -> 115,964\n",
      "  seventh decade stage -> 66,740\n",
      "  7-month-old stage -> 51,878\n",
      "  26-year-old stage -> 49,772\n",
      "  69-year-old stage -> 43,764\n",
      "  60-year-old stage -> 36,428\n",
      "  young adult stage -> 30,750\n",
      "  63-year-old stage -> 30,708\n",
      "  54-year-old stage -> 29,585\n",
      "  unknown -> 28,634\n",
      "  prime adult stage -> 27,864\n",
      "  sixth decade stage -> 25,501\n",
      "  10th week post-fertilization stage -> 25,358\n",
      "  62-year-old stage -> 25,070\n",
      "  3-year-old stage -> 24,917\n",
      "  65-year-old stage -> 24,366\n",
      "  middle aged stage -> 24,222\n",
      "  71-year-old stage -> 23,847\n",
      "  59-year-old stage -> 23,336\n",
      "  sixth LMP month stage -> 20,475\n",
      "  53-year-old stage -> 19,407\n",
      "\n",
      "Top development_stage_ontology_term_id:\n",
      "  HsapDv:0000144 -> 304,688\n",
      "  HsapDv:0000136 -> 196,204\n",
      "  HsapDv:0000123 -> 165,224\n",
      "  HsapDv:0000059 -> 118,647\n",
      "  HsapDv:0000258 -> 115,964\n",
      "  HsapDv:0000241 -> 66,740\n",
      "  HsapDv:0000180 -> 51,878\n",
      "  HsapDv:0000120 -> 49,772\n",
      "  HsapDv:0000163 -> 43,764\n",
      "  HsapDv:0000154 -> 36,428\n",
      "  HsapDv:0000266 -> 30,750\n",
      "  HsapDv:0000157 -> 30,708\n",
      "  HsapDv:0000148 -> 29,585\n",
      "  unknown -> 28,634\n",
      "  HsapDv:0000226 -> 27,864\n",
      "  HsapDv:0000240 -> 25,501\n",
      "  HsapDv:0000047 -> 25,358\n",
      "  HsapDv:0000156 -> 25,070\n",
      "  HsapDv:0000097 -> 24,917\n",
      "  HsapDv:0000159 -> 24,366\n",
      "  HsapDv:0000267 -> 24,222\n",
      "  HsapDv:0000165 -> 23,847\n",
      "  HsapDv:0000153 -> 23,336\n",
      "  HsapDv:0000200 -> 20,475\n",
      "  HsapDv:0000147 -> 19,407\n",
      "\n",
      "Top (development_stage, development_stage_ontology_term_id) pairs:\n",
      "  (50-year-old stage, HsapDv:0000144) -> 304,688\n",
      "  (42-year-old stage, HsapDv:0000136) -> 196,204\n",
      "  (29-year-old stage, HsapDv:0000123) -> 165,224\n",
      "  (22nd week post-fertilization stage, HsapDv:0000059) -> 118,647\n",
      "  (adult stage, HsapDv:0000258) -> 115,964\n",
      "  (seventh decade stage, HsapDv:0000241) -> 66,740\n",
      "  (7-month-old stage, HsapDv:0000180) -> 51,878\n",
      "  (26-year-old stage, HsapDv:0000120) -> 49,772\n",
      "  (69-year-old stage, HsapDv:0000163) -> 43,764\n",
      "  (60-year-old stage, HsapDv:0000154) -> 36,428\n",
      "  (young adult stage, HsapDv:0000266) -> 30,750\n",
      "  (63-year-old stage, HsapDv:0000157) -> 30,708\n",
      "  (54-year-old stage, HsapDv:0000148) -> 29,585\n",
      "  (unknown, unknown) -> 28,634\n",
      "  (prime adult stage, HsapDv:0000226) -> 27,864\n",
      "  (sixth decade stage, HsapDv:0000240) -> 25,501\n",
      "  (10th week post-fertilization stage, HsapDv:0000047) -> 25,358\n",
      "  (62-year-old stage, HsapDv:0000156) -> 25,070\n",
      "  (3-year-old stage, HsapDv:0000097) -> 24,917\n",
      "  (65-year-old stage, HsapDv:0000159) -> 24,366\n",
      "  (middle aged stage, HsapDv:0000267) -> 24,222\n",
      "  (71-year-old stage, HsapDv:0000165) -> 23,847\n",
      "  (59-year-old stage, HsapDv:0000153) -> 23,336\n",
      "  (sixth LMP month stage, HsapDv:0000200) -> 20,475\n",
      "  (53-year-old stage, HsapDv:0000147) -> 19,407\n",
      "\n",
      "Stages containing 'adult' (if stored as English labels):\n",
      "  adult stage -> 115,964\n",
      "  young adult stage -> 30,750\n",
      "  prime adult stage -> 27,864\n",
      "  late adult stage -> 5,682\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "P = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "\n",
    "def _col(batch: pa.RecordBatch, name: str):\n",
    "    idx = batch.schema.get_field_index(name)\n",
    "    return None if idx == -1 else batch.column(idx)\n",
    "\n",
    "def _update_counter_from_value_counts(counter: Counter, arr: pa.Array):\n",
    "    # Cast to string to avoid dictionary-type edge cases\n",
    "    arr = pc.cast(arr, pa.string())\n",
    "    vc = pc.value_counts(arr)  # struct array with fields: values, counts\n",
    "    vals = vc.field(\"values\").to_pylist()\n",
    "    cts  = vc.field(\"counts\").to_pylist()\n",
    "    for v, c in zip(vals, cts):\n",
    "        if v is None:\n",
    "            continue\n",
    "        counter[v] += int(c)\n",
    "\n",
    "def scan_dev_stage(parquet_path: Path, batch_size=250_000, max_rows=2_000_000):\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "    cols = [\n",
    "        \"development_stage\",\n",
    "        \"development_stage_ontology_term_id\",\n",
    "        \"donor_age\",\n",
    "        \"donor_age_unit\",\n",
    "    ]\n",
    "    available = set(pf.schema_arrow.names)\n",
    "    cols = [c for c in cols if c in available]\n",
    "\n",
    "    if \"development_stage\" not in cols:\n",
    "        raise ValueError(\"development_stage column not found in the parquet.\")\n",
    "\n",
    "    stage_counts = Counter()\n",
    "    stage_otid_counts = Counter()\n",
    "    pair_counts = Counter()\n",
    "    adult_like_counts = Counter()\n",
    "\n",
    "    rows_scanned = 0\n",
    "    looks_like_ontid = 0\n",
    "\n",
    "    for batch in pf.iter_batches(columns=cols, batch_size=batch_size):\n",
    "        # Stop after max_rows (sample is usually enough to infer encoding)\n",
    "        if rows_scanned >= max_rows:\n",
    "            break\n",
    "\n",
    "        if rows_scanned + batch.num_rows > max_rows:\n",
    "            batch = batch.slice(0, max_rows - rows_scanned)\n",
    "\n",
    "        rows_scanned += batch.num_rows\n",
    "\n",
    "        stage = _col(batch, \"development_stage\")\n",
    "        otid  = _col(batch, \"development_stage_ontology_term_id\")\n",
    "\n",
    "        stage_str = pc.cast(stage, pa.string())\n",
    "\n",
    "        # 1) top development_stage\n",
    "        _update_counter_from_value_counts(stage_counts, stage_str)\n",
    "\n",
    "        # 2) does development_stage look like ontology IDs?\n",
    "        ont_mask = pc.match_substring_regex(stage_str, r\"^[A-Za-z]+:[0-9]+$\")\n",
    "        looks_like_ontid += int(pc.sum(pc.cast(ont_mask, pa.int64())).as_py())\n",
    "\n",
    "        # 3) any English-like \"adult\" labels?\n",
    "        adult_mask = pc.match_substring(pc.utf8_lower(stage_str), \"adult\")\n",
    "        adult_vals = pc.filter(stage_str, adult_mask)\n",
    "        if len(adult_vals) > 0:\n",
    "            _update_counter_from_value_counts(adult_like_counts, adult_vals)\n",
    "\n",
    "        # 4) stage ontology IDs + (stage, otid) pairs\n",
    "        if otid is not None:\n",
    "            otid_str = pc.cast(otid, pa.string())\n",
    "            _update_counter_from_value_counts(stage_otid_counts, otid_str)\n",
    "\n",
    "            # Pair counting without struct value_counts:\n",
    "            # Convert just this batch to Python lists (bounded by max_rows sample)\n",
    "            stage_list = stage_str.to_pylist()\n",
    "            otid_list  = otid_str.to_pylist()\n",
    "\n",
    "            for s, o in zip(stage_list, otid_list):\n",
    "                if s is None and o is None:\n",
    "                    continue\n",
    "                pair_counts[(s, o)] += 1\n",
    "\n",
    "    return {\n",
    "        \"rows_scanned\": rows_scanned,\n",
    "        \"pct_stage_looks_like_ontid\": 100.0 * looks_like_ontid / max(rows_scanned, 1),\n",
    "        \"stage_counts\": stage_counts,\n",
    "        \"stage_otid_counts\": stage_otid_counts,\n",
    "        \"pair_counts\": pair_counts,\n",
    "        \"adult_like_counts\": adult_like_counts,\n",
    "        \"columns_present\": cols,\n",
    "    }\n",
    "\n",
    "res = scan_dev_stage(P, batch_size=250_000, max_rows=2_000_000)\n",
    "\n",
    "print(\"Columns present:\", res[\"columns_present\"])\n",
    "print(\"Rows scanned:\", f\"{res['rows_scanned']:,}\")\n",
    "print(\"Pct development_stage that looks like ontology ID:\", f\"{res['pct_stage_looks_like_ontid']:.2f}%\")\n",
    "\n",
    "print(\"\\nTop development_stage:\")\n",
    "for k, v in res[\"stage_counts\"].most_common(25):\n",
    "    print(f\"  {k} -> {v:,}\")\n",
    "\n",
    "print(\"\\nTop development_stage_ontology_term_id:\")\n",
    "for k, v in res[\"stage_otid_counts\"].most_common(25):\n",
    "    print(f\"  {k} -> {v:,}\")\n",
    "\n",
    "print(\"\\nTop (development_stage, development_stage_ontology_term_id) pairs:\")\n",
    "for (s, o), n in res[\"pair_counts\"].most_common(25):\n",
    "    print(f\"  ({s}, {o}) -> {n:,}\")\n",
    "\n",
    "print(\"\\nStages containing 'adult' (if stored as English labels):\")\n",
    "for k, v in res[\"adult_like_counts\"].most_common(25):\n",
    "    print(f\"  {k} -> {v:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd47712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique development_stage entries containing 'adult':\n",
      "development_stage development_stage_ontology_term_id\n",
      "      adult stage                     HsapDv:0000258\n",
      " late adult stage                     HsapDv:0000227\n",
      "prime adult stage                     HsapDv:0000226\n",
      "young adult stage                     HsapDv:0000266\n",
      "[progress] batches_seen=10 rows_written=1,480,461\n",
      "[progress] batches_seen=20 rows_written=3,304,803\n",
      "[progress] batches_seen=30 rows_written=4,995,514\n",
      "[progress] batches_seen=40 rows_written=6,193,966\n",
      "[progress] batches_seen=50 rows_written=7,306,571\n",
      "[progress] batches_seen=60 rows_written=8,733,869\n",
      "[progress] batches_seen=70 rows_written=9,551,196\n",
      "[progress] batches_seen=80 rows_written=10,556,448\n",
      "[progress] batches_seen=90 rows_written=11,442,345\n",
      "[progress] batches_seen=110 rows_written=13,670,545\n",
      "[progress] batches_seen=120 rows_written=15,431,734\n",
      "[progress] batches_seen=140 rows_written=17,775,774\n",
      "[progress] batches_seen=150 rows_written=19,872,926\n",
      "[progress] batches_seen=160 rows_written=21,839,252\n",
      "[progress] batches_seen=170 rows_written=22,983,163\n",
      "[progress] batches_seen=180 rows_written=24,890,864\n",
      "[progress] batches_seen=190 rows_written=26,073,247\n",
      "[progress] batches_seen=200 rows_written=27,874,578\n",
      "[progress] batches_seen=220 rows_written=28,737,474\n",
      "[progress] batches_seen=230 rows_written=29,612,104\n",
      "[progress] batches_seen=240 rows_written=30,283,558\n",
      "[progress] batches_seen=260 rows_written=30,292,826\n",
      "[progress] batches_seen=270 rows_written=30,997,543\n",
      "[progress] batches_seen=280 rows_written=32,493,131\n",
      "[progress] batches_seen=290 rows_written=34,131,990\n",
      "[progress] batches_seen=300 rows_written=35,929,201\n",
      "Done. Wrote 36,866,589 rows to:\n",
      "  /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_adult15.parquet\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "INP = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "OUT = Path(\"/home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_adult15.parquet\")\n",
    "\n",
    "TARGET_ASSAYS = {\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"}\n",
    "TARGET_SEXES  = {\"male\", \"female\"}\n",
    "TARGET_SUSP   = \"cell\"\n",
    "\n",
    "# normal disease in Census:\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"\n",
    "NORMAL_DISEASE_LABEL = \"normal\"\n",
    "\n",
    "BATCH_SIZE = 250_000  # adjust up/down depending on memory\n",
    "\n",
    "\n",
    "def require_columns(pf: pq.ParquetFile, required: list[str]) -> None:\n",
    "    missing = [c for c in required if c not in pf.schema_arrow.names]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"Missing required column(s) in the Parquet file: \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\nRe-export the Census obs including these fields, or adjust the filter to available columns.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def discover_adult_labels_and_ids(pf: pq.ParquetFile) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan development_stage + ontology id and return unique pairs where stage contains 'adult'.\n",
    "    \"\"\"\n",
    "    cols = [\"development_stage\", \"development_stage_ontology_term_id\"]\n",
    "    have_otid = \"development_stage_ontology_term_id\" in pf.schema_arrow.names\n",
    "    if not have_otid:\n",
    "        cols = [\"development_stage\"]\n",
    "\n",
    "    rows = []\n",
    "    for batch in pf.iter_batches(columns=cols, batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        s = df[\"development_stage\"].astype(\"string\")\n",
    "        m = s.str.contains(\"adult\", case=False, na=False)\n",
    "        if not m.any():\n",
    "            continue\n",
    "        if have_otid:\n",
    "            rows.append(df.loc[m, [\"development_stage\", \"development_stage_ontology_term_id\"]])\n",
    "        else:\n",
    "            rows.append(df.loc[m, [\"development_stage\"]])\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True).drop_duplicates()\n",
    "    return out.sort_values(cols[0]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def adult15_mask(stage: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Adults (>=15 years) inferred from development_stage strings.\n",
    "    Includes:\n",
    "      - NN-year-old stage where NN>=15\n",
    "      - adult/young adult/prime adult/late adult labels\n",
    "      - decade stages except the first decade stage (0â€“9)\n",
    "      - middle aged stage (>=15)\n",
    "    Excludes:\n",
    "      - month-old stages, post-fertilization stages, and NN-year-old < 15\n",
    "    \"\"\"\n",
    "    s = stage.astype(\"string\")\n",
    "\n",
    "    # 1) numeric year-old stages\n",
    "    age = pd.to_numeric(s.str.extract(r\"^(\\d+)-year-old stage$\")[0], errors=\"coerce\")\n",
    "    m_year15 = age.ge(15)\n",
    "\n",
    "    # 2) explicit adult labels\n",
    "    m_adult_labels = s.isin([\"adult stage\", \"young adult stage\", \"prime adult stage\", \"late adult stage\"])\n",
    "\n",
    "    # 3) decade stages (sixth decade, seventh decade, etc.)\n",
    "    m_decade = s.str.endswith(\"decade stage\", na=False) & ~s.eq(\"first decade stage\")\n",
    "\n",
    "    # 4) middle aged stage\n",
    "    m_middle_aged = s.eq(\"middle aged stage\")\n",
    "\n",
    "    return (m_year15 | m_adult_labels | m_decade | m_middle_aged)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    pf = pq.ParquetFile(INP)\n",
    "\n",
    "    # Required columns for your filters:\n",
    "    required = [\"assay\", \"sex\", \"development_stage\"]\n",
    "    require_columns(pf, required)\n",
    "\n",
    "    # disease: allow either label or ontology id\n",
    "    has_disease_label = \"disease\" in pf.schema_arrow.names\n",
    "    has_disease_otid  = \"disease_ontology_term_id\" in pf.schema_arrow.names\n",
    "    if not (has_disease_label or has_disease_otid):\n",
    "        raise ValueError(\"Parquet lacks both 'disease' and 'disease_ontology_term_id' columns.\")\n",
    "\n",
    "    # Discover how adult is stored (optional, but useful)\n",
    "    adult_pairs = discover_adult_labels_and_ids(pf)\n",
    "    print(\"Unique development_stage entries containing 'adult':\")\n",
    "    print(adult_pairs.head(50).to_string(index=False))\n",
    "\n",
    "    # Columns to read/write (keep only what you need; expand if desired)\n",
    "    cols = [\"assay\", \"sex\", \"suspension_type\", \"development_stage\"]\n",
    "    for c in [\"disease\", \"disease_ontology_term_id\",\n",
    "              \"development_stage_ontology_term_id\",\n",
    "              \"dataset_id\", \"donor_id\", \"tissue_general\", \"cell_type\",\n",
    "              \"is_primary_data\"]:\n",
    "        if c in pf.schema_arrow.names:\n",
    "            cols.append(c)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows_written = 0\n",
    "    batches_seen = 0\n",
    "\n",
    "    try:\n",
    "        for batch in pf.iter_batches(columns=cols, batch_size=BATCH_SIZE):\n",
    "            batches_seen += 1\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # core filters\n",
    "            m = (\n",
    "                df[\"assay\"].isin(TARGET_ASSAYS) &\n",
    "                df[\"sex\"].isin(TARGET_SEXES) &\n",
    "                adult15_mask(df[\"development_stage\"])\n",
    "            )\n",
    "\n",
    "            # disease filter\n",
    "            if has_disease_otid:\n",
    "                m = m & (df[\"disease_ontology_term_id\"] == NORMAL_DISEASE_OTID)\n",
    "            else:\n",
    "                m = m & (df[\"disease\"].astype(\"string\") == NORMAL_DISEASE_LABEL)\n",
    "\n",
    "            df2 = df.loc[m].copy()\n",
    "            if df2.empty:\n",
    "                continue\n",
    "\n",
    "            tbl = pa.Table.from_pandas(df2, preserve_index=False)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(OUT), tbl.schema, compression=\"zstd\")\n",
    "            writer.write_table(tbl)\n",
    "            rows_written += tbl.num_rows\n",
    "\n",
    "            if batches_seen % 10 == 0:\n",
    "                print(f\"[progress] batches_seen={batches_seen:,} rows_written={rows_written:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    print(f\"Done. Wrote {rows_written:,} rows to:\\n  {OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9768475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 15\n",
      "\n",
      "Column names:\n",
      "  dataset_id\n",
      "  donor_id\n",
      "  assay\n",
      "  sex\n",
      "  tissue\n",
      "  tissue_general\n",
      "  cell_type\n",
      "  disease\n",
      "  development_stage\n",
      "  is_primary_data\n",
      "  tissue_ontology_term_id\n",
      "  tissue_general_ontology_term_id\n",
      "  cell_type_ontology_term_id\n",
      "  disease_ontology_term_id\n",
      "  development_stage_ontology_term_id\n",
      "\n",
      "Schema (name: type):\n",
      "dataset_id: dictionary<values=string, indices=int16, ordered=0>\n",
      "donor_id: dictionary<values=string, indices=int16, ordered=0>\n",
      "assay: dictionary<values=string, indices=int8, ordered=0>\n",
      "sex: dictionary<values=string, indices=int8, ordered=0>\n",
      "tissue: dictionary<values=string, indices=int16, ordered=0>\n",
      "tissue_general: dictionary<values=string, indices=int8, ordered=0>\n",
      "cell_type: dictionary<values=string, indices=int16, ordered=0>\n",
      "disease: dictionary<values=string, indices=int16, ordered=0>\n",
      "development_stage: dictionary<values=string, indices=int16, ordered=0>\n",
      "is_primary_data: bool\n",
      "tissue_ontology_term_id: dictionary<values=string, indices=int16, ordered=0>\n",
      "tissue_general_ontology_term_id: dictionary<values=string, indices=int8, ordered=0>\n",
      "cell_type_ontology_term_id: dictionary<values=string, indices=int16, ordered=0>\n",
      "disease_ontology_term_id: dictionary<values=string, indices=int16, ordered=0>\n",
      "development_stage_ontology_term_id: dictionary<values=string, indices=int16, ordered=0>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "p = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "pf = pq.ParquetFile(p)\n",
    "\n",
    "print(\"Number of columns:\", len(pf.schema_arrow.names))\n",
    "print(\"\\nColumn names:\")\n",
    "for c in pf.schema_arrow.names:\n",
    "    print(\" \", c)\n",
    "\n",
    "print(\"\\nSchema (name: type):\")\n",
    "print(pf.schema_arrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c7f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows scanned: 62,464,283\n",
      "Unique dataset_id: 469\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow as pa\n",
    "\n",
    "P = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "pf = pq.ParquetFile(P)\n",
    "\n",
    "# sanity check\n",
    "if \"dataset_id\" not in pf.schema_arrow.names:\n",
    "    raise ValueError(\"This parquet does not contain a 'dataset_id' column. Print schema to confirm columns.\")\n",
    "\n",
    "unique_ids = set()\n",
    "rows_scanned = 0\n",
    "\n",
    "for batch in pf.iter_batches(columns=[\"dataset_id\"], batch_size=1_000_000):\n",
    "    rows_scanned += batch.num_rows\n",
    "    col = batch.column(0)\n",
    "\n",
    "    # robustly convert to strings (handles dictionary-encoded columns)\n",
    "    col = pc.cast(col, pa.string())\n",
    "\n",
    "    # update python set\n",
    "    unique_ids.update([x for x in col.to_pylist() if x is not None])\n",
    "\n",
    "print(\"Rows scanned:\", f\"{rows_scanned:,}\")\n",
    "print(\"Unique dataset_id:\", len(unique_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4341543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 dataset_id values (may include duplicates):\n",
      "['a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8', 'a5d95a42-0137-496f-8a60-101e17f263c8']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow as pa\n",
    "\n",
    "P = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "pf = pq.ParquetFile(P)\n",
    "\n",
    "batch = next(pf.iter_batches(columns=[\"dataset_id\"], batch_size=1000))\n",
    "col = pc.cast(batch.column(0), pa.string())\n",
    "vals = [v for v in col.to_pylist() if v is not None]\n",
    "\n",
    "print(\"First 25 dataset_id values (may include duplicates):\")\n",
    "print(vals[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef012102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soma_joinid</th>\n",
       "      <th>citation</th>\n",
       "      <th>collection_id</th>\n",
       "      <th>collection_name</th>\n",
       "      <th>collection_doi</th>\n",
       "      <th>collection_doi_label</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>dataset_version_id</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_h5ad_path</th>\n",
       "      <th>dataset_total_cell_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>Publication: https://doi.org/10.1038/s41467-02...</td>\n",
       "      <td>bf325905-5e8e-42e3-933d-9a9053e9af80</td>\n",
       "      <td>Single-cell Atlas of common variable immunodef...</td>\n",
       "      <td>10.1038/s41467-022-29450-x</td>\n",
       "      <td>RodrÃ­guez-Ubreva et al. (2022) Nat Commun</td>\n",
       "      <td>a5d95a42-0137-496f-8a60-101e17f263c8</td>\n",
       "      <td>963e004b-77ac-451d-a73c-bd9e1e14c232</td>\n",
       "      <td>Steady-state B cells - scRNA-seq</td>\n",
       "      <td>a5d95a42-0137-496f-8a60-101e17f263c8.h5ad</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soma_joinid                                           citation  \\\n",
       "0           21  Publication: https://doi.org/10.1038/s41467-02...   \n",
       "\n",
       "                          collection_id  \\\n",
       "0  bf325905-5e8e-42e3-933d-9a9053e9af80   \n",
       "\n",
       "                                     collection_name  \\\n",
       "0  Single-cell Atlas of common variable immunodef...   \n",
       "\n",
       "               collection_doi                       collection_doi_label  \\\n",
       "0  10.1038/s41467-022-29450-x  RodrÃ­guez-Ubreva et al. (2022) Nat Commun   \n",
       "\n",
       "                             dataset_id                    dataset_version_id  \\\n",
       "0  a5d95a42-0137-496f-8a60-101e17f263c8  963e004b-77ac-451d-a73c-bd9e1e14c232   \n",
       "\n",
       "                      dataset_title  \\\n",
       "0  Steady-state B cells - scRNA-seq   \n",
       "\n",
       "                           dataset_h5ad_path  dataset_total_cell_count  \n",
       "0  a5d95a42-0137-496f-8a60-101e17f263c8.h5ad                      1324  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "\n",
    "dataset_id = \"a5d95a42-0137-496f-8a60-101e17f263c8\"\n",
    "\n",
    "with cellxgene_census.open_soma(census_version=\"stable\") as census:\n",
    "    ds = (\n",
    "        census[\"census_info\"][\"datasets\"]\n",
    "        .read(value_filter=f\"dataset_id == '{dataset_id}'\")\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote inventory to: /home/crizza/census_metadata_inventory\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "OUTDIR = Path.home() / \"census_metadata_inventory\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def schema_to_df(schema):\n",
    "    return pd.DataFrame({\n",
    "        \"column\": schema.names,\n",
    "        \"arrow_type\": [str(schema.field(i).type) for i in range(len(schema.names))]\n",
    "    })\n",
    "\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    # List organisms available in this Census build (e.g., homo_sapiens, mus_musculus)\n",
    "    organisms = list(census[\"census_data\"].keys())\n",
    "\n",
    "    # List all census_info tables available\n",
    "    info_tables = list(census[\"census_info\"].keys())\n",
    "    (OUTDIR / \"census_info_tables.txt\").write_text(\"\\n\".join(info_tables) + \"\\n\")\n",
    "\n",
    "    # Save schema for each organismâ€™s obs and RNA var\n",
    "    for org in organisms:\n",
    "        exp = census[\"census_data\"][org]\n",
    "\n",
    "        obs_schema = exp.obs.schema\n",
    "        schema_to_df(obs_schema).to_csv(OUTDIR / f\"{org}_obs_schema.csv\", index=False)\n",
    "\n",
    "        var_schema = exp.ms[\"RNA\"].var.schema\n",
    "        schema_to_df(var_schema).to_csv(OUTDIR / f\"{org}_rna_var_schema.csv\", index=False)\n",
    "\n",
    "    # Save schema for key census_info tables (if present)\n",
    "    for name in [\"datasets\", \"summary_cell_counts\"]:\n",
    "        if name in census[\"census_info\"]:\n",
    "            df = schema_to_df(census[\"census_info\"][name].schema)\n",
    "            df.to_csv(OUTDIR / f\"census_info_{name}_schema.csv\", index=False)\n",
    "\n",
    "print(\"Wrote inventory to:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2062e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] batches=20 rows_scanned=7,000,000 unique_pairs=133 unique_stages=133\n",
      "[scan] batches=40 rows_scanned=13,631,488 unique_pairs=158 unique_stages=158\n",
      "[scan] batches=60 rows_scanned=20,500,000 unique_pairs=171 unique_stages=171\n",
      "[scan] batches=80 rows_scanned=27,262,976 unique_pairs=181 unique_stages=181\n",
      "[scan] batches=100 rows_scanned=34,000,000 unique_pairs=182 unique_stages=182\n",
      "[scan] batches=120 rows_scanned=40,894,464 unique_pairs=182 unique_stages=182\n",
      "[scan] batches=140 rows_scanned=47,500,000 unique_pairs=182 unique_stages=182\n",
      "[scan] batches=160 rows_scanned=54,500,000 unique_pairs=185 unique_stages=185\n",
      "[scan] batches=180 rows_scanned=61,000,000 unique_pairs=186 unique_stages=186\n",
      "Done scanning. Rows scanned: 62,464,283\n",
      "Unique development_stage values: 186\n",
      "Unique (stage, otid) pairs: 186\n",
      "Unique numeric year ages: 98\n",
      "\n",
      "Wrote outputs to:\n",
      "  /home/crizza/census_exports/dev_stage_inventory/development_stage_pairs_counts.csv\n",
      "  /home/crizza/census_exports/dev_stage_inventory/year_old_stage_pairs_counts.csv\n",
      "  /home/crizza/census_exports/dev_stage_inventory/age_years_counts.csv\n",
      "  /home/crizza/census_exports/dev_stage_inventory/development_stage_counts.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "PARQUET_PATH = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "OUTDIR = PARQUET_PATH.parent / \"dev_stage_inventory\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 500_000          # adjust to your RAM (250kâ€“1M is typical)\n",
    "PROGRESS_EVERY = 20           # print progress every N batches\n",
    "\n",
    "# Regex for extracting numeric ages from labels like \"50-year-old stage\"\n",
    "YEAR_OLD_RE = re.compile(r\"^(\\d+)-year-old stage$\")\n",
    "\n",
    "\n",
    "def update_counter_from_value_counts(counter: Counter, arr: pa.Array) -> None:\n",
    "    \"\"\"Update a Python Counter using Arrow value_counts for a 1D array.\"\"\"\n",
    "    arr = pc.cast(arr, pa.string())\n",
    "    vc = pc.value_counts(arr)  # struct: values, counts\n",
    "    vals = vc.field(\"values\").to_pylist()\n",
    "    cts = vc.field(\"counts\").to_pylist()\n",
    "    for v, c in zip(vals, cts):\n",
    "        if v is None:\n",
    "            continue\n",
    "        counter[v] += int(c)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    pf = pq.ParquetFile(PARQUET_PATH)\n",
    "    cols_present = set(pf.schema_arrow.names)\n",
    "\n",
    "    required = [\"development_stage\", \"development_stage_ontology_term_id\"]\n",
    "    missing = [c for c in required if c not in cols_present]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing required column(s) in Parquet: {missing}\\n\"\n",
    "            f\"Available columns include: {sorted(list(cols_present))[:50]} ...\"\n",
    "        )\n",
    "\n",
    "    # Global tallies\n",
    "    stage_counts = Counter()\n",
    "    otid_counts = Counter()\n",
    "    pair_counts = Counter()      # key: (stage, otid) -> count\n",
    "    year_age_counts = Counter()  # key: int(age_years) -> count\n",
    "    year_stage_counts = Counter()# key: (age_years, stage, otid) -> count (mostly redundant, but explicit)\n",
    "\n",
    "    rows_scanned = 0\n",
    "    batches_seen = 0\n",
    "\n",
    "    # Iterate through the entire file streaming\n",
    "    for batch in pf.iter_batches(columns=required, batch_size=BATCH_SIZE):\n",
    "        batches_seen += 1\n",
    "        rows_scanned += batch.num_rows\n",
    "\n",
    "        stage = pc.cast(batch.column(0), pa.string())\n",
    "        otid  = pc.cast(batch.column(1), pa.string())\n",
    "\n",
    "        # (1) counts for stage and ontology ids (fast, via Arrow value_counts)\n",
    "        update_counter_from_value_counts(stage_counts, stage)\n",
    "        update_counter_from_value_counts(otid_counts, otid)\n",
    "\n",
    "        # (2) counts for (stage, otid) pairs\n",
    "        # Arrow can't value_count struct<dict,string> reliably in your build, so do per-batch groupby in pandas.\n",
    "        df = pa.Table.from_arrays([stage, otid], names=[\"development_stage\", \"development_stage_ontology_term_id\"]) \\\n",
    "                    .to_pandas()\n",
    "\n",
    "        gb = df.groupby([\"development_stage\", \"development_stage_ontology_term_id\"], dropna=False).size()\n",
    "        for (s, o), n in gb.items():\n",
    "            if pd.isna(s) and pd.isna(o):\n",
    "                continue\n",
    "            pair_counts[(s, o)] += int(n)\n",
    "\n",
    "            # (3) extract numeric ages from \"N-year-old stage\" labels and accumulate\n",
    "            # Do this on unique pair keys per batch (via gb), not per row.\n",
    "            if isinstance(s, str):\n",
    "                m = YEAR_OLD_RE.match(s)\n",
    "                if m:\n",
    "                    age = int(m.group(1))\n",
    "                    year_age_counts[age] += int(n)\n",
    "                    year_stage_counts[(age, s, o)] += int(n)\n",
    "\n",
    "        if PROGRESS_EVERY and (batches_seen % PROGRESS_EVERY == 0):\n",
    "            print(f\"[scan] batches={batches_seen:,} rows_scanned={rows_scanned:,} \"\n",
    "                  f\"unique_pairs={len(pair_counts):,} unique_stages={len(stage_counts):,}\")\n",
    "\n",
    "    print(f\"Done scanning. Rows scanned: {rows_scanned:,}\")\n",
    "    print(f\"Unique development_stage values: {len(stage_counts):,}\")\n",
    "    print(f\"Unique (stage, otid) pairs: {len(pair_counts):,}\")\n",
    "    print(f\"Unique numeric year ages: {len(year_age_counts):,}\")\n",
    "\n",
    "    # ---- Write outputs ----\n",
    "\n",
    "    # A) full catalog of (stage, otid) pairs with counts\n",
    "    pairs_df = pd.DataFrame(\n",
    "        [(s, o, n) for (s, o), n in pair_counts.items()],\n",
    "        columns=[\"development_stage\", \"development_stage_ontology_term_id\", \"cell_count_in_parquet\"]\n",
    "    ).sort_values(\"cell_count_in_parquet\", ascending=False)\n",
    "    pairs_csv = OUTDIR / \"development_stage_pairs_counts.csv\"\n",
    "    pairs_df.to_csv(pairs_csv, index=False)\n",
    "\n",
    "    # B) numeric year-old stages with ontology ids + counts (age resolved)\n",
    "    year_df = pd.DataFrame(\n",
    "        [(age, s, o, n) for (age, s, o), n in year_stage_counts.items()],\n",
    "        columns=[\"age_years\", \"development_stage\", \"development_stage_ontology_term_id\", \"cell_count_in_parquet\"]\n",
    "    ).sort_values([\"age_years\", \"cell_count_in_parquet\"], ascending=[True, False])\n",
    "    year_csv = OUTDIR / \"year_old_stage_pairs_counts.csv\"\n",
    "    year_df.to_csv(year_csv, index=False)\n",
    "\n",
    "    # C) collapsed age summary (counts by numeric age)\n",
    "    age_df = pd.DataFrame(\n",
    "        sorted(year_age_counts.items(), key=lambda x: x[0]),\n",
    "        columns=[\"age_years\", \"cell_count_in_parquet\"]\n",
    "    )\n",
    "    age_csv = OUTDIR / \"age_years_counts.csv\"\n",
    "    age_df.to_csv(age_csv, index=False)\n",
    "\n",
    "    # D) also write a quick â€œstages onlyâ€ list (no otid) if helpful\n",
    "    stage_df = pd.DataFrame(\n",
    "        stage_counts.most_common(),\n",
    "        columns=[\"development_stage\", \"cell_count_in_parquet\"]\n",
    "    )\n",
    "    stage_csv = OUTDIR / \"development_stage_counts.csv\"\n",
    "    stage_df.to_csv(stage_csv, index=False)\n",
    "\n",
    "    print(\"\\nWrote outputs to:\")\n",
    "    print(\" \", pairs_csv)\n",
    "    print(\" \", year_csv)\n",
    "    print(\" \", age_csv)\n",
    "    print(\" \", stage_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a1d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soma_joinid', 'citation', 'collection_id', 'collection_name', 'collection_doi', 'collection_doi_label', 'dataset_id', 'dataset_version_id', 'dataset_title', 'dataset_h5ad_path', 'dataset_total_cell_count']\n",
      "                                                                          0  \\\n",
      "soma_joinid                                                               0   \n",
      "citation                  Publication: https://doi.org/10.1016/j.isci.20...   \n",
      "collection_id                          8e880741-bf9a-4c8e-9227-934204631d2a   \n",
      "collection_name           High Resolution Slide-seqV2 Spatial Transcript...   \n",
      "collection_doi                                   10.1016/j.isci.2022.104097   \n",
      "collection_doi_label                        Marshall et al. (2022) iScience   \n",
      "dataset_id                             4eb29386-de81-452f-b3c0-e00844e8c7fd   \n",
      "dataset_version_id                     66699060-0389-4fbd-b3a5-196b3b4e32d6   \n",
      "dataset_title              Spatial transcriptomics in mouse: Puck_191112_05   \n",
      "dataset_h5ad_path                 4eb29386-de81-452f-b3c0-e00844e8c7fd.h5ad   \n",
      "dataset_total_cell_count                                              10888   \n",
      "\n",
      "                                                                          1  \\\n",
      "soma_joinid                                                               1   \n",
      "citation                  Publication: https://doi.org/10.1016/j.isci.20...   \n",
      "collection_id                          8e880741-bf9a-4c8e-9227-934204631d2a   \n",
      "collection_name           High Resolution Slide-seqV2 Spatial Transcript...   \n",
      "collection_doi                                   10.1016/j.isci.2022.104097   \n",
      "collection_doi_label                        Marshall et al. (2022) iScience   \n",
      "dataset_id                             78d59e4a-82eb-4a61-a1dc-da974d7ea54b   \n",
      "dataset_version_id                     f64950a2-a3c8-490a-8431-7121eeb4f5f4   \n",
      "dataset_title              Spatial transcriptomics in mouse: Puck_191112_08   \n",
      "dataset_h5ad_path                 78d59e4a-82eb-4a61-a1dc-da974d7ea54b.h5ad   \n",
      "dataset_total_cell_count                                              10250   \n",
      "\n",
      "                                                                          2  \n",
      "soma_joinid                                                               2  \n",
      "citation                  Publication: https://doi.org/10.1016/j.isci.20...  \n",
      "collection_id                          8e880741-bf9a-4c8e-9227-934204631d2a  \n",
      "collection_name           High Resolution Slide-seqV2 Spatial Transcript...  \n",
      "collection_doi                                   10.1016/j.isci.2022.104097  \n",
      "collection_doi_label                        Marshall et al. (2022) iScience  \n",
      "dataset_id                             add5eb84-5fc9-4f01-982e-a346dd42ee82  \n",
      "dataset_version_id                     781a724a-b0f5-46c4-9a13-e6293ef4364f  \n",
      "dataset_title              Spatial transcriptomics in mouse: Puck_191109_20  \n",
      "dataset_h5ad_path                 add5eb84-5fc9-4f01-982e-a346dd42ee82.h5ad  \n",
      "dataset_total_cell_count                                              12906  \n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census as cxc\n",
    "\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    ds = census[\"census_info\"][\"datasets\"].read().concat().to_pandas()\n",
    "\n",
    "print(ds.columns.tolist())\n",
    "print(ds.head(3).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c186732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of development_stage term IDs for age >= 15: 100\n",
      "Example IDs: ['HsapDv:0000095', 'HsapDv:0000109', 'HsapDv:0000110', 'HsapDv:0000111', 'HsapDv:0000112', 'HsapDv:0000113', 'HsapDv:0000114', 'HsapDv:0000115', 'HsapDv:0000116', 'HsapDv:0000117']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "INVENTORY_DIR = Path(\"/home/crizza/census_exports/dev_stage_inventory\")\n",
    "\n",
    "year_pairs = pd.read_csv(INVENTORY_DIR / \"year_old_stage_pairs_counts.csv\")\n",
    "pairs      = pd.read_csv(INVENTORY_DIR / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "\n",
    "# 1) All exact \"N-year-old stage\" where N >= 15\n",
    "ids_ge15 = set(\n",
    "    year_pairs.loc[year_pairs[\"age_years\"] >= 15, \"development_stage_ontology_term_id\"]\n",
    ")\n",
    "\n",
    "# 2) Add non-1-year-bin adult-ish buckets/ranges that show up in Census exports\n",
    "extra_labels = [\n",
    "    \"15-19 year-old\",\n",
    "    \"adult stage\",\n",
    "    \"young adult stage\",\n",
    "    \"prime adult stage\",\n",
    "    \"late adult stage\",\n",
    "    \"middle aged stage\",\n",
    "    \"third decade stage\",\n",
    "    \"fourth decade stage\",\n",
    "    \"fifth decade stage\",\n",
    "    \"sixth decade stage\",\n",
    "    \"seventh decade stage\",\n",
    "    \"eighth decade stage\",\n",
    "    \"ninth decade stage\",\n",
    "    \"60-79 year-old stage\",\n",
    "    \"80 year-old and over stage\",\n",
    "    \"90 year-old and over stage\",\n",
    "]\n",
    "ids_ge15 |= set(\n",
    "    pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    ")\n",
    "\n",
    "# 3) Clean-up\n",
    "ids_ge15.discard(\"unknown\")\n",
    "ids_ge15 = sorted(ids_ge15)\n",
    "\n",
    "print(\"Number of development_stage term IDs for age >= 15:\", len(ids_ge15))\n",
    "print(\"Example IDs:\", ids_ge15[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c332b967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined OBS value_filter:\n",
      " is_primary_data == True and disease_ontology_term_id == 'PATO:0000461' and sex in ['male', 'female'] and suspension_type == 'cell' and assay_ontology_term_id in ['EFO:0009899', 'EFO:0009922', 'EFO:0030059'] and development_stage_ontology_term_id in ['HsapDv:0000095', 'HsapDv:0000109', 'HsapDv:0000110', 'HsapDv:0000111', 'HsapDv:0000112', 'HsapDv:0000113', 'HsapDv:0000114', 'HsapDv:0000115', 'HsapDv:0000116', 'HsapDv:0000117', 'HsapDv:0000118', 'HsapDv:0000119', 'HsapDv:0000120', 'HsapDv:0000121', 'HsapDv:0000122', 'HsapDv:0000123', 'HsapDv:0000124', 'HsapDv:0000125', 'HsapDv:0000126', 'HsapDv:0000127', 'HsapDv:0000128', 'HsapDv:0000129', 'HsapDv:0000130', 'HsapDv:0000131', 'HsapDv:0000132', 'HsapDv:0000133', 'HsapDv:0000134', 'HsapDv:0000135', 'HsapDv:0000136', 'HsapDv:0000137', 'HsapDv:0000138', 'HsapDv:0000139', 'HsapDv:0000140', 'HsapDv:0000141', 'HsapDv:0000142', 'HsapDv:0000143', 'HsapDv:0000144', 'HsapDv:0000145', 'HsapDv:0000146', 'HsapDv:0000147', 'HsapDv:0000148', 'HsapDv:0000149', 'HsapDv:0000150', 'HsapDv:0000151', 'HsapDv:0000152', 'HsapDv:0000153', 'HsapDv:0000154', 'HsapDv:0000155', 'HsapDv:0000156', 'HsapDv:0000157', 'HsapDv:0000158', 'HsapDv:0000159', 'HsapDv:0000160', 'HsapDv:0000161', 'HsapDv:0000162', 'HsapDv:0000163', 'HsapDv:0000164', 'HsapDv:0000165', 'HsapDv:0000166', 'HsapDv:0000167', 'HsapDv:0000168', 'HsapDv:0000169', 'HsapDv:0000170', 'HsapDv:0000171', 'HsapDv:0000172', 'HsapDv:0000173', 'HsapDv:0000206', 'HsapDv:0000207', 'HsapDv:0000208', 'HsapDv:0000209', 'HsapDv:0000210', 'HsapDv:0000211', 'HsapDv:0000212', 'HsapDv:0000213', 'HsapDv:0000214', 'HsapDv:0000215', 'HsapDv:0000216', 'HsapDv:0000217', 'HsapDv:0000218', 'HsapDv:0000219', 'HsapDv:0000220', 'HsapDv:0000221', 'HsapDv:0000222', 'HsapDv:0000223', 'HsapDv:0000226', 'HsapDv:0000227', 'HsapDv:0000229', 'HsapDv:0000237', 'HsapDv:0000238', 'HsapDv:0000239', 'HsapDv:0000240', 'HsapDv:0000241', 'HsapDv:0000242', 'HsapDv:0000243', 'HsapDv:0000258', 'HsapDv:0000266', 'HsapDv:0000267', 'HsapDv:0000268', 'HsapDv:0000272', 'HsapDv:0000274']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# User parameters\n",
    "# -----------------------------\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"  # normal\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "TARGET_SEXES = [\"male\", \"female\"]\n",
    "TARGET_SUSPENSION_TYPE = \"cell\"\n",
    "\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# Point this to where you wrote the inventory CSVs\n",
    "INVENTORY_DIR = Path(\"/home/crizza/census_exports/dev_stage_inventory\")\n",
    "# (If you are using the uploaded files in this chat, set INVENTORY_DIR = Path(\"/mnt/data\"))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _format_in_list(values: List[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def build_stage_ids_ge15(inventory_dir: Path, min_age_years: int = 15) -> List[str]:\n",
    "    \"\"\"\n",
    "    Construct a development_stage_ontology_term_id allow-list for age >= min_age_years\n",
    "    using your inventory CSVs:\n",
    "      - year_old_stage_pairs_counts.csv\n",
    "      - development_stage_pairs_counts.csv\n",
    "    \"\"\"\n",
    "    year_pairs = pd.read_csv(inventory_dir / \"year_old_stage_pairs_counts.csv\")\n",
    "    pairs = pd.read_csv(inventory_dir / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "    # 1) All exact N-year-old stages where N >= 15\n",
    "    ids = set(\n",
    "        year_pairs.loc[year_pairs[\"age_years\"] >= min_age_years, \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # 2) Add adult/older bins and labels (these appear in your file)\n",
    "    extra_labels = [\n",
    "        \"15-19 year-old\",\n",
    "        \"adult stage\",\n",
    "        \"young adult stage\",\n",
    "        \"prime adult stage\",\n",
    "        \"late adult stage\",\n",
    "        \"middle aged stage\",\n",
    "        \"third decade stage\",\n",
    "        \"fourth decade stage\",\n",
    "        \"fifth decade stage\",\n",
    "        \"sixth decade stage\",\n",
    "        \"seventh decade stage\",\n",
    "        \"eighth decade stage\",\n",
    "        \"ninth decade stage\",\n",
    "        \"60-79 year-old stage\",\n",
    "        \"80 year-old and over stage\",\n",
    "        \"90 year-old and over stage\",\n",
    "    ]\n",
    "\n",
    "    extra_ids = (\n",
    "        pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    ids.update(extra_ids)\n",
    "\n",
    "    # 3) Remove unknown (if present)\n",
    "    ids.discard(\"unknown\")\n",
    "\n",
    "    return sorted(ids)\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id using census_info/summary_cell_counts,\n",
    "    avoiding quoting issues with \"10x 3' v3\".\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(TARGET_ASSAY_LABELS))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in TARGET_ASSAY_LABELS if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for assay label(s): {missing}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def build_obs_value_filter(census, inventory_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Build a single combined obs value_filter implementing:\n",
    "      - normal\n",
    "      - male/female\n",
    "      - suspension_type == cell\n",
    "      - assay is 10x v2/v3/multiome (via ontology IDs)\n",
    "      - development_stage >= 15 years (via allow-list of stage ontology IDs)\n",
    "    \"\"\"\n",
    "    # Resolve assay ontology IDs\n",
    "    assay_map = lookup_assay_term_ids(census)\n",
    "    assay_term_ids = [assay_map[x] for x in TARGET_ASSAY_LABELS]\n",
    "\n",
    "    # Resolve developmental-stage ontology IDs for >=15 years\n",
    "    stage_ids_ge15 = build_stage_ids_ge15(inventory_dir, MIN_AGE_YEARS)\n",
    "\n",
    "    vf_parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{NORMAL_DISEASE_OTID}'\",\n",
    "        f\"sex in {_format_in_list(TARGET_SEXES)}\",\n",
    "        f\"suspension_type == '{TARGET_SUSPENSION_TYPE}'\",\n",
    "        f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "        f\"development_stage_ontology_term_id in {_format_in_list(stage_ids_ge15)}\",\n",
    "    ]\n",
    "    return \" and \".join(vf_parts)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example: produce filter and use it\n",
    "# -----------------------------\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    human = census[\"census_data\"][ORGANISM]\n",
    "\n",
    "    value_filter = build_obs_value_filter(census, INVENTORY_DIR)\n",
    "    print(\"Combined OBS value_filter:\\n\", value_filter)\n",
    "\n",
    "    # Example read (cell metadata only)\n",
    "    # tbl_iter = human.obs.read(value_filter=value_filter, column_names=[...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca0b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inventory CSV directory: /home/crizza/census_exports/dev_stage_inventory\n",
      "Output file: /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined OBS value_filter:\n",
      " is_primary_data == True and disease_ontology_term_id == 'PATO:0000461' and sex in ['male', 'female'] and suspension_type == 'cell' and assay_ontology_term_id in ['EFO:0009899', 'EFO:0009922', 'EFO:0030059'] and development_stage_ontology_term_id in ['HsapDv:0000095', 'HsapDv:0000109', 'HsapDv:0000110', 'HsapDv:0000111', 'HsapDv:0000112', 'HsapDv:0000113', 'HsapDv:0000114', 'HsapDv:0000115', 'HsapDv:0000116', 'HsapDv:0000117', 'HsapDv:0000118', 'HsapDv:0000119', 'HsapDv:0000120', 'HsapDv:0000121', 'HsapDv:0000122', 'HsapDv:0000123', 'HsapDv:0000124', 'HsapDv:0000125', 'HsapDv:0000126', 'HsapDv:0000127', 'HsapDv:0000128', 'HsapDv:0000129', 'HsapDv:0000130', 'HsapDv:0000131', 'HsapDv:0000132', 'HsapDv:0000133', 'HsapDv:0000134', 'HsapDv:0000135', 'HsapDv:0000136', 'HsapDv:0000137', 'HsapDv:0000138', 'HsapDv:0000139', 'HsapDv:0000140', 'HsapDv:0000141', 'HsapDv:0000142', 'HsapDv:0000143', 'HsapDv:0000144', 'HsapDv:0000145', 'HsapDv:0000146', 'HsapDv:0000147', 'HsapDv:0000148', 'HsapDv:0000149', 'HsapDv:0000150', 'HsapDv:0000151', 'HsapDv:0000152', 'HsapDv:0000153', 'HsapDv:0000154', 'HsapDv:0000155', 'HsapDv:0000156', 'HsapDv:0000157', 'HsapDv:0000158', 'HsapDv:0000159', 'HsapDv:0000160', 'HsapDv:0000161', 'HsapDv:0000162', 'HsapDv:0000163', 'HsapDv:0000164', 'HsapDv:0000165', 'HsapDv:0000166', 'HsapDv:0000167', 'HsapDv:0000168', 'HsapDv:0000169', 'HsapDv:0000170', 'HsapDv:0000171', 'HsapDv:0000172', 'HsapDv:0000173', 'HsapDv:0000206', 'HsapDv:0000207', 'HsapDv:0000208', 'HsapDv:0000209', 'HsapDv:0000210', 'HsapDv:0000211', 'HsapDv:0000212', 'HsapDv:0000213', 'HsapDv:0000214', 'HsapDv:0000215', 'HsapDv:0000216', 'HsapDv:0000217', 'HsapDv:0000218', 'HsapDv:0000219', 'HsapDv:0000220', 'HsapDv:0000221', 'HsapDv:0000222', 'HsapDv:0000223', 'HsapDv:0000226', 'HsapDv:0000227', 'HsapDv:0000229', 'HsapDv:0000237', 'HsapDv:0000238', 'HsapDv:0000239', 'HsapDv:0000240', 'HsapDv:0000241', 'HsapDv:0000242', 'HsapDv:0000243', 'HsapDv:0000258', 'HsapDv:0000266', 'HsapDv:0000267', 'HsapDv:0000268', 'HsapDv:0000272', 'HsapDv:0000274'] \n",
      "\n",
      "Done. Wrote 20,327,572 rows across 1 batches to:\n",
      "  /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "### Current Version \n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# Filters requested\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"  # normal\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "TARGET_SEXES = [\"male\", \"female\"]\n",
    "TARGET_SUSPENSION_TYPE = \"cell\"\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# Inventory CSVs directory (set this explicitly if you know it)\n",
    "# Example: Path(\"/home/crizza/census_exports/dev_stage_inventory\")\n",
    "INVENTORY_DIR: Optional[Path] = None\n",
    "\n",
    "# Output\n",
    "OUTDIR = Path(os.environ.get(\"CENSUS_OUTDIR\", str(Path.home() / \"census_exports\")))\n",
    "OUTFILE = \"census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Columns to export (keep compact; anything missing will be skipped safely)\n",
    "OBS_COLS = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    \"sex\",\n",
    "    \"suspension_type\",\n",
    "    \"disease\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"is_primary_data\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and OVERWRITE=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def _format_in_list(values: List[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def detect_inventory_dir(user_hint: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Locate the folder containing:\n",
    "      - year_old_stage_pairs_counts.csv\n",
    "      - development_stage_pairs_counts.csv\n",
    "    \"\"\"\n",
    "    required = [\"year_old_stage_pairs_counts.csv\", \"development_stage_pairs_counts.csv\"]\n",
    "\n",
    "    candidates = []\n",
    "    if user_hint is not None:\n",
    "        candidates.append(user_hint)\n",
    "\n",
    "    # common locations\n",
    "    candidates.extend([\n",
    "        Path(\"/home/crizza/census_exports/dev_stage_inventory\"),\n",
    "        Path.home() / \"census_exports\" / \"dev_stage_inventory\",\n",
    "        Path(\"/mnt/data\"),  # in-chat uploads\n",
    "    ])\n",
    "\n",
    "    for d in candidates:\n",
    "        if all((d / f).exists() for f in required):\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find inventory CSVs. Set INVENTORY_DIR to the folder containing:\\n\"\n",
    "        \"  - year_old_stage_pairs_counts.csv\\n\"\n",
    "        \"  - development_stage_pairs_counts.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stage_ids_ge15(inventory_dir: Path, min_age_years: int = 15) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build development_stage_ontology_term_id allow-list for age >= min_age_years using your CSV inventories.\n",
    "    - Includes all N-year-old stages where N >= min_age_years\n",
    "    - Adds adult bins/labels seen in your data (adult stage, decades, 15-19, 80+ etc.)\n",
    "    \"\"\"\n",
    "    year_pairs = pd.read_csv(inventory_dir / \"year_old_stage_pairs_counts.csv\")\n",
    "    pairs = pd.read_csv(inventory_dir / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "    ids = set(\n",
    "        year_pairs.loc[year_pairs[\"age_years\"] >= min_age_years, \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    extra_labels = [\n",
    "        \"15-19 year-old\",\n",
    "        \"adult stage\",\n",
    "        \"young adult stage\",\n",
    "        \"prime adult stage\",\n",
    "        \"late adult stage\",\n",
    "        \"middle aged stage\",\n",
    "        \"third decade stage\",\n",
    "        \"fourth decade stage\",\n",
    "        \"fifth decade stage\",\n",
    "        \"sixth decade stage\",\n",
    "        \"seventh decade stage\",\n",
    "        \"eighth decade stage\",\n",
    "        \"ninth decade stage\",\n",
    "        \"60-79 year-old stage\",\n",
    "        \"80 year-old and over stage\",\n",
    "        \"90 year-old and over stage\",\n",
    "    ]\n",
    "\n",
    "    extra_ids = (\n",
    "        pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    ids.update(extra_ids)\n",
    "\n",
    "    ids.discard(\"unknown\")\n",
    "    return sorted(ids)\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id via census_info/summary_cell_counts.\n",
    "    This avoids issues with apostrophes in assay labels.\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(TARGET_ASSAY_LABELS))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in TARGET_ASSAY_LABELS if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for assay label(s): {missing}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def pick_existing_cols(schema_names: List[str], desired: List[str]) -> List[str]:\n",
    "    s = set(schema_names)\n",
    "    return [c for c in desired if c in s]\n",
    "\n",
    "\n",
    "def build_obs_value_filter(\n",
    "    census,\n",
    "    inventory_dir: Path,\n",
    ") -> str:\n",
    "    assay_map = lookup_assay_term_ids(census)\n",
    "    assay_term_ids = [assay_map[x] for x in TARGET_ASSAY_LABELS]\n",
    "    stage_ids_ge15 = build_stage_ids_ge15(inventory_dir, MIN_AGE_YEARS)\n",
    "\n",
    "    vf_parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{NORMAL_DISEASE_OTID}'\",\n",
    "        f\"sex in {_format_in_list(TARGET_SEXES)}\",\n",
    "        f\"suspension_type == '{TARGET_SUSPENSION_TYPE}'\",\n",
    "        f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "        f\"development_stage_ontology_term_id in {_format_in_list(stage_ids_ge15)}\",\n",
    "    ]\n",
    "    return \" and \".join(vf_parts)\n",
    "\n",
    "\n",
    "def export_obs_to_parquet(\n",
    "    exp,\n",
    "    value_filter: str,\n",
    "    outpath: Path,\n",
    "    obs_cols: Optional[List[str]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Stream obs rows matching value_filter and write to a single parquet.\n",
    "    No BatchSize required; TileDB-SOMA will still yield an iterator of Arrow tables.\n",
    "    \"\"\"\n",
    "    schema_names = exp.obs.schema.names\n",
    "    cols = pick_existing_cols(schema_names, obs_cols) if obs_cols else None\n",
    "\n",
    "    it = exp.obs.read(value_filter=value_filter, column_names=cols)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "\n",
    "    try:\n",
    "        for tbl in it:\n",
    "            if not isinstance(tbl, pa.Table):\n",
    "                tbl = pa.Table.from_batches([tbl])\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=\"zstd\")\n",
    "\n",
    "            writer.write_table(tbl)\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "\n",
    "            if batches % 10 == 0:\n",
    "                print(f\"[export] batches={batches:,} rows={rows:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    print(f\"Done. Wrote {rows:,} rows across {batches:,} batches to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    inventory_dir = detect_inventory_dir(INVENTORY_DIR)\n",
    "    print(\"Using inventory CSV directory:\", inventory_dir)\n",
    "\n",
    "    outpath = resolve_outpath(OUTDIR, OUTFILE, OVERWRITE)\n",
    "    print(\"Output file:\", outpath)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        exp = census[\"census_data\"][ORGANISM]\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, inventory_dir)\n",
    "        print(\"\\nCombined OBS value_filter:\\n\", value_filter, \"\\n\")\n",
    "\n",
    "        export_obs_to_parquet(exp, value_filter, outpath, obs_cols=OBS_COLS)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c5c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows scanned: 20,327,572\n",
      "Unique dataset_id: 165\n",
      "Example dataset_id values:\n",
      "  0087cde2-967d-4f7c-8e6e-40e4c9ad1891\n",
      "  01209dce-3575-4bed-b1df-129f57fbc031\n",
      "  019c7af2-c827-4454-9970-44d5e39ce068\n",
      "  0436a180-cb44-47ba-8ffa-807b7a468469\n",
      "  093d3bfe-6f0f-4ac0-a7a1-829f94d0a49f\n",
      "  0b4a15a7-4e9e-4555-9733-2423e5c66469\n",
      "  0ba636a1-4754-4786-a8be-7ab3cf760fd6\n",
      "  0bae7ebf-eb54-46a6-be9a-3461cecefa4c\n",
      "  0c9a8cfb-6649-4d52-b418-6d8e56bd7afe\n",
      "  0de831e0-a525-4ec5-b717-df56f2de2bf0\n",
      "  0f4865d5-8000-4f68-8ac7-f5efea9e5e70\n",
      "  124744b8-4681-474a-9894-683896122708\n",
      "  12c3efd8-d515-42e6-b93c-4961e599d524\n",
      "  14363b6e-3428-45b0-8704-10fd5d6316df\n",
      "  149b2c3f-ee11-47a7-984b-923570280bd7\n",
      "  15d374d6-0dfd-4d8e-ade7-81f73dc921ee\n",
      "  16023185-de21-4c0d-a9c8-73abdd52d142\n",
      "  171b03c9-516b-499c-a434-fa4f70191988\n",
      "  19053a82-9c89-4fb8-bd19-d7b1800b0b7b\n",
      "  19e46756-9100-4e01-8b0e-23b557558a4c\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "P = Path(\"/home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\")\n",
    "pf = pq.ParquetFile(P)\n",
    "\n",
    "if \"dataset_id\" not in pf.schema_arrow.names:\n",
    "    raise ValueError(\"Column 'dataset_id' not found. Print pf.schema_arrow.names to confirm.\")\n",
    "\n",
    "unique_ids = set()\n",
    "rows_scanned = 0\n",
    "\n",
    "for batch in pf.iter_batches(columns=[\"dataset_id\"], batch_size=1_000_000):\n",
    "    rows_scanned += batch.num_rows\n",
    "    col = pc.cast(batch.column(0), pa.string())\n",
    "\n",
    "    vc = pc.value_counts(col)  # struct array with fields: values, counts\n",
    "    vals = vc.field(\"values\").to_pylist()\n",
    "    unique_ids.update([v for v in vals if v is not None])\n",
    "\n",
    "print(\"Rows scanned:\", f\"{rows_scanned:,}\")\n",
    "print(\"Unique dataset_id:\", len(unique_ids))\n",
    "\n",
    "# show examples (optional)\n",
    "examples = sorted(unique_ids)[:20]\n",
    "print(\"Example dataset_id values:\")\n",
    "for x in examples:\n",
    "    print(\" \", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2054221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dataset_id: 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/crizza/census_exports/dataset_metadata_for_filtered165.csv\n",
      "Rows: 165\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "PARQ = Path(\"/home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\")\n",
    "OUT  = Path(\"/home/crizza/census_exports/dataset_metadata_for_filtered165.csv\")\n",
    "\n",
    "# 1) Collect unique dataset_ids from parquet\n",
    "pf = pq.ParquetFile(PARQ)\n",
    "unique_ids = set()\n",
    "\n",
    "for batch in pf.iter_batches(columns=[\"dataset_id\"], batch_size=1_000_000):\n",
    "    col = pc.cast(batch.column(0), pa.string())\n",
    "    vc = pc.value_counts(col)\n",
    "    vals = vc.field(\"values\").to_pylist()\n",
    "    unique_ids.update([v for v in vals if v is not None])\n",
    "\n",
    "ids = sorted(unique_ids)\n",
    "print(\"Unique dataset_id:\", len(ids))\n",
    "\n",
    "# 2) Fetch dataset metadata from census_info[\"datasets\"]\n",
    "def chunks(lst, n=250):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "rows = []\n",
    "with cxc.open_soma(census_version=\"stable\") as census:\n",
    "    ds_tbl = census[\"census_info\"][\"datasets\"]\n",
    "\n",
    "    for ch in chunks(ids, 250):\n",
    "        clause = \" or \".join([f\"dataset_id == '{x}'\" for x in ch])\n",
    "        df = ds_tbl.read(value_filter=clause).concat().to_pandas()\n",
    "        rows.append(df)\n",
    "\n",
    "meta = pd.concat(rows, ignore_index=True).drop_duplicates(subset=[\"dataset_id\"])\n",
    "\n",
    "# Keep useful columns if present\n",
    "keep = [c for c in [\n",
    "    \"dataset_id\",\n",
    "    \"dataset_title\",\n",
    "    \"dataset_total_cell_count\",\n",
    "    \"collection_id\",\n",
    "    \"collection_name\",\n",
    "    \"collection_doi\",\n",
    "    \"collection_doi_label\",\n",
    "    \"dataset_version_id\",\n",
    "    \"citation\",\n",
    "] if c in meta.columns]\n",
    "\n",
    "meta = meta[keep].sort_values([\"collection_name\", \"dataset_title\"])\n",
    "meta.to_csv(OUT, index=False)\n",
    "\n",
    "print(\"Wrote:\", OUT)\n",
    "print(\"Rows:\", meta.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0243b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "### Current Version \n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# Filters requested\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"  # normal\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "TARGET_SEXES = [\"male\", \"female\"]\n",
    "TARGET_SUSPENSION_TYPE = \"cell\"\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# Inventory CSVs directory (set this explicitly if you know it)\n",
    "# Example: Path(\"/home/crizza/census_exports/dev_stage_inventory\")\n",
    "INVENTORY_DIR: Optional[Path] = None\n",
    "\n",
    "# Output\n",
    "OUTDIR = Path(os.environ.get(\"CENSUS_OUTDIR\", str(Path.home() / \"census_exports\")))\n",
    "OUTFILE = \"census_hsapiens_normal_cell_mf_10x_age15plus_obs.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Columns to export (keep compact; anything missing will be skipped safely)\n",
    "OBS_COLS = [\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"assay\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    \"sex\",\n",
    "    \"suspension_type\",\n",
    "    \"disease\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"development_stage\",\n",
    "    \"development_stage_ontology_term_id\",\n",
    "    \"tissue\",\n",
    "    \"tissue_general\",\n",
    "    \"cell_type\",\n",
    "    \"is_primary_data\",\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def list_obs_columns(census, organism: str) -> List[str]:\n",
    "    \"\"\"Return all available obs column names for this organism in the current Census version.\"\"\"\n",
    "    exp = census[\"census_data\"][organism]\n",
    "    return list(exp.obs.schema.names)\n",
    "\n",
    "\n",
    "def build_obs_columns_no_ontology(\n",
    "    all_obs_cols: List[str],\n",
    "    require: Optional[List[str]] = None,\n",
    "    drop_soma_internal: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Keep all obs columns except ontology term IDs.\n",
    "    Force-include any 'require' fields if present.\n",
    "    \"\"\"\n",
    "    require = require or []\n",
    "\n",
    "    # Drop ontology ID columns\n",
    "    cols = [c for c in all_obs_cols if \"ontology_term_id\" not in c]\n",
    "\n",
    "    # Optionally drop internal SOMA join index\n",
    "    if drop_soma_internal:\n",
    "        cols = [c for c in cols if c != \"soma_joinid\"]\n",
    "\n",
    "    # Ensure required fields exist\n",
    "    missing = [c for c in require if c not in all_obs_cols]\n",
    "    if missing:\n",
    "        # Provide a helpful hint for near-matches\n",
    "        hints = []\n",
    "        for m in missing:\n",
    "            near = [c for c in all_obs_cols if m.split(\"_\")[0] in c]\n",
    "            hints.append((m, near[:20]))\n",
    "        msg = \"Required obs columns not found in this Census build:\\n\"\n",
    "        msg += \"\\n\".join([f\"  - {m} (near matches: {near})\" for m, near in hints])\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Force-include required fields (and preserve order)\n",
    "    for r in require:\n",
    "        if r not in cols:\n",
    "            cols.append(r)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(value_filter: str, out_h5ad: Path) -> None:\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        all_obs_cols = list_obs_columns(census, ORGANISM)\n",
    "        obs_cols = build_obs_columns_no_ontology(\n",
    "            all_obs_cols,\n",
    "            require=REQUIRE_OBS_FIELDS,\n",
    "            drop_soma_internal=True,\n",
    "        )\n",
    "\n",
    "        # Optional: choose a minimal var schema; or omit var_column_names to take defaults.\n",
    "        # If you want *all* genes, omit var_column_names; by default Census fetches all genes. :contentReference[oaicite:2]{index=2}\n",
    "        var_cols = [\"feature_id\", \"feature_name\"]\n",
    "\n",
    "        adata = cxc.get_anndata(\n",
    "            census,\n",
    "            organism=ORGANISM,\n",
    "            obs_value_filter=value_filter,\n",
    "            obs_column_names=obs_cols,\n",
    "            var_column_names=var_cols,\n",
    "        )  # :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "    # Defensive: ensure no ontology ID columns slipped in\n",
    "    drop_cols = [c for c in adata.obs.columns if \"ontology_term_id\" in c]\n",
    "    if drop_cols:\n",
    "        adata.obs.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # Write\n",
    "    out_h5ad.parent.mkdir(parents=True, exist_ok=True)\n",
    "    adata.write_h5ad(out_h5ad)\n",
    "    print(f\"Wrote: {out_h5ad}  (cells={adata.n_obs:,}, genes={adata.n_vars:,})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca424d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inventory CSV directory: /home/crizza/census_exports/dev_stage_inventory\n",
      "Output file: /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined OBS value_filter:\n",
      " is_primary_data == True and disease_ontology_term_id == 'PATO:0000461' and sex in ['male', 'female'] and suspension_type == 'cell' and assay_ontology_term_id in ['EFO:0009899', 'EFO:0009922', 'EFO:0030059'] and development_stage_ontology_term_id in ['HsapDv:0000095', 'HsapDv:0000109', 'HsapDv:0000110', 'HsapDv:0000111', 'HsapDv:0000112', 'HsapDv:0000113', 'HsapDv:0000114', 'HsapDv:0000115', 'HsapDv:0000116', 'HsapDv:0000117', 'HsapDv:0000118', 'HsapDv:0000119', 'HsapDv:0000120', 'HsapDv:0000121', 'HsapDv:0000122', 'HsapDv:0000123', 'HsapDv:0000124', 'HsapDv:0000125', 'HsapDv:0000126', 'HsapDv:0000127', 'HsapDv:0000128', 'HsapDv:0000129', 'HsapDv:0000130', 'HsapDv:0000131', 'HsapDv:0000132', 'HsapDv:0000133', 'HsapDv:0000134', 'HsapDv:0000135', 'HsapDv:0000136', 'HsapDv:0000137', 'HsapDv:0000138', 'HsapDv:0000139', 'HsapDv:0000140', 'HsapDv:0000141', 'HsapDv:0000142', 'HsapDv:0000143', 'HsapDv:0000144', 'HsapDv:0000145', 'HsapDv:0000146', 'HsapDv:0000147', 'HsapDv:0000148', 'HsapDv:0000149', 'HsapDv:0000150', 'HsapDv:0000151', 'HsapDv:0000152', 'HsapDv:0000153', 'HsapDv:0000154', 'HsapDv:0000155', 'HsapDv:0000156', 'HsapDv:0000157', 'HsapDv:0000158', 'HsapDv:0000159', 'HsapDv:0000160', 'HsapDv:0000161', 'HsapDv:0000162', 'HsapDv:0000163', 'HsapDv:0000164', 'HsapDv:0000165', 'HsapDv:0000166', 'HsapDv:0000167', 'HsapDv:0000168', 'HsapDv:0000169', 'HsapDv:0000170', 'HsapDv:0000171', 'HsapDv:0000172', 'HsapDv:0000173', 'HsapDv:0000206', 'HsapDv:0000207', 'HsapDv:0000208', 'HsapDv:0000209', 'HsapDv:0000210', 'HsapDv:0000211', 'HsapDv:0000212', 'HsapDv:0000213', 'HsapDv:0000214', 'HsapDv:0000215', 'HsapDv:0000216', 'HsapDv:0000217', 'HsapDv:0000218', 'HsapDv:0000219', 'HsapDv:0000220', 'HsapDv:0000221', 'HsapDv:0000222', 'HsapDv:0000223', 'HsapDv:0000226', 'HsapDv:0000227', 'HsapDv:0000229', 'HsapDv:0000237', 'HsapDv:0000238', 'HsapDv:0000239', 'HsapDv:0000240', 'HsapDv:0000241', 'HsapDv:0000242', 'HsapDv:0000243', 'HsapDv:0000258', 'HsapDv:0000266', 'HsapDv:0000267', 'HsapDv:0000268', 'HsapDv:0000272', 'HsapDv:0000274'] \n",
      "\n",
      "Exporting 19 obs columns (ontology_term_id columns dropped from output).\n",
      "Done. Wrote 20,327,572 rows across 1 batches to:\n",
      "  /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CENSUS_VERSION = \"stable\"\n",
    "ORGANISM = \"homo_sapiens\"\n",
    "\n",
    "# Filters requested\n",
    "NORMAL_DISEASE_OTID = \"PATO:0000461\"  # normal\n",
    "TARGET_ASSAY_LABELS = [\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\"]\n",
    "TARGET_SEXES = [\"male\", \"female\"]\n",
    "TARGET_SUSPENSION_TYPE = \"cell\"\n",
    "MIN_AGE_YEARS = 15\n",
    "\n",
    "# Inventory CSVs directory (leave None to auto-detect)\n",
    "INVENTORY_DIR: Optional[Path] = None\n",
    "\n",
    "# Output\n",
    "OUTDIR = Path(\"/home/crizza/census_exports\")\n",
    "OUTFILE = \"census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\"\n",
    "OVERWRITE = True\n",
    "\n",
    "# Required metadata columns to include in the exported parquet\n",
    "REQUIRED_EXPORT_COLS = [\"dataset_id\", \"donor_id\", \"self_reported_ethnicity\", \"\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and OVERWRITE=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def _format_in_list(values: List[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def detect_inventory_dir(user_hint: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Locate the folder containing:\n",
    "      - year_old_stage_pairs_counts.csv\n",
    "      - development_stage_pairs_counts.csv\n",
    "    \"\"\"\n",
    "    required = [\"year_old_stage_pairs_counts.csv\", \"development_stage_pairs_counts.csv\"]\n",
    "    candidates: List[Path] = []\n",
    "    if user_hint is not None:\n",
    "        candidates.append(user_hint)\n",
    "\n",
    "    candidates.extend([\n",
    "        Path(\"/home/crizza/census_exports/dev_stage_inventory\"),\n",
    "        Path.home() / \"census_exports\" / \"dev_stage_inventory\",\n",
    "        Path(\"/mnt/data\"),  # if running inside this chat session\n",
    "    ])\n",
    "\n",
    "    for d in candidates:\n",
    "        if all((d / f).exists() for f in required):\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find inventory CSVs. Set INVENTORY_DIR to the folder containing:\\n\"\n",
    "        \"  - year_old_stage_pairs_counts.csv\\n\"\n",
    "        \"  - development_stage_pairs_counts.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stage_ids_ge15(inventory_dir: Path, min_age_years: int = 15) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build development_stage_ontology_term_id allow-list for age >= min_age_years using your CSV inventories.\n",
    "    \"\"\"\n",
    "    year_pairs = pd.read_csv(inventory_dir / \"year_old_stage_pairs_counts.csv\")\n",
    "    pairs = pd.read_csv(inventory_dir / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "    # 1) All exact N-year-old stages where N >= min_age_years\n",
    "    ids = set(\n",
    "        year_pairs.loc[year_pairs[\"age_years\"] >= min_age_years, \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # 2) Add adult bins/labels seen in your data\n",
    "    extra_labels = [\n",
    "        \"15-19 year-old\",\n",
    "        \"adult stage\",\n",
    "        \"young adult stage\",\n",
    "        \"prime adult stage\",\n",
    "        \"late adult stage\",\n",
    "        \"middle aged stage\",\n",
    "        \"third decade stage\",\n",
    "        \"fourth decade stage\",\n",
    "        \"fifth decade stage\",\n",
    "        \"sixth decade stage\",\n",
    "        \"seventh decade stage\",\n",
    "        \"eighth decade stage\",\n",
    "        \"ninth decade stage\",\n",
    "        \"60-79 year-old stage\",\n",
    "        \"80 year-old and over stage\",\n",
    "        \"90 year-old and over stage\",\n",
    "    ]\n",
    "\n",
    "    extra_ids = (\n",
    "        pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    ids.update(extra_ids)\n",
    "\n",
    "    ids.discard(\"unknown\")\n",
    "    return sorted(ids)\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id via census_info/summary_cell_counts.\n",
    "    Avoids quoting issues with \"10x 3' v3\".\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(TARGET_ASSAY_LABELS))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in TARGET_ASSAY_LABELS if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for assay label(s): {missing}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def build_obs_value_filter(census, inventory_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Combined filters:\n",
    "      - is_primary_data\n",
    "      - normal disease\n",
    "      - male/female\n",
    "      - suspension_type cell\n",
    "      - assays: 10x v2/v3/multiome (via assay_ontology_term_id)\n",
    "      - age >= 15 years (via development_stage_ontology_term_id allow-list)\n",
    "    \"\"\"\n",
    "    assay_map = lookup_assay_term_ids(census)\n",
    "    assay_term_ids = [assay_map[x] for x in TARGET_ASSAY_LABELS]\n",
    "    stage_ids_ge15 = build_stage_ids_ge15(inventory_dir, MIN_AGE_YEARS)\n",
    "\n",
    "    vf_parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{NORMAL_DISEASE_OTID}'\",\n",
    "        f\"sex in {_format_in_list(TARGET_SEXES)}\",\n",
    "        f\"suspension_type == '{TARGET_SUSPENSION_TYPE}'\",\n",
    "        f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "        f\"development_stage_ontology_term_id in {_format_in_list(stage_ids_ge15)}\",\n",
    "    ]\n",
    "    return \" and \".join(vf_parts)\n",
    "\n",
    "\n",
    "def build_export_columns(exp_obs_schema_names: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Export ALL non-ontology columns + required columns.\n",
    "    Drops any column containing 'ontology_term_id' from OUTPUT.\n",
    "    \"\"\"\n",
    "    all_cols = list(exp_obs_schema_names)\n",
    "\n",
    "    # Validate required columns exist\n",
    "    missing = [c for c in REQUIRED_EXPORT_COLS if c not in all_cols]\n",
    "    if missing:\n",
    "        # Suggest likely alternatives (especially for ethnicity)\n",
    "        hints = {}\n",
    "        for m in missing:\n",
    "            key = m.split(\"_\")[0]\n",
    "            hints[m] = [c for c in all_cols if key in c.lower()][:50]\n",
    "\n",
    "        msg = \"Required export column(s) not found in exp.obs schema:\\n\"\n",
    "        for m in missing:\n",
    "            msg += f\"  - {m}\\n\"\n",
    "            if hints.get(m):\n",
    "                msg += f\"    suggestions: {hints[m]}\\n\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Start from all columns, drop ontology IDs + internal join id\n",
    "    cols = [c for c in all_cols if (\"ontology_term_id\" not in c and c != \"soma_joinid\")]\n",
    "\n",
    "    # Ensure required cols included (in case they were filtered out for any reason)\n",
    "    for r in REQUIRED_EXPORT_COLS:\n",
    "        if r not in cols:\n",
    "            cols.append(r)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def export_obs_to_parquet(exp, value_filter: str, outpath: Path, column_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Stream obs rows and write to a single parquet.\n",
    "    Avoids tiledbsoma BatchSize import entirely.\n",
    "    \"\"\"\n",
    "    it = exp.obs.read(value_filter=value_filter, column_names=column_names)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "\n",
    "    try:\n",
    "        for tbl in it:\n",
    "            if isinstance(tbl, pa.RecordBatch):\n",
    "                tbl = pa.Table.from_batches([tbl])\n",
    "            if not isinstance(tbl, pa.Table):\n",
    "                tbl = pa.Table.from_batches(tbl.to_batches())\n",
    "\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=\"zstd\")\n",
    "            writer.write_table(tbl)\n",
    "\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "            if batches % 10 == 0:\n",
    "                print(f\"[export] batches={batches:,} rows={rows:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    print(f\"Done. Wrote {rows:,} rows across {batches:,} batches to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    inventory_dir = detect_inventory_dir(INVENTORY_DIR)\n",
    "    print(\"Using inventory CSV directory:\", inventory_dir)\n",
    "\n",
    "    outpath = resolve_outpath(OUTDIR, OUTFILE, OVERWRITE)\n",
    "    print(\"Output file:\", outpath)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS_VERSION) as census:\n",
    "        exp = census[\"census_data\"][ORGANISM]\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, inventory_dir)\n",
    "        print(\"\\nCombined OBS value_filter:\\n\", value_filter, \"\\n\")\n",
    "\n",
    "        export_cols = build_export_columns(exp.obs.schema.names)\n",
    "        print(f\"Exporting {len(export_cols)} obs columns (ontology_term_id columns dropped from output).\")\n",
    "\n",
    "        export_obs_to_parquet(exp, value_filter, outpath, export_cols)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c549b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] batches=10 rows_scanned=5,242,880 unique_donors=950\n",
      "[scan] batches=20 rows_scanned=10,485,760 unique_donors=2,181\n",
      "[scan] batches=30 rows_scanned=15,728,640 unique_donors=2,265\n",
      "[scan] batches=40 rows_scanned=20,327,572 unique_donors=2,615\n",
      "\n",
      "Done.\n",
      "Wrote exports to:\n",
      "  /home/crizza/census_exports/metadata_inventory/metadata_summary.csv\n",
      "  /home/crizza/census_exports/metadata_inventory/sex_counts.csv\n",
      "  /home/crizza/census_exports/metadata_inventory/ethnicity_counts.csv\n",
      "  /home/crizza/census_exports/metadata_inventory/sex_by_ethnicity_counts.csv\n",
      "  /home/crizza/census_exports/metadata_inventory/donor_level_counts.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "PARQUET_PATH = Path(\"/home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\")\n",
    "OUTDIR = PARQUET_PATH.parent / \"metadata_inventory\"   # analogous to dev_stage_inventory\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 1_000_000\n",
    "PROGRESS_EVERY = 10\n",
    "\n",
    "COL_DONOR = \"donor_id\"\n",
    "COL_SEX = \"sex\"\n",
    "COL_ETH = \"self_reported_ethnicity\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _require_cols(pf: pq.ParquetFile, cols: list[str]) -> None:\n",
    "    missing = [c for c in cols if c not in pf.schema_arrow.names]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"Missing required columns in parquet: \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\nAvailable columns include: \"\n",
    "            + \", \".join(pf.schema_arrow.names[:80])\n",
    "            + (\" ...\" if len(pf.schema_arrow.names) > 80 else \"\")\n",
    "        )\n",
    "\n",
    "\n",
    "def _norm_str(x) -> str:\n",
    "    \"\"\"Normalize string-like metadata consistently for grouping.\"\"\"\n",
    "    if x is None:\n",
    "        return \"unknown\"\n",
    "    s = str(x).strip()\n",
    "    return s if s else \"unknown\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main scan\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    pf = pq.ParquetFile(PARQUET_PATH)\n",
    "    _require_cols(pf, [COL_DONOR, COL_SEX, COL_ETH])\n",
    "\n",
    "    # Cell-level counts (like your development_stage_counts)\n",
    "    cell_by_sex = Counter()\n",
    "    cell_by_eth = Counter()\n",
    "    cell_by_sex_eth = Counter()\n",
    "\n",
    "    # Donor-level aggregation\n",
    "    donors_all = set()\n",
    "    donors_by_sex = defaultdict(set)        # sex -> set(donor_id)\n",
    "    donors_by_eth = defaultdict(set)        # eth -> set(donor_id)\n",
    "    donors_by_sex_eth = defaultdict(set)    # (sex, eth) -> set(donor_id)\n",
    "\n",
    "    donor_cell_count = Counter()            # donor_id -> total cells\n",
    "    donor_sex = {}                          # donor_id -> canonical sex\n",
    "    donor_eth = {}                          # donor_id -> canonical ethnicity\n",
    "    donor_inconsistency = Counter()         # donor_id -> count of conflicting assignments (sex/eth)\n",
    "\n",
    "    rows_scanned = 0\n",
    "    batches = 0\n",
    "\n",
    "    cols = [COL_DONOR, COL_SEX, COL_ETH]\n",
    "\n",
    "    for batch in pf.iter_batches(columns=cols, batch_size=BATCH_SIZE):\n",
    "        batches += 1\n",
    "        rows_scanned += batch.num_rows\n",
    "\n",
    "        # Cast to string for robust handling (dictionary encoded columns are common in Arrow)\n",
    "        donor = pc.cast(batch.column(0), pa.string())\n",
    "        sex = pc.cast(batch.column(1), pa.string())\n",
    "        eth = pc.cast(batch.column(2), pa.string())\n",
    "\n",
    "        df = pa.Table.from_arrays([donor, sex, eth], names=cols).to_pandas()\n",
    "\n",
    "        # Normalize values\n",
    "        df[COL_DONOR] = df[COL_DONOR].map(_norm_str)\n",
    "        df[COL_SEX] = df[COL_SEX].map(_norm_str)\n",
    "        df[COL_ETH] = df[COL_ETH].map(_norm_str)\n",
    "\n",
    "        # Drop rows without a meaningful donor_id (if any)\n",
    "        df = df[df[COL_DONOR] != \"unknown\"]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Batch group-by to avoid per-row loops: (donor_id, sex, eth) -> cell count\n",
    "        gb = df.groupby([COL_DONOR, COL_SEX, COL_ETH], dropna=False).size()\n",
    "\n",
    "        for (d, s, e), n in gb.items():\n",
    "            n = int(n)\n",
    "\n",
    "            # cell-level counts\n",
    "            cell_by_sex[s] += n\n",
    "            cell_by_eth[e] += n\n",
    "            cell_by_sex_eth[(s, e)] += n\n",
    "\n",
    "            # donor-level sets and counts\n",
    "            donors_all.add(d)\n",
    "            donors_by_sex[s].add(d)\n",
    "            donors_by_eth[e].add(d)\n",
    "            donors_by_sex_eth[(s, e)].add(d)\n",
    "            donor_cell_count[d] += n\n",
    "\n",
    "            # metadata consistency check per donor\n",
    "            if d in donor_sex and donor_sex[d] != s:\n",
    "                donor_inconsistency[d] += 1\n",
    "            else:\n",
    "                donor_sex.setdefault(d, s)\n",
    "\n",
    "            if d in donor_eth and donor_eth[d] != e:\n",
    "                donor_inconsistency[d] += 1\n",
    "            else:\n",
    "                donor_eth.setdefault(d, e)\n",
    "\n",
    "        if PROGRESS_EVERY and (batches % PROGRESS_EVERY == 0):\n",
    "            print(f\"[scan] batches={batches:,} rows_scanned={rows_scanned:,} unique_donors={len(donors_all):,}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Write exports (analogous to dev_stage_inventory)\n",
    "    # -----------------------------\n",
    "    total_unique_donors = len(donors_all)\n",
    "    total_cells = rows_scanned  # should match pf.metadata.num_rows for a single-file parquet\n",
    "\n",
    "    # 1) High-level summary\n",
    "    summary = pd.DataFrame([{\n",
    "        \"parquet_path\": str(PARQUET_PATH),\n",
    "        \"rows_scanned_cells\": total_cells,\n",
    "        \"unique_donors\": total_unique_donors,\n",
    "        \"unique_sex_labels\": len(cell_by_sex),\n",
    "        \"unique_ethnicity_labels\": len(cell_by_eth),\n",
    "        \"donors_with_conflicting_sex_or_ethnicity\": sum(1 for d in donor_inconsistency if donor_inconsistency[d] > 0),\n",
    "    }])\n",
    "    summary.to_csv(OUTDIR / \"metadata_summary.csv\", index=False)\n",
    "\n",
    "    # 2) Sex: cell counts + unique donors\n",
    "    sex_rows = []\n",
    "    for s, cell_n in cell_by_sex.most_common():\n",
    "        sex_rows.append({\n",
    "            \"sex\": s,\n",
    "            \"cell_count_in_parquet\": cell_n,\n",
    "            \"unique_donors\": len(donors_by_sex[s]),\n",
    "        })\n",
    "    pd.DataFrame(sex_rows).to_csv(OUTDIR / \"sex_counts.csv\", index=False)\n",
    "\n",
    "    # 3) Ethnicity: cell counts + unique donors\n",
    "    eth_rows = []\n",
    "    for e, cell_n in cell_by_eth.most_common():\n",
    "        eth_rows.append({\n",
    "            \"self_reported_ethnicity\": e,\n",
    "            \"cell_count_in_parquet\": cell_n,\n",
    "            \"unique_donors\": len(donors_by_eth[e]),\n",
    "        })\n",
    "    pd.DataFrame(eth_rows).to_csv(OUTDIR / \"ethnicity_counts.csv\", index=False)\n",
    "\n",
    "    # 4) Sex Ã— Ethnicity: cell counts + unique donors\n",
    "    se_rows = []\n",
    "    for (s, e), cell_n in cell_by_sex_eth.most_common():\n",
    "        se_rows.append({\n",
    "            \"sex\": s,\n",
    "            \"self_reported_ethnicity\": e,\n",
    "            \"cell_count_in_parquet\": cell_n,\n",
    "            \"unique_donors\": len(donors_by_sex_eth[(s, e)]),\n",
    "        })\n",
    "    pd.DataFrame(se_rows).to_csv(OUTDIR / \"sex_by_ethnicity_counts.csv\", index=False)\n",
    "\n",
    "    # 5) Donor-level table (cells per donor + canonical sex/eth + inconsistency flag)\n",
    "    donor_rows = []\n",
    "    for d, cell_n in donor_cell_count.most_common():\n",
    "        donor_rows.append({\n",
    "            \"donor_id\": d,\n",
    "            \"cell_count_in_parquet\": cell_n,\n",
    "            \"sex\": donor_sex.get(d, \"unknown\"),\n",
    "            \"self_reported_ethnicity\": donor_eth.get(d, \"unknown\"),\n",
    "            \"metadata_conflict_events\": int(donor_inconsistency.get(d, 0)),\n",
    "        })\n",
    "    pd.DataFrame(donor_rows).to_csv(OUTDIR / \"donor_level_counts.csv\", index=False)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(\"Wrote exports to:\")\n",
    "    for f in [\n",
    "        \"metadata_summary.csv\",\n",
    "        \"sex_counts.csv\",\n",
    "        \"ethnicity_counts.csv\",\n",
    "        \"sex_by_ethnicity_counts.csv\",\n",
    "        \"donor_level_counts.csv\",\n",
    "    ]:\n",
    "        print(\" \", OUTDIR / f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c84929",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (NA), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 188\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# Toggle this:\u001b[39;00m\n\u001b[32m    186\u001b[39m     ADULT_ONLY = \u001b[38;5;28;01mTrue\u001b[39;00m   \u001b[38;5;66;03m# set False if you want the full healthy-only composition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     counts_1d, counts_2d = \u001b[43maggregate_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43madult_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mADULT_ONLY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_age_years\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     prefix = \u001b[33m\"\u001b[39m\u001b[33madult15\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ADULT_ONLY \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mall_stages\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1D plots\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36maggregate_counts\u001b[39m\u001b[34m(adult_only, min_age_years)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (a, b) \u001b[38;5;129;01min\u001b[39;00m COLS_2D:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;129;01mand\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         pairs = \u001b[38;5;28mzip\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m), df[b].fillna(\u001b[33m\"\u001b[39m\u001b[33mNA\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[32m    175\u001b[39m         counts_2d[(a, b)].update(pairs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rows_seen % (\u001b[32m5_000_000\u001b[39m) < BATCH_SIZE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:7372\u001b[39m, in \u001b[36mNDFrame.fillna\u001b[39m\u001b[34m(self, value, method, axis, inplace, limit, downcast)\u001b[39m\n\u001b[32m   7365\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   7367\u001b[39m             \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m parameter must be a scalar, dict \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mor Series, but you passed a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   7369\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7370\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m7372\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\n\u001b[32m   7374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7376\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mdict\u001b[39m, ABCSeries)):\n\u001b[32m   7377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/base.py:186\u001b[39m, in \u001b[36mDataManager.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[32m    184\u001b[39m     limit = libalgos.validate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit=limit)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfillna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:2407\u001b[39m, in \u001b[36mExtensionBlock.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   2405\u001b[39m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n\u001b[32m   2406\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2408\u001b[39m     \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[32m   2409\u001b[39m     \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n\u001b[32m   2410\u001b[39m     warnings.warn(\n\u001b[32m   2411\u001b[39m         \u001b[38;5;66;03m# GH#53278\u001b[39;00m\n\u001b[32m   2412\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExtensionArray.fillna added a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword in pandas \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2418\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   2419\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/arrays/_mixins.py:376\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.fillna\u001b[39m\u001b[34m(self, value, method, limit, copy)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# We validate the fill_value even if there is nothing to fill\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[32m    379\u001b[39m         new_values = \u001b[38;5;28mself\u001b[39m[:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/arrays/categorical.py:1590\u001b[39m, in \u001b[36mCategorical._validate_setitem_value\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_listlike(value)\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/arrays/categorical.py:1615\u001b[39m, in \u001b[36mCategorical._validate_scalar\u001b[39m\u001b[34m(self, fill_value)\u001b[39m\n\u001b[32m   1613\u001b[39m     fill_value = \u001b[38;5;28mself\u001b[39m._unbox_scalar(fill_value)\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1616\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot setitem on a Categorical with a new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1617\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), set the categories first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1618\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[31mTypeError\u001b[39m: Cannot setitem on a Categorical with a new category (NA), set the categories first"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "PARQUET_PATH = Path(\"/home/crizza/census_exports/census_hsapiens_healthy_adult_obs.parquet\")\n",
    "OUTDIR = Path(\"/home/crizza/census_exports/figures_obs_summary\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 250_000\n",
    "\n",
    "# Columns weâ€™ll summarize (must exist in the parquet)\n",
    "COLS_1D = [\"tissue_general\", \"tissue\", \"assay\", \"cell_type\", \"sex\", \"development_stage\"]\n",
    "COLS_2D = [(\"tissue_general\", \"assay\")]  # for a stacked-bar style summary\n",
    "\n",
    "\n",
    "_year_old_re = re.compile(r\"^\\s*(\\d+)\\s*-\\s*year-old stage\\s*$\", re.IGNORECASE)\n",
    "_decade_re = re.compile(r\"^\\s*(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\\s+decade stage\\s*$\",\n",
    "                        re.IGNORECASE)\n",
    "\n",
    "def is_adult_stage_label(stage: str, min_age_years: int = 15) -> bool:\n",
    "    \"\"\"\n",
    "    Best-effort adult>=min_age_years classifier from development_stage label.\n",
    "    Keeps:\n",
    "      - 'adult stage', 'young adult stage', 'prime adult stage', 'late adult stage', 'middle aged stage'\n",
    "      - '{N}-year-old stage' where N>=min_age_years\n",
    "      - '{X} decade stage' where decade >= second (>=10) is ambiguous; we keep >= second only if you want.\n",
    "        Here we keep >= second decade conservatively; edit if you want stricter.\n",
    "    Drops fetal/embryonic (weeks post-fertilization, LMP months), months-old, and unknown.\n",
    "    \"\"\"\n",
    "    if stage is None:\n",
    "        return False\n",
    "    s = stage.strip().lower()\n",
    "    if s == \"\" or s == \"unknown\":\n",
    "        return False\n",
    "\n",
    "    if \"adult\" in s or \"middle aged\" in s:\n",
    "        return True\n",
    "\n",
    "    m = _year_old_re.match(stage)\n",
    "    if m:\n",
    "        age = int(m.group(1))\n",
    "        return age >= min_age_years\n",
    "\n",
    "    m2 = _decade_re.match(stage)\n",
    "    if m2:\n",
    "        # Map decade words to starting age approx\n",
    "        decade_map = {\n",
    "            \"first\": 0, \"second\": 10, \"third\": 20, \"fourth\": 30, \"fifth\": 40,\n",
    "            \"sixth\": 50, \"seventh\": 60, \"eighth\": 70, \"ninth\": 80, \"tenth\": 90\n",
    "        }\n",
    "        start_age = decade_map[m2.group(1).lower()]\n",
    "        return start_age >= min_age_years\n",
    "\n",
    "    # Months-old, weeks post-fertilization, LMP months, etc. -> treat as not adult\n",
    "    return False\n",
    "\n",
    "\n",
    "def plot_top_bar(counts: Counter, title: str, out_prefix: str, top_n: int = 25):\n",
    "    items = counts.most_common(top_n)\n",
    "    df = pd.DataFrame(items, columns=[\"label\", \"count\"]).iloc[::-1]  # reverse for horizontal bar\n",
    "    fig = plt.figure(figsize=(10, max(4, 0.25 * len(df))))\n",
    "    plt.barh(df[\"label\"], df[\"count\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Cell count\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(OUTDIR / f\"{out_prefix}.svg\")\n",
    "    fig.savefig(OUTDIR / f\"{out_prefix}.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    df.to_csv(OUTDIR / f\"{out_prefix}.csv\", index=False)\n",
    "\n",
    "\n",
    "def plot_stacked_bar(pair_counts: Counter, row_name: str, col_name: str,\n",
    "                     title: str, out_prefix: str, top_rows: int = 15, top_cols: int = 6):\n",
    "    \"\"\"\n",
    "    pair_counts keys: (row_value, col_value) -> count\n",
    "    Produces stacked bars: top row categories, stacked by top col categories (others collapsed).\n",
    "    \"\"\"\n",
    "    # Determine top rows/cols by marginal sums\n",
    "    row_sum = Counter()\n",
    "    col_sum = Counter()\n",
    "    for (r, c), n in pair_counts.items():\n",
    "        row_sum[r] += n\n",
    "        col_sum[c] += n\n",
    "\n",
    "    rows = [r for r, _ in row_sum.most_common(top_rows)]\n",
    "    cols = [c for c, _ in col_sum.most_common(top_cols)]\n",
    "\n",
    "    # Build matrix with an \"Other\" column\n",
    "    mat = []\n",
    "    for r in rows:\n",
    "        row_total = 0\n",
    "        row_vals = []\n",
    "        for c in cols:\n",
    "            v = pair_counts.get((r, c), 0)\n",
    "            row_vals.append(v)\n",
    "            row_total += v\n",
    "        other = row_sum[r] - row_total\n",
    "        row_vals.append(max(other, 0))\n",
    "        mat.append(row_vals)\n",
    "\n",
    "    col_labels = cols + [\"Other\"]\n",
    "    df = pd.DataFrame(mat, index=rows, columns=col_labels)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    bottom = None\n",
    "    x = range(len(df.index))\n",
    "    for j, cname in enumerate(df.columns):\n",
    "        vals = df[cname].values\n",
    "        if bottom is None:\n",
    "            plt.bar(x, vals, label=cname)\n",
    "            bottom = vals.astype(float)\n",
    "        else:\n",
    "            plt.bar(x, vals, bottom=bottom, label=cname)\n",
    "            bottom = bottom + vals\n",
    "\n",
    "    plt.xticks(list(x), df.index, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Cell count\")\n",
    "    plt.title(title)\n",
    "    plt.legend(frameon=False, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(OUTDIR / f\"{out_prefix}.svg\")\n",
    "    fig.savefig(OUTDIR / f\"{out_prefix}.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    df.reset_index(names=row_name).to_csv(OUTDIR / f\"{out_prefix}.csv\", index=False)\n",
    "\n",
    "\n",
    "def aggregate_counts(adult_only: bool = False, min_age_years: int = 15):\n",
    "    pf = pq.ParquetFile(PARQUET_PATH)\n",
    "\n",
    "    # Counters for 1D summaries\n",
    "    counts_1d = {c: Counter() for c in COLS_1D}\n",
    "\n",
    "    # Counters for 2D summaries\n",
    "    counts_2d = {pair: Counter() for pair in COLS_2D}\n",
    "\n",
    "    rows_seen = 0\n",
    "    rows_kept = 0\n",
    "\n",
    "    # Only load columns that exist\n",
    "    available = set(pf.schema_arrow.names)\n",
    "    cols_needed = [c for c in set(COLS_1D + [x for pair in COLS_2D for x in pair]) if c in available]\n",
    "\n",
    "    for batch in pf.iter_batches(columns=cols_needed, batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        rows_seen += len(df)\n",
    "\n",
    "        if adult_only and \"development_stage\" in df.columns:\n",
    "            mask = df[\"development_stage\"].astype(\"string\").apply(lambda s: is_adult_stage_label(s, min_age_years))\n",
    "            df = df.loc[mask].copy()\n",
    "\n",
    "        rows_kept += len(df)\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "\n",
    "        # Update 1D counters\n",
    "        for c in COLS_1D:\n",
    "            if c in df.columns:\n",
    "                counts_1d[c].update(df[c].dropna().astype(str).tolist())\n",
    "\n",
    "        # Update 2D counters\n",
    "        for (a, b) in COLS_2D:\n",
    "            if a in df.columns and b in df.columns:\n",
    "                pairs = zip(df[a].fillna(\"NA\").astype(str), df[b].fillna(\"NA\").astype(str))\n",
    "                counts_2d[(a, b)].update(pairs)\n",
    "\n",
    "        if rows_seen % (5_000_000) < BATCH_SIZE:\n",
    "            print(f\"Scanned {rows_seen:,} rows; kept {rows_kept:,} rows (adult_only={adult_only})\")\n",
    "\n",
    "    print(f\"DONE. Total scanned={rows_seen:,}; total kept={rows_kept:,} (adult_only={adult_only})\")\n",
    "    return counts_1d, counts_2d\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Toggle this:\n",
    "    ADULT_ONLY = True   # set False if you want the full healthy-only composition\n",
    "\n",
    "    counts_1d, counts_2d = aggregate_counts(adult_only=ADULT_ONLY, min_age_years=15)\n",
    "\n",
    "    prefix = \"adult15\" if ADULT_ONLY else \"all_stages\"\n",
    "\n",
    "    # 1D plots\n",
    "    plot_top_bar(counts_1d[\"tissue_general\"],\n",
    "                 title=f\"Healthy (normal) composition by tissue_general ({prefix})\",\n",
    "                 out_prefix=f\"{prefix}_tissue_general_top25\",\n",
    "                 top_n=25)\n",
    "\n",
    "    plot_top_bar(counts_1d[\"assay\"],\n",
    "                 title=f\"Healthy (normal) composition by assay ({prefix})\",\n",
    "                 out_prefix=f\"{prefix}_assay_top25\",\n",
    "                 top_n=25)\n",
    "\n",
    "    plot_top_bar(counts_1d[\"cell_type\"],\n",
    "                 title=f\"Healthy (normal) composition by cell_type ({prefix})\",\n",
    "                 out_prefix=f\"{prefix}_cell_type_top25\",\n",
    "                 top_n=25)\n",
    "\n",
    "    plot_top_bar(counts_1d[\"development_stage\"],\n",
    "                 title=f\"Healthy (normal) composition by development_stage ({prefix})\",\n",
    "                 out_prefix=f\"{prefix}_development_stage_top25\",\n",
    "                 top_n=25)\n",
    "\n",
    "    plot_top_bar(counts_1d[\"sex\"],\n",
    "                 title=f\"Healthy (normal) composition by sex ({prefix})\",\n",
    "                 out_prefix=f\"{prefix}_sex\",\n",
    "                 top_n=10)\n",
    "\n",
    "    # 2D stacked plot: assay composition within top tissues\n",
    "    plot_stacked_bar(counts_2d[(\"tissue_general\", \"assay\")],\n",
    "                     row_name=\"tissue_general\",\n",
    "                     col_name=\"assay\",\n",
    "                     title=f\"Assay composition within top tissues ({prefix})\",\n",
    "                     out_prefix=f\"{prefix}_tissue_general_by_assay_stacked\",\n",
    "                     top_rows=15,\n",
    "                     top_cols=6)\n",
    "\n",
    "    print(f\"Outputs written to: {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inventory CSV directory: /home/crizza/census_exports/dev_stage_inventory\n",
      "Output file: /home/crizza/census_exports/census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-11-08. Specify 'census_version=\"2025-11-08\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined OBS value_filter:\n",
      " is_primary_data == True and disease_ontology_term_id == 'PATO:0000461' and sex in ['male', 'female'] and suspension_type == 'cell' and assay_ontology_term_id in ['EFO:0009899', 'EFO:0009922', 'EFO:0030059'] and development_stage_ontology_term_id in ['HsapDv:0000095', 'HsapDv:0000109', 'HsapDv:0000110', 'HsapDv:0000111', 'HsapDv:0000112', 'HsapDv:0000113', 'HsapDv:0000114', 'HsapDv:0000115', 'HsapDv:0000116', 'HsapDv:0000117', 'HsapDv:0000118', 'HsapDv:0000119', 'HsapDv:0000120', 'HsapDv:0000121', 'HsapDv:0000122', 'HsapDv:0000123', 'HsapDv:0000124', 'HsapDv:0000125', 'HsapDv:0000126', 'HsapDv:0000127', 'HsapDv:0000128', 'HsapDv:0000129', 'HsapDv:0000130', 'HsapDv:0000131', 'HsapDv:0000132', 'HsapDv:0000133', 'HsapDv:0000134', 'HsapDv:0000135', 'HsapDv:0000136', 'HsapDv:0000137', 'HsapDv:0000138', 'HsapDv:0000139', 'HsapDv:0000140', 'HsapDv:0000141', 'HsapDv:0000142', 'HsapDv:0000143', 'HsapDv:0000144', 'HsapDv:0000145', 'HsapDv:0000146', 'HsapDv:0000147', 'HsapDv:0000148', 'HsapDv:0000149', 'HsapDv:0000150', 'HsapDv:0000151', 'HsapDv:0000152', 'HsapDv:0000153', 'HsapDv:0000154', 'HsapDv:0000155', 'HsapDv:0000156', 'HsapDv:0000157', 'HsapDv:0000158', 'HsapDv:0000159', 'HsapDv:0000160', 'HsapDv:0000161', 'HsapDv:0000162', 'HsapDv:0000163', 'HsapDv:0000164', 'HsapDv:0000165', 'HsapDv:0000166', 'HsapDv:0000167', 'HsapDv:0000168', 'HsapDv:0000169', 'HsapDv:0000170', 'HsapDv:0000171', 'HsapDv:0000172', 'HsapDv:0000173', 'HsapDv:0000206', 'HsapDv:0000207', 'HsapDv:0000208', 'HsapDv:0000209', 'HsapDv:0000210', 'HsapDv:0000211', 'HsapDv:0000212', 'HsapDv:0000213', 'HsapDv:0000214', 'HsapDv:0000215', 'HsapDv:0000216', 'HsapDv:0000217', 'HsapDv:0000218', 'HsapDv:0000219', 'HsapDv:0000220', 'HsapDv:0000221', 'HsapDv:0000222', 'HsapDv:0000223', 'HsapDv:0000226', 'HsapDv:0000227', 'HsapDv:0000229', 'HsapDv:0000237', 'HsapDv:0000238', 'HsapDv:0000239', 'HsapDv:0000240', 'HsapDv:0000241', 'HsapDv:0000242', 'HsapDv:0000243', 'HsapDv:0000258', 'HsapDv:0000266', 'HsapDv:0000267', 'HsapDv:0000268', 'HsapDv:0000272', 'HsapDv:0000274'] \n",
      "\n",
      "Organ column (coarse): tissue_general\n",
      "Will export 19 obs columns (all non-ontology).\n"
     ]
    },
    {
     "ename": "SOMAError",
     "evalue": "[ManagedQuery] [unnamed] Query FAILED: [TileDB::Task] Error: Caught std::exception: VFS: VFS parallel read error 's3://cellxgene-census-public-us-west-2/cell-census/2025-11-08/soma/census_data/homo_sapiens/obs/__fragments/__1762679146914_1762679146914_71bc71667c02c028dbb501720895e58e_22/a24.tdb'; Caught std::exception: S3: Failed to read S3 object s3://cellxgene-census-public-us-west-2/cell-census/2025-11-08/soma/census_data/homo_sapiens/obs/__fragments/__1762679146914_1762679146914_71bc71667c02c028dbb501720895e58e_22/a24.tdb[Error Type: 99] [HTTP Response Code: -1] : curlCode: 28, Timeout was reached; Details: Resolving timed out after 10800 milliseconds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSOMAError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 332\u001b[39m\n\u001b[32m    328\u001b[39m         export_obs_to_parquet(exp, value_filter, outpath, export_cols)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 328\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOrgan column (coarse): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPORT.organ_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWill export \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(export_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m obs columns \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    326\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mall non-ontology\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mEXPORT.export_all_non_ontology\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequired only\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[43mexport_obs_to_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_cols\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 279\u001b[39m, in \u001b[36mexport_obs_to_parquet\u001b[39m\u001b[34m(exp, value_filter, outpath, column_names)\u001b[39m\n\u001b[32m    276\u001b[39m batches = \u001b[32m0\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRecordBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tiledbsoma/_read_iters.py:109\u001b[39m, in \u001b[36mTableReadIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pa.Table:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tiledbsoma/_read_iters.py:551\u001b[39m, in \u001b[36mArrowTableRead.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pa.Table:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmq\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSOMAError\u001b[39m: [ManagedQuery] [unnamed] Query FAILED: [TileDB::Task] Error: Caught std::exception: VFS: VFS parallel read error 's3://cellxgene-census-public-us-west-2/cell-census/2025-11-08/soma/census_data/homo_sapiens/obs/__fragments/__1762679146914_1762679146914_71bc71667c02c028dbb501720895e58e_22/a24.tdb'; Caught std::exception: S3: Failed to read S3 object s3://cellxgene-census-public-us-west-2/cell-census/2025-11-08/soma/census_data/homo_sapiens/obs/__fragments/__1762679146914_1762679146914_71bc71667c02c028dbb501720895e58e_22/a24.tdb[Error Type: 99] [HTTP Response Code: -1] : curlCode: 28, Timeout was reached; Details: Resolving timed out after 10800 milliseconds"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Set\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class CensusConfig:\n",
    "    census_version: str = \"stable\"\n",
    "    organism: str = \"homo_sapiens\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FiltersConfig:\n",
    "    # Filters requested\n",
    "    normal_disease_otid: str = \"PATO:0000461\"  # normal\n",
    "    target_assay_labels: Sequence[str] = (\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\")\n",
    "    target_sexes: Sequence[str] = (\"male\", \"female\")\n",
    "    target_suspension_type: str = \"cell\"\n",
    "    min_age_years: int = 15\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ExportConfig:\n",
    "    outdir: Path = Path(\"/home/crizza/census_exports\")\n",
    "    outfile: str = \"census_hsapiens_normal_cell_mf_10x_age15plus_obs_donor_ethnicity.parquet\"\n",
    "    overwrite: bool = True\n",
    "\n",
    "    # Inventory CSVs directory (leave None to auto-detect)\n",
    "    inventory_dir: Optional[Path] = None\n",
    "\n",
    "    # Organ/tissue/cell-type metadata\n",
    "    # - organ best default: tissue_general (coarse organ-level)\n",
    "    organ_col: str = \"tissue_general\"\n",
    "    tissue_cols: Sequence[str] = (\"tissue\", \"tissue_general\")\n",
    "    cell_type_col: str = \"cell_type\"\n",
    "\n",
    "    # Export behavior\n",
    "    export_all_non_ontology: bool = True\n",
    "    drop_ontology_term_id_cols: bool = True  # keep labels; drop *_ontology_term_id\n",
    "\n",
    "\n",
    "CENSUS = CensusConfig()\n",
    "FILTERS = FiltersConfig()\n",
    "EXPORT = ExportConfig()\n",
    "\n",
    "# Minimal required columns (fixed: removed the empty string)\n",
    "BASE_REQUIRED_EXPORT_COLS: Sequence[str] = (\n",
    "    \"dataset_id\",\n",
    "    \"donor_id\",\n",
    "    \"self_reported_ethnicity\",\n",
    ")\n",
    "\n",
    "# Explicitly require these â€œbusinessâ€ metadata fields too\n",
    "REQUIRED_METADATA_COLS: Sequence[str] = (\n",
    "    EXPORT.organ_col,         # organ-level (default: tissue_general)\n",
    "    \"tissue\",                 # finer tissue\n",
    "    EXPORT.cell_type_col,     # cell type label\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and overwrite=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def _format_in_list(values: Sequence[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def detect_inventory_dir(user_hint: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Locate the folder containing:\n",
    "      - year_old_stage_pairs_counts.csv\n",
    "      - development_stage_pairs_counts.csv\n",
    "    \"\"\"\n",
    "    required = [\"year_old_stage_pairs_counts.csv\", \"development_stage_pairs_counts.csv\"]\n",
    "    candidates: List[Path] = []\n",
    "    if user_hint is not None:\n",
    "        candidates.append(user_hint)\n",
    "\n",
    "    candidates.extend(\n",
    "        [\n",
    "            Path(\"/home/crizza/census_exports/dev_stage_inventory\"),\n",
    "            Path.home() / \"census_exports\" / \"dev_stage_inventory\",\n",
    "            Path(\"/mnt/data\"),  # useful if running inside a notebook/chat environment\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for d in candidates:\n",
    "        if all((d / f).exists() for f in required):\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find inventory CSVs. Set inventory_dir to the folder containing:\\n\"\n",
    "        \"  - year_old_stage_pairs_counts.csv\\n\"\n",
    "        \"  - development_stage_pairs_counts.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stage_ids_ge15(inventory_dir: Path, min_age_years: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build development_stage_ontology_term_id allow-list for age >= min_age_years\n",
    "    using your CSV inventories.\n",
    "    \"\"\"\n",
    "    year_pairs = pd.read_csv(inventory_dir / \"year_old_stage_pairs_counts.csv\")\n",
    "    pairs = pd.read_csv(inventory_dir / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "    # Defensive typing\n",
    "    year_pairs[\"age_years\"] = pd.to_numeric(year_pairs[\"age_years\"], errors=\"coerce\")\n",
    "\n",
    "    # 1) All exact N-year-old stages where N >= min_age_years\n",
    "    ids: Set[str] = set(\n",
    "        year_pairs.loc[year_pairs[\"age_years\"] >= min_age_years, \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # 2) Add adult bins/labels seen in your data\n",
    "    extra_labels = [\n",
    "        \"15-19 year-old\",\n",
    "        \"adult stage\",\n",
    "        \"young adult stage\",\n",
    "        \"prime adult stage\",\n",
    "        \"late adult stage\",\n",
    "        \"middle aged stage\",\n",
    "        \"third decade stage\",\n",
    "        \"fourth decade stage\",\n",
    "        \"fifth decade stage\",\n",
    "        \"sixth decade stage\",\n",
    "        \"seventh decade stage\",\n",
    "        \"eighth decade stage\",\n",
    "        \"ninth decade stage\",\n",
    "        \"60-79 year-old stage\",\n",
    "        \"80 year-old and over stage\",\n",
    "        \"90 year-old and over stage\",\n",
    "    ]\n",
    "\n",
    "    extra_ids = (\n",
    "        pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    ids.update(extra_ids)\n",
    "\n",
    "    ids.discard(\"unknown\")\n",
    "    ids.discard(\"nan\")\n",
    "    return sorted(ids)\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id via census_info/summary_cell_counts.\n",
    "    Avoids quoting issues with \"10x 3' v3\".\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(FILTERS.target_assay_labels))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in FILTERS.target_assay_labels if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for assay label(s): {missing}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def build_obs_value_filter(census, inventory_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Combined filters:\n",
    "      - is_primary_data\n",
    "      - normal disease\n",
    "      - male/female\n",
    "      - suspension_type cell\n",
    "      - assays: 10x v2/v3/multiome (via assay_ontology_term_id)\n",
    "      - age >= min_age_years (via development_stage_ontology_term_id allow-list)\n",
    "    \"\"\"\n",
    "    assay_map = lookup_assay_term_ids(census)\n",
    "    assay_term_ids = [assay_map[x] for x in FILTERS.target_assay_labels]\n",
    "    stage_ids = build_stage_ids_ge15(inventory_dir, FILTERS.min_age_years)\n",
    "\n",
    "    vf_parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{FILTERS.normal_disease_otid}'\",\n",
    "        f\"sex in {_format_in_list(FILTERS.target_sexes)}\",\n",
    "        f\"suspension_type == '{FILTERS.target_suspension_type}'\",\n",
    "        f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "        f\"development_stage_ontology_term_id in {_format_in_list(stage_ids)}\",\n",
    "    ]\n",
    "    return \" and \".join(vf_parts)\n",
    "\n",
    "\n",
    "def _suggest_columns(schema_names: Sequence[str], needle: str, k: int = 25) -> List[str]:\n",
    "    n = needle.lower()\n",
    "    hits = [c for c in schema_names if n in c.lower()]\n",
    "    return hits[:k]\n",
    "\n",
    "\n",
    "def build_export_columns(schema_names: Sequence[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Export ALL non-ontology columns (optionally), and guarantee required columns exist.\n",
    "    Drops any column containing 'ontology_term_id' from OUTPUT by default.\n",
    "    \"\"\"\n",
    "    required_cols = list(BASE_REQUIRED_EXPORT_COLS) + list(REQUIRED_METADATA_COLS)\n",
    "\n",
    "    missing = [c for c in required_cols if c not in schema_names]\n",
    "    if missing:\n",
    "        msg = \"Required export column(s) not found in exp.obs schema:\\n\"\n",
    "        for m in missing:\n",
    "            msg += f\"  - {m}\\n\"\n",
    "            msg += f\"    suggestions: {_suggest_columns(schema_names, m.split('_')[0])}\\n\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    cols: List[str]\n",
    "    if EXPORT.export_all_non_ontology:\n",
    "        cols = list(schema_names)\n",
    "        # Always drop soma_joinid from output unless you explicitly want it\n",
    "        cols = [c for c in cols if c != \"soma_joinid\"]\n",
    "        if EXPORT.drop_ontology_term_id_cols:\n",
    "            cols = [c for c in cols if \"ontology_term_id\" not in c]\n",
    "    else:\n",
    "        # Only export the minimal set (plus whatever you add later)\n",
    "        cols = list(dict.fromkeys(required_cols))  # preserve order, unique\n",
    "\n",
    "    # Ensure required columns included\n",
    "    for r in required_cols:\n",
    "        if r not in cols:\n",
    "            cols.append(r)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def export_obs_to_parquet(exp, value_filter: str, outpath: Path, column_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Stream obs rows and write to a single parquet.\n",
    "    \"\"\"\n",
    "    it = exp.obs.read(value_filter=value_filter, column_names=column_names)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "\n",
    "    try:\n",
    "        for tbl in it:\n",
    "            if isinstance(tbl, pa.RecordBatch):\n",
    "                tbl = pa.Table.from_batches([tbl])\n",
    "            elif not isinstance(tbl, pa.Table):\n",
    "                tbl = pa.Table.from_batches(tbl.to_batches())\n",
    "\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=\"zstd\")\n",
    "\n",
    "            writer.write_table(tbl)\n",
    "\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "            if batches % 10 == 0:\n",
    "                print(f\"[export] batches={batches:,} rows={rows:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    print(f\"Done. Wrote {rows:,} rows across {batches:,} batches to:\\n  {outpath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    inventory_dir = detect_inventory_dir(EXPORT.inventory_dir)\n",
    "    print(\"Using inventory CSV directory:\", inventory_dir)\n",
    "\n",
    "    outpath = resolve_outpath(EXPORT.outdir, EXPORT.outfile, EXPORT.overwrite)\n",
    "    print(\"Output file:\", outpath)\n",
    "\n",
    "    with cxc.open_soma(census_version=CENSUS.census_version) as census:\n",
    "        exp = census[\"census_data\"][CENSUS.organism]\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, inventory_dir)\n",
    "        print(\"\\nCombined OBS value_filter:\\n\", value_filter, \"\\n\")\n",
    "\n",
    "        schema_names = list(exp.obs.schema.names)\n",
    "        export_cols = build_export_columns(schema_names)\n",
    "\n",
    "        print(f\"Organ column (coarse): {EXPORT.organ_col}\")\n",
    "        print(f\"Will export {len(export_cols)} obs columns \"\n",
    "              f\"({'all non-ontology' if EXPORT.export_all_non_ontology else 'required only'}).\")\n",
    "\n",
    "        export_obs_to_parquet(exp, value_filter, outpath, export_cols)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23b5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Iterator, List, Optional, Sequence, Set\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class CensusConfig:\n",
    "    census_version: str = \"stable\"\n",
    "    organism: str = \"homo_sapiens\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class FiltersConfig:\n",
    "    normal_disease_otid: str = \"PATO:0000461\"  # normal\n",
    "    target_assay_labels: Sequence[str] = (\"10x 3' v2\", \"10x 3' v3\", \"10x multiome\")\n",
    "    target_sexes: Sequence[str] = (\"male\", \"female\")\n",
    "    target_suspension_type: str = \"cell\"\n",
    "    min_age_years: Optional[int] = 15  # set None to disable age filter\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class ExportConfig:\n",
    "    outdir: Path = Path(\"./census_exports\")\n",
    "    outfile: str = \"census_obs.parquet\"\n",
    "    overwrite: bool = True\n",
    "\n",
    "    # Inventory CSVs directory (leave None to auto-detect)\n",
    "    inventory_dir: Optional[Path] = None\n",
    "\n",
    "    # Organ/tissue/cell-type metadata\n",
    "    organ_col: str = \"tissue_general\"\n",
    "    tissue_cols: Sequence[str] = (\"tissue\", \"tissue_general\")\n",
    "    cell_type_col: str = \"cell_type\"\n",
    "\n",
    "    # Export columns behavior\n",
    "    export_all_columns: bool = False  # safer default than \"all non-ontology\"\n",
    "    drop_ontology_term_id_cols: bool = False  # for atlas: keep key IDs by default\n",
    "\n",
    "    # If dropping ontology IDs, allow-list the ones you still want (atlas-friendly)\n",
    "    keep_ontology_term_id_cols: Sequence[str] = (\n",
    "        \"cell_type_ontology_term_id\",\n",
    "        \"tissue_ontology_term_id\",\n",
    "        \"assay_ontology_term_id\",\n",
    "        \"disease_ontology_term_id\",\n",
    "        \"development_stage_ontology_term_id\",\n",
    "    )\n",
    "\n",
    "    # progress\n",
    "    log_every_batches: int = 10\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class CensusObsExportJob:\n",
    "    census: CensusConfig = CensusConfig()\n",
    "    filters: FiltersConfig = FiltersConfig()\n",
    "    export: ExportConfig = ExportConfig()\n",
    "\n",
    "    # minimal business columns you always want\n",
    "    base_required_export_cols: Sequence[str] = (\n",
    "        \"dataset_id\",\n",
    "        \"donor_id\",\n",
    "        \"self_reported_ethnicity\",\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Filesystem helpers\n",
    "# -----------------------------\n",
    "def ensure_writable_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path}\")\n",
    "\n",
    "\n",
    "def resolve_outpath(outdir: Path, outfile: str, overwrite: bool) -> Path:\n",
    "    ensure_writable_dir(outdir)\n",
    "    outpath = outdir / outfile\n",
    "    if outpath.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(outpath, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {outpath}\")\n",
    "            outpath.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and overwrite=False: {outpath}\")\n",
    "    return outpath\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Census filter helpers\n",
    "# -----------------------------\n",
    "def _format_in_list(values: Sequence[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def detect_inventory_dir(user_hint: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Locate folder containing:\n",
    "      - year_old_stage_pairs_counts.csv\n",
    "      - development_stage_pairs_counts.csv\n",
    "    \"\"\"\n",
    "    required = [\"year_old_stage_pairs_counts.csv\", \"development_stage_pairs_counts.csv\"]\n",
    "    candidates: List[Path] = []\n",
    "    if user_hint is not None:\n",
    "        candidates.append(user_hint)\n",
    "\n",
    "    candidates.extend(\n",
    "        [\n",
    "            Path(\"./dev_stage_inventory\"),\n",
    "            Path.home() / \"census_exports\" / \"dev_stage_inventory\",\n",
    "            Path(\"/mnt/data\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for d in candidates:\n",
    "        if all((d / f).exists() for f in required):\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find inventory CSVs. Set export.inventory_dir to the folder containing:\\n\"\n",
    "        \"  - year_old_stage_pairs_counts.csv\\n\"\n",
    "        \"  - development_stage_pairs_counts.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stage_ids_ge_min_age(inventory_dir: Path, min_age_years: int) -> List[str]:\n",
    "    year_pairs = pd.read_csv(inventory_dir / \"year_old_stage_pairs_counts.csv\")\n",
    "    pairs = pd.read_csv(inventory_dir / \"development_stage_pairs_counts.csv\")\n",
    "\n",
    "    year_pairs[\"age_years\"] = pd.to_numeric(year_pairs[\"age_years\"], errors=\"coerce\")\n",
    "\n",
    "    ids: Set[str] = set(\n",
    "        year_pairs.loc[year_pairs[\"age_years\"] >= min_age_years, \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    extra_labels = [\n",
    "        \"15-19 year-old\",\n",
    "        \"adult stage\",\n",
    "        \"young adult stage\",\n",
    "        \"prime adult stage\",\n",
    "        \"late adult stage\",\n",
    "        \"middle aged stage\",\n",
    "        \"third decade stage\",\n",
    "        \"fourth decade stage\",\n",
    "        \"fifth decade stage\",\n",
    "        \"sixth decade stage\",\n",
    "        \"seventh decade stage\",\n",
    "        \"eighth decade stage\",\n",
    "        \"ninth decade stage\",\n",
    "        \"60-79 year-old stage\",\n",
    "        \"80 year-old and over stage\",\n",
    "        \"90 year-old and over stage\",\n",
    "    ]\n",
    "    extra_ids = (\n",
    "        pairs.loc[pairs[\"development_stage\"].isin(extra_labels), \"development_stage_ontology_term_id\"]\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    ids.update(extra_ids)\n",
    "\n",
    "    ids.discard(\"unknown\")\n",
    "    ids.discard(\"nan\")\n",
    "    return sorted(ids)\n",
    "\n",
    "\n",
    "def lookup_assay_term_ids(census, assay_labels: Sequence[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map assay label -> assay_ontology_term_id via census_info/summary_cell_counts.\n",
    "    Avoids quoting issues with \"10x 3' v3\".\n",
    "    \"\"\"\n",
    "    scc = (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    assay_rows = scc[(scc[\"category\"] == \"assay\") & (scc[\"label\"].isin(assay_labels))][\n",
    "        [\"label\", \"ontology_term_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    found = dict(zip(assay_rows[\"label\"], assay_rows[\"ontology_term_id\"]))\n",
    "    missing = [x for x in assay_labels if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for assay label(s): {missing}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def build_obs_value_filter(\n",
    "    census,\n",
    "    filters: FiltersConfig,\n",
    "    inventory_dir: Optional[Path],\n",
    ") -> str:\n",
    "    assay_map = lookup_assay_term_ids(census, filters.target_assay_labels)\n",
    "    assay_term_ids = [assay_map[x] for x in filters.target_assay_labels]\n",
    "\n",
    "    vf_parts = [\n",
    "        \"is_primary_data == True\",\n",
    "        f\"disease_ontology_term_id == '{filters.normal_disease_otid}'\",\n",
    "        f\"sex in {_format_in_list(filters.target_sexes)}\",\n",
    "        f\"suspension_type == '{filters.target_suspension_type}'\",\n",
    "        f\"assay_ontology_term_id in {_format_in_list(assay_term_ids)}\",\n",
    "    ]\n",
    "\n",
    "    if filters.min_age_years is not None:\n",
    "        if inventory_dir is None:\n",
    "            raise ValueError(\"min_age_years is set, but inventory_dir is None.\")\n",
    "        stage_ids = build_stage_ids_ge_min_age(inventory_dir, filters.min_age_years)\n",
    "        vf_parts.append(f\"development_stage_ontology_term_id in {_format_in_list(stage_ids)}\")\n",
    "\n",
    "    return \" and \".join(vf_parts)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Column selection\n",
    "# -----------------------------\n",
    "def _suggest_columns(schema_names: Sequence[str], needle: str, k: int = 25) -> List[str]:\n",
    "    n = needle.lower()\n",
    "    hits = [c for c in schema_names if n in c.lower()]\n",
    "    return hits[:k]\n",
    "\n",
    "\n",
    "def build_export_columns(\n",
    "    schema_names: Sequence[str],\n",
    "    job: CensusObsExportJob,\n",
    ") -> List[str]:\n",
    "    exp_cfg = job.export\n",
    "\n",
    "    required_cols = list(job.base_required_export_cols) + [\n",
    "        exp_cfg.organ_col,\n",
    "        \"tissue\",\n",
    "        exp_cfg.cell_type_col,\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in required_cols if c not in schema_names]\n",
    "    if missing:\n",
    "        msg = \"Required export column(s) not found in exp.obs schema:\\n\"\n",
    "        for m in missing:\n",
    "            msg += f\"  - {m}\\n\"\n",
    "            msg += f\"    suggestions: {_suggest_columns(schema_names, m.split('_')[0])}\\n\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if exp_cfg.export_all_columns:\n",
    "        cols = [c for c in schema_names if c != \"soma_joinid\"]\n",
    "    else:\n",
    "        cols = list(dict.fromkeys(required_cols))\n",
    "\n",
    "    # Handle ontology term ids\n",
    "    if exp_cfg.drop_ontology_term_id_cols:\n",
    "        keep = set(exp_cfg.keep_ontology_term_id_cols)\n",
    "        cols = [c for c in cols if (\"ontology_term_id\" not in c) or (c in keep)]\n",
    "        # if we kept some, ensure they exist\n",
    "        for c in keep:\n",
    "            if c in schema_names and c not in cols:\n",
    "                cols.append(c)\n",
    "    else:\n",
    "        # Keep ontology IDs if they are already in cols (or if export_all_columns=True)\n",
    "        pass\n",
    "\n",
    "    # Ensure required columns included\n",
    "    for r in required_cols:\n",
    "        if r not in cols:\n",
    "            cols.append(r)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Public API: stream or write parquet\n",
    "# -----------------------------\n",
    "def iter_obs_tables(job: CensusObsExportJob) -> Iterator[pa.Table]:\n",
    "    \"\"\"\n",
    "    Stream obs as Arrow tables according to the job config.\n",
    "    This is the â€œno-Parquet-requiredâ€ path for downstream processing.\n",
    "    \"\"\"\n",
    "    exp_cfg = job.export\n",
    "    inventory_dir = None\n",
    "    if job.filters.min_age_years is not None:\n",
    "        inventory_dir = detect_inventory_dir(exp_cfg.inventory_dir)\n",
    "\n",
    "    with cxc.open_soma(census_version=job.census.census_version) as census:\n",
    "        exp = census[\"census_data\"][job.census.organism]\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, job.filters, inventory_dir)\n",
    "        schema_names = list(exp.obs.schema.names)\n",
    "        export_cols = build_export_columns(schema_names, job)\n",
    "\n",
    "        logger.info(\"OBS value_filter: %s\", value_filter)\n",
    "        logger.info(\"Exporting %d columns.\", len(export_cols))\n",
    "\n",
    "        it = exp.obs.read(value_filter=value_filter, column_names=export_cols)\n",
    "        for tbl in it:\n",
    "            if isinstance(tbl, pa.RecordBatch):\n",
    "                tbl = pa.Table.from_batches([tbl])\n",
    "            elif not isinstance(tbl, pa.Table):\n",
    "                tbl = pa.Table.from_batches(tbl.to_batches())\n",
    "            if tbl.num_rows:\n",
    "                yield tbl\n",
    "\n",
    "\n",
    "def export_obs_parquet(job: CensusObsExportJob) -> Path:\n",
    "    \"\"\"\n",
    "    Write streamed obs rows to a single parquet file.\n",
    "    \"\"\"\n",
    "    exp_cfg = job.export\n",
    "    outpath = resolve_outpath(exp_cfg.outdir, exp_cfg.outfile, exp_cfg.overwrite)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "\n",
    "    try:\n",
    "        for tbl in iter_obs_tables(job):\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=\"zstd\")\n",
    "            else:\n",
    "                # Defensive: enforce consistent schema if something shifts unexpectedly\n",
    "                if tbl.schema != writer.schema:\n",
    "                    tbl = tbl.cast(writer.schema)\n",
    "\n",
    "            writer.write_table(tbl)\n",
    "\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "            if exp_cfg.log_every_batches and (batches % exp_cfg.log_every_batches == 0):\n",
    "                logger.info(\"export progress: batches=%s rows=%s\", f\"{batches:,}\", f\"{rows:,}\")\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    logger.info(\"Done. Wrote %s rows across %s batches to %s\", f\"{rows:,}\", f\"{batches:,}\", outpath)\n",
    "    return outpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49830fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cellstrata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mcellstrata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcensus_obs_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CensusObsExportJob, ExportConfig, export_obs_parquet\n\u001b[32m      6\u001b[39m logging.basicConfig(level=logging.INFO)\n\u001b[32m      8\u001b[39m job = CensusObsExportJob(\n\u001b[32m      9\u001b[39m     export=ExportConfig(\n\u001b[32m     10\u001b[39m         outdir=Path(\u001b[33m\"\u001b[39m\u001b[33m/home/crizza/census_exports\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cellstrata'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import yaml\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "try:\n",
    "    import tiledbsoma as soma  # optional but recommended for TileDB context tuning\n",
    "except Exception:  # pragma: no cover\n",
    "    soma = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Schema helpers / constants\n",
    "# -----------------------------\n",
    "# These columns are shown in the official census query tutorial for obs keys. :contentReference[oaicite:2]{index=2}\n",
    "OBS_TERM_ID_COLS: Dict[str, str] = {\n",
    "    \"assay\": \"assay_ontology_term_id\",\n",
    "    \"cell_type\": \"cell_type_ontology_term_id\",\n",
    "    \"development_stage\": \"development_stage_ontology_term_id\",\n",
    "    \"disease\": \"disease_ontology_term_id\",\n",
    "    \"self_reported_ethnicity\": \"self_reported_ethnicity_ontology_term_id\",\n",
    "    \"sex\": \"sex_ontology_term_id\",\n",
    "    \"tissue\": \"tissue_ontology_term_id\",\n",
    "    \"tissue_general\": \"tissue_general_ontology_term_id\",\n",
    "}\n",
    "\n",
    "OBS_LABEL_COLS: Dict[str, str] = {\n",
    "    \"assay\": \"assay\",\n",
    "    \"cell_type\": \"cell_type\",\n",
    "    \"development_stage\": \"development_stage\",\n",
    "    \"disease\": \"disease\",\n",
    "    \"self_reported_ethnicity\": \"self_reported_ethnicity\",\n",
    "    \"sex\": \"sex\",\n",
    "    \"tissue\": \"tissue\",\n",
    "    \"tissue_general\": \"tissue_general\",\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config dataclasses\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class CensusTarget:\n",
    "    census_version: str = \"stable\"\n",
    "    organism: str = \"homo_sapiens\"\n",
    "    measurement: str = \"RNA\"  # used for get_anndata\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ObsFilters:\n",
    "    # Canonical filters\n",
    "    is_primary_data: Optional[bool] = True\n",
    "    suspension_type: Optional[str] = \"cell\"\n",
    "\n",
    "    # Prefer ontology term IDs where possible to avoid quoting/label instability\n",
    "    disease_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "    disease_labels: List[str] = field(default_factory=list)\n",
    "\n",
    "    sex_labels: List[str] = field(default_factory=list)\n",
    "    sex_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    assay_labels: List[str] = field(default_factory=list)\n",
    "    assay_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Tissue / organ\n",
    "    tissue_general_labels: List[str] = field(default_factory=list)          # \"organ\" (coarse)\n",
    "    tissue_general_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    tissue_labels: List[str] = field(default_factory=list)                  # more specific\n",
    "    tissue_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Cell type\n",
    "    cell_type_labels: List[str] = field(default_factory=list)\n",
    "    cell_type_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Development stage (optional)\n",
    "    development_stage_labels: List[str] = field(default_factory=list)\n",
    "    development_stage_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Any additional value_filter fragment (appended with \"and\")\n",
    "    extra_value_filter: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OutputSpec:\n",
    "    mode: Literal[\"pandas\", \"arrow\", \"anndata\", \"parquet\"] = \"pandas\"\n",
    "    outpath: Optional[str] = None\n",
    "    overwrite: bool = True\n",
    "    parquet_compression: str = \"zstd\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class QuerySpec:\n",
    "    target: CensusTarget = field(default_factory=CensusTarget)\n",
    "    obs_filters: ObsFilters = field(default_factory=ObsFilters)\n",
    "\n",
    "    # What to return / export\n",
    "    export_all_non_ontology_obs_columns: bool = False\n",
    "    obs_columns: List[str] = field(default_factory=list)  # if empty, use a sensible default\n",
    "\n",
    "    # Only used for mode=\"anndata\"\n",
    "    var_value_filter: Optional[str] = None\n",
    "    var_columns: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Optional TileDB config to mitigate S3 timeouts (see Output below)\n",
    "    tiledb_config: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    output: OutputSpec = field(default_factory=OutputSpec)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YAML loading\n",
    "# -----------------------------\n",
    "def load_query_spec_yaml(path: Union[str, Path]) -> QuerySpec:\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = yaml.safe_load(f)\n",
    "\n",
    "    # minimal manual parsing (keeps dependencies light)\n",
    "    target = CensusTarget(**raw.get(\"target\", {}))\n",
    "    obs_filters = ObsFilters(**raw.get(\"obs_filters\", {}))\n",
    "    output = OutputSpec(**raw.get(\"output\", {}))\n",
    "\n",
    "    return QuerySpec(\n",
    "        target=target,\n",
    "        obs_filters=obs_filters,\n",
    "        export_all_non_ontology_obs_columns=raw.get(\"export_all_non_ontology_obs_columns\", False),\n",
    "        obs_columns=raw.get(\"obs_columns\", []) or [],\n",
    "        var_value_filter=raw.get(\"var_value_filter\"),\n",
    "        var_columns=raw.get(\"var_columns\", []) or [],\n",
    "        tiledb_config=raw.get(\"tiledb_config\", {}) or {},\n",
    "        output=output,\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core helpers\n",
    "# -----------------------------\n",
    "def _format_in_list(values: Sequence[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def _get_scc_table(census) -> pd.DataFrame:\n",
    "    \"\"\"Load census_info.summary_cell_counts as a pandas table.\"\"\"\n",
    "    return (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_ontology_term_ids(\n",
    "    census,\n",
    "    category: str,\n",
    "    labels: Sequence[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Resolve label -> ontology_term_id for a given summary_cell_counts category.\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        return []\n",
    "    scc = _get_scc_table(census)\n",
    "    sub = scc[(scc[\"category\"] == category) & (scc[\"label\"].isin(list(labels)))]\n",
    "    found = dict(zip(sub[\"label\"], sub[\"ontology_term_id\"]))\n",
    "    missing = [x for x in labels if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for {category} label(s): {missing}\")\n",
    "    return [found[x] for x in labels]\n",
    "\n",
    "\n",
    "def choose_organ_column(obs_keys: Sequence[str]) -> str:\n",
    "    \"\"\"\n",
    "    In Census obs, the typical coarse â€œorganâ€ column is `tissue_general`.\n",
    "    If absent, fall back to `tissue`. :contentReference[oaicite:3]{index=3}\n",
    "    \"\"\"\n",
    "    if \"tissue_general\" in obs_keys:\n",
    "        return \"tissue_general\"\n",
    "    if \"tissue\" in obs_keys:\n",
    "        return \"tissue\"\n",
    "    raise ValueError(\"Neither tissue_general nor tissue found in obs schema keys.\")\n",
    "\n",
    "\n",
    "def build_obs_value_filter(census, obs_filters: ObsFilters) -> str:\n",
    "    \"\"\"\n",
    "    Build a Census-compatible obs value_filter string.\n",
    "    Prefers ontology-term-id columns when possible.\n",
    "    \"\"\"\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # is_primary_data / suspension_type (columns are present in obs keys) :contentReference[oaicite:4]{index=4}\n",
    "    if obs_filters.is_primary_data is not None:\n",
    "        parts.append(f\"is_primary_data == {bool(obs_filters.is_primary_data)}\")\n",
    "    if obs_filters.suspension_type:\n",
    "        parts.append(f\"suspension_type == '{obs_filters.suspension_type}'\")\n",
    "\n",
    "    # disease\n",
    "    disease_ids = list(obs_filters.disease_ontology_term_ids)\n",
    "    if not disease_ids and obs_filters.disease_labels:\n",
    "        disease_ids = resolve_ontology_term_ids(census, \"disease\", obs_filters.disease_labels)\n",
    "    if disease_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['disease']} in {_format_in_list(disease_ids)}\")\n",
    "\n",
    "    # sex\n",
    "    sex_ids = list(obs_filters.sex_ontology_term_ids)\n",
    "    if not sex_ids and obs_filters.sex_labels:\n",
    "        # Either filter by label, or resolve to ontology ids. Prefer ids.\n",
    "        sex_ids = resolve_ontology_term_ids(census, \"sex\", obs_filters.sex_labels)\n",
    "    if sex_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['sex']} in {_format_in_list(sex_ids)}\")\n",
    "\n",
    "    # assay\n",
    "    assay_ids = list(obs_filters.assay_ontology_term_ids)\n",
    "    if not assay_ids and obs_filters.assay_labels:\n",
    "        assay_ids = resolve_ontology_term_ids(census, \"assay\", obs_filters.assay_labels)\n",
    "    if assay_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['assay']} in {_format_in_list(assay_ids)}\")\n",
    "\n",
    "    # tissue_general (organ/coarse)\n",
    "    tg_ids = list(obs_filters.tissue_general_ontology_term_ids)\n",
    "    if not tg_ids and obs_filters.tissue_general_labels:\n",
    "        tg_ids = resolve_ontology_term_ids(census, \"tissue_general\", obs_filters.tissue_general_labels)\n",
    "    if tg_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['tissue_general']} in {_format_in_list(tg_ids)}\")\n",
    "\n",
    "    # tissue (fine)\n",
    "    tissue_ids = list(obs_filters.tissue_ontology_term_ids)\n",
    "    if not tissue_ids and obs_filters.tissue_labels:\n",
    "        tissue_ids = resolve_ontology_term_ids(census, \"tissue\", obs_filters.tissue_labels)\n",
    "    if tissue_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['tissue']} in {_format_in_list(tissue_ids)}\")\n",
    "\n",
    "    # cell_type\n",
    "    ct_ids = list(obs_filters.cell_type_ontology_term_ids)\n",
    "    if not ct_ids and obs_filters.cell_type_labels:\n",
    "        ct_ids = resolve_ontology_term_ids(census, \"cell_type\", obs_filters.cell_type_labels)\n",
    "    if ct_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['cell_type']} in {_format_in_list(ct_ids)}\")\n",
    "\n",
    "    # development_stage (optional)\n",
    "    ds_ids = list(obs_filters.development_stage_ontology_term_ids)\n",
    "    if not ds_ids and obs_filters.development_stage_labels:\n",
    "        ds_ids = resolve_ontology_term_ids(census, \"development_stage\", obs_filters.development_stage_labels)\n",
    "    if ds_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['development_stage']} in {_format_in_list(ds_ids)}\")\n",
    "\n",
    "    # extra fragment\n",
    "    if obs_filters.extra_value_filter:\n",
    "        parts.append(f\"({obs_filters.extra_value_filter})\")\n",
    "\n",
    "    return \" and \".join(parts) if parts else \"\"\n",
    "\n",
    "\n",
    "def _default_obs_columns() -> List[str]:\n",
    "    # Includes tissue + coarse organ + cell type labels; excludes *_ontology_term_id by default.\n",
    "    return [\n",
    "        \"dataset_id\",\n",
    "        \"donor_id\",\n",
    "        \"assay\",\n",
    "        \"cell_type\",\n",
    "        \"tissue\",\n",
    "        \"tissue_general\",\n",
    "        \"disease\",\n",
    "        \"development_stage\",\n",
    "        \"sex\",\n",
    "        \"self_reported_ethnicity\",\n",
    "        \"is_primary_data\",\n",
    "        \"suspension_type\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_obs_export_columns(obs_keys: Sequence[str], export_all_non_ontology: bool, user_cols: Sequence[str]) -> List[str]:\n",
    "    keys = list(obs_keys)\n",
    "\n",
    "    if export_all_non_ontology:\n",
    "        cols = [c for c in keys if (c != \"soma_joinid\" and \"ontology_term_id\" not in c)]\n",
    "        return cols\n",
    "\n",
    "    cols = list(user_cols) if user_cols else _default_obs_columns()\n",
    "    missing = [c for c in cols if c not in keys]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested obs columns not present in schema: {missing}\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# IO / execution\n",
    "# -----------------------------\n",
    "def _ensure_writable_parent(path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path.parent, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path.parent}\")\n",
    "\n",
    "\n",
    "def _resolve_outpath(outpath: str, overwrite: bool) -> Path:\n",
    "    p = Path(outpath)\n",
    "    _ensure_writable_parent(p)\n",
    "    if p.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(p, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {p}\")\n",
    "            p.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and overwrite=False: {p}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def _make_soma_context(tiledb_config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Optional: create a SOMATileDBContext to tune S3 timeouts/retries/parallel ops.\n",
    "    \"\"\"\n",
    "    if not tiledb_config:\n",
    "        return None\n",
    "    if soma is None:\n",
    "        raise RuntimeError(\"tiledbsoma is not importable, but tiledb_config was provided.\")\n",
    "    return soma.SOMATileDBContext(tiledb_config=tiledb_config)\n",
    "\n",
    "\n",
    "def stream_obs_tables(\n",
    "    exp,\n",
    "    value_filter: str,\n",
    "    column_names: Sequence[str],\n",
    ") -> Iterator[pa.Table]:\n",
    "    \"\"\"\n",
    "    Stream obs results as Arrow Tables.\n",
    "    \"\"\"\n",
    "    it = exp.obs.read(value_filter=value_filter, column_names=list(column_names))\n",
    "    for tbl in it:\n",
    "        if isinstance(tbl, pa.RecordBatch):\n",
    "            yield pa.Table.from_batches([tbl])\n",
    "        elif isinstance(tbl, pa.Table):\n",
    "            yield tbl\n",
    "        else:\n",
    "            yield pa.Table.from_batches(tbl.to_batches())\n",
    "\n",
    "\n",
    "def write_parquet_stream(\n",
    "    tables: Iterable[pa.Table],\n",
    "    outpath: Path,\n",
    "    compression: str = \"zstd\",\n",
    ") -> Tuple[int, int]:\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "    try:\n",
    "        for tbl in tables:\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=compression)\n",
    "            writer.write_table(tbl)\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    return rows, batches\n",
    "\n",
    "\n",
    "def run_query(spec: QuerySpec) -> Union[pd.DataFrame, pa.Table, Any, Path]:\n",
    "    \"\"\"\n",
    "    Execute the query.\n",
    "    Returns:\n",
    "      - pandas DataFrame (mode=pandas)\n",
    "      - Arrow Table (mode=arrow; concatenated, so only for moderate result sizes)\n",
    "      - AnnData (mode=anndata) via cellxgene_census.get_anndata :contentReference[oaicite:5]{index=5}\n",
    "      - Path to parquet (mode=parquet)\n",
    "    \"\"\"\n",
    "    context = _make_soma_context(spec.tiledb_config)\n",
    "    with cxc.open_soma(census_version=spec.target.census_version, context=context) as census:\n",
    "        exp = census[\"census_data\"][spec.target.organism]\n",
    "\n",
    "        obs_keys = list(exp.obs.keys())\n",
    "        organ_col = choose_organ_column(obs_keys)  # tissue_general preferred :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, spec.obs_filters)\n",
    "\n",
    "        # Build export columns; ensure organ/tissue + cell_type available\n",
    "        export_cols = build_obs_export_columns(\n",
    "            obs_keys=obs_keys,\n",
    "            export_all_non_ontology=spec.export_all_non_ontology_obs_columns,\n",
    "            user_cols=spec.obs_columns,\n",
    "        )\n",
    "        # Ensure core metadata are present\n",
    "        for required in (\"cell_type\", \"tissue\", organ_col):\n",
    "            if required in obs_keys and required not in export_cols:\n",
    "                export_cols.append(required)\n",
    "\n",
    "        mode = spec.output.mode\n",
    "\n",
    "        if mode == \"anndata\":\n",
    "            # get_anndata is explicitly documented as the convenient API for expression+metadata slices. :contentReference[oaicite:7]{index=7}\n",
    "            column_names = {\"obs\": export_cols}\n",
    "            if spec.var_columns:\n",
    "                column_names[\"var\"] = spec.var_columns\n",
    "\n",
    "            adata = cxc.get_anndata(\n",
    "                census=census,\n",
    "                organism=\"Homo sapiens\" if spec.target.organism == \"homo_sapiens\" else \"Mus musculus\",\n",
    "                obs_value_filter=value_filter or None,\n",
    "                var_value_filter=spec.var_value_filter or None,\n",
    "                column_names=column_names,\n",
    "            )\n",
    "            return adata\n",
    "\n",
    "        if mode == \"pandas\":\n",
    "            df = exp.obs.read(value_filter=value_filter or None, column_names=export_cols).concat().to_pandas()\n",
    "            return df\n",
    "\n",
    "        if mode == \"arrow\":\n",
    "            tables = list(stream_obs_tables(exp, value_filter=value_filter, column_names=export_cols))\n",
    "            if not tables:\n",
    "                return pa.table({c: [] for c in export_cols})\n",
    "            return pa.concat_tables(tables, promote=True)\n",
    "\n",
    "        if mode == \"parquet\":\n",
    "            if not spec.output.outpath:\n",
    "                raise ValueError(\"output.outpath is required for mode='parquet'\")\n",
    "            outpath = _resolve_outpath(spec.output.outpath, spec.output.overwrite)\n",
    "\n",
    "            tables = stream_obs_tables(exp, value_filter=value_filter, column_names=export_cols)\n",
    "            rows, batches = write_parquet_stream(tables, outpath, compression=spec.output.parquet_compression)\n",
    "            # Optionally you could log rows/batches here; returning the path keeps this a clean module API.\n",
    "            return outpath\n",
    "\n",
    "        raise ValueError(f\"Unknown output mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfce6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import yaml\n",
    "\n",
    "import cellxgene_census as cxc\n",
    "\n",
    "try:\n",
    "    import tiledbsoma as soma  # optional but recommended for TileDB context tuning\n",
    "except Exception:  # pragma: no cover\n",
    "    soma = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Schema helpers / constants\n",
    "# -----------------------------\n",
    "# These columns are shown in the official census query tutorial for obs keys. :contentReference[oaicite:2]{index=2}\n",
    "OBS_TERM_ID_COLS: Dict[str, str] = {\n",
    "    \"assay\": \"assay_ontology_term_id\",\n",
    "    \"cell_type\": \"cell_type_ontology_term_id\",\n",
    "    \"development_stage\": \"development_stage_ontology_term_id\",\n",
    "    \"disease\": \"disease_ontology_term_id\",\n",
    "    \"self_reported_ethnicity\": \"self_reported_ethnicity_ontology_term_id\",\n",
    "    \"sex\": \"sex_ontology_term_id\",\n",
    "    \"tissue\": \"tissue_ontology_term_id\",\n",
    "    \"tissue_general\": \"tissue_general_ontology_term_id\",\n",
    "}\n",
    "\n",
    "OBS_LABEL_COLS: Dict[str, str] = {\n",
    "    \"assay\": \"assay\",\n",
    "    \"cell_type\": \"cell_type\",\n",
    "    \"development_stage\": \"development_stage\",\n",
    "    \"disease\": \"disease\",\n",
    "    \"self_reported_ethnicity\": \"self_reported_ethnicity\",\n",
    "    \"sex\": \"sex\",\n",
    "    \"tissue\": \"tissue\",\n",
    "    \"tissue_general\": \"tissue_general\",\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config dataclasses\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class CensusTarget:\n",
    "    census_version: str = \"stable\"\n",
    "    organism: str = \"homo_sapiens\"\n",
    "    measurement: str = \"RNA\"  # used for get_anndata\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ObsFilters:\n",
    "    # Canonical filters\n",
    "    is_primary_data: Optional[bool] = True\n",
    "    suspension_type: Optional[str] = \"cell\"\n",
    "\n",
    "    # Prefer ontology term IDs where possible to avoid quoting/label instability\n",
    "    disease_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "    disease_labels: List[str] = field(default_factory=list)\n",
    "\n",
    "    sex_labels: List[str] = field(default_factory=list)\n",
    "    sex_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    assay_labels: List[str] = field(default_factory=list)\n",
    "    assay_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Tissue / organ\n",
    "    tissue_general_labels: List[str] = field(default_factory=list)          # \"organ\" (coarse)\n",
    "    tissue_general_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    tissue_labels: List[str] = field(default_factory=list)                  # more specific\n",
    "    tissue_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Cell type\n",
    "    cell_type_labels: List[str] = field(default_factory=list)\n",
    "    cell_type_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Development stage (optional)\n",
    "    development_stage_labels: List[str] = field(default_factory=list)\n",
    "    development_stage_ontology_term_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Any additional value_filter fragment (appended with \"and\")\n",
    "    extra_value_filter: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OutputSpec:\n",
    "    mode: Literal[\"pandas\", \"arrow\", \"anndata\", \"parquet\"] = \"pandas\"\n",
    "    outpath: Optional[str] = None\n",
    "    overwrite: bool = True\n",
    "    parquet_compression: str = \"zstd\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class QuerySpec:\n",
    "    target: CensusTarget = field(default_factory=CensusTarget)\n",
    "    obs_filters: ObsFilters = field(default_factory=ObsFilters)\n",
    "\n",
    "    # What to return / export\n",
    "    export_all_non_ontology_obs_columns: bool = False\n",
    "    obs_columns: List[str] = field(default_factory=list)  # if empty, use a sensible default\n",
    "\n",
    "    # Only used for mode=\"anndata\"\n",
    "    var_value_filter: Optional[str] = None\n",
    "    var_columns: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Optional TileDB config to mitigate S3 timeouts (see Output below)\n",
    "    tiledb_config: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    output: OutputSpec = field(default_factory=OutputSpec)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YAML loading\n",
    "# -----------------------------\n",
    "def load_query_spec_yaml(path: Union[str, Path]) -> QuerySpec:\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = yaml.safe_load(f)\n",
    "\n",
    "    # minimal manual parsing (keeps dependencies light)\n",
    "    target = CensusTarget(**raw.get(\"target\", {}))\n",
    "    obs_filters = ObsFilters(**raw.get(\"obs_filters\", {}))\n",
    "    output = OutputSpec(**raw.get(\"output\", {}))\n",
    "\n",
    "    return QuerySpec(\n",
    "        target=target,\n",
    "        obs_filters=obs_filters,\n",
    "        export_all_non_ontology_obs_columns=raw.get(\"export_all_non_ontology_obs_columns\", False),\n",
    "        obs_columns=raw.get(\"obs_columns\", []) or [],\n",
    "        var_value_filter=raw.get(\"var_value_filter\"),\n",
    "        var_columns=raw.get(\"var_columns\", []) or [],\n",
    "        tiledb_config=raw.get(\"tiledb_config\", {}) or {},\n",
    "        output=output,\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core helpers\n",
    "# -----------------------------\n",
    "def _format_in_list(values: Sequence[str]) -> str:\n",
    "    \"\"\"SOMA value_filter list literal: ['a','b'] with minimal escaping.\"\"\"\n",
    "    esc = [v.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\") for v in values]\n",
    "    return \"[\" + \", \".join([f\"'{v}'\" for v in esc]) + \"]\"\n",
    "\n",
    "\n",
    "def _get_scc_table(census) -> pd.DataFrame:\n",
    "    \"\"\"Load census_info.summary_cell_counts as a pandas table.\"\"\"\n",
    "    return (\n",
    "        census[\"census_info\"][\"summary_cell_counts\"]\n",
    "        .read(column_names=[\"category\", \"label\", \"ontology_term_id\"])\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_ontology_term_ids(\n",
    "    census,\n",
    "    category: str,\n",
    "    labels: Sequence[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Resolve label -> ontology_term_id for a given summary_cell_counts category.\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        return []\n",
    "    scc = _get_scc_table(census)\n",
    "    sub = scc[(scc[\"category\"] == category) & (scc[\"label\"].isin(list(labels)))]\n",
    "    found = dict(zip(sub[\"label\"], sub[\"ontology_term_id\"]))\n",
    "    missing = [x for x in labels if x not in found]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Could not resolve ontology_term_id for {category} label(s): {missing}\")\n",
    "    return [found[x] for x in labels]\n",
    "\n",
    "\n",
    "def choose_organ_column(obs_keys: Sequence[str]) -> str:\n",
    "    \"\"\"\n",
    "    In Census obs, the typical coarse â€œorganâ€ column is `tissue_general`.\n",
    "    If absent, fall back to `tissue`. :contentReference[oaicite:3]{index=3}\n",
    "    \"\"\"\n",
    "    if \"tissue_general\" in obs_keys:\n",
    "        return \"tissue_general\"\n",
    "    if \"tissue\" in obs_keys:\n",
    "        return \"tissue\"\n",
    "    raise ValueError(\"Neither tissue_general nor tissue found in obs schema keys.\")\n",
    "\n",
    "\n",
    "def build_obs_value_filter(census, obs_filters: ObsFilters) -> str:\n",
    "    \"\"\"\n",
    "    Build a Census-compatible obs value_filter string.\n",
    "    Prefers ontology-term-id columns when possible.\n",
    "    \"\"\"\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # is_primary_data / suspension_type (columns are present in obs keys) :contentReference[oaicite:4]{index=4}\n",
    "    if obs_filters.is_primary_data is not None:\n",
    "        parts.append(f\"is_primary_data == {bool(obs_filters.is_primary_data)}\")\n",
    "    if obs_filters.suspension_type:\n",
    "        parts.append(f\"suspension_type == '{obs_filters.suspension_type}'\")\n",
    "\n",
    "    # disease\n",
    "    disease_ids = list(obs_filters.disease_ontology_term_ids)\n",
    "    if not disease_ids and obs_filters.disease_labels:\n",
    "        disease_ids = resolve_ontology_term_ids(census, \"disease\", obs_filters.disease_labels)\n",
    "    if disease_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['disease']} in {_format_in_list(disease_ids)}\")\n",
    "\n",
    "    # sex\n",
    "    sex_ids = list(obs_filters.sex_ontology_term_ids)\n",
    "    if not sex_ids and obs_filters.sex_labels:\n",
    "        # Either filter by label, or resolve to ontology ids. Prefer ids.\n",
    "        sex_ids = resolve_ontology_term_ids(census, \"sex\", obs_filters.sex_labels)\n",
    "    if sex_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['sex']} in {_format_in_list(sex_ids)}\")\n",
    "\n",
    "    # assay\n",
    "    assay_ids = list(obs_filters.assay_ontology_term_ids)\n",
    "    if not assay_ids and obs_filters.assay_labels:\n",
    "        assay_ids = resolve_ontology_term_ids(census, \"assay\", obs_filters.assay_labels)\n",
    "    if assay_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['assay']} in {_format_in_list(assay_ids)}\")\n",
    "\n",
    "    # tissue_general (organ/coarse)\n",
    "    tg_ids = list(obs_filters.tissue_general_ontology_term_ids)\n",
    "    if not tg_ids and obs_filters.tissue_general_labels:\n",
    "        tg_ids = resolve_ontology_term_ids(census, \"tissue_general\", obs_filters.tissue_general_labels)\n",
    "    if tg_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['tissue_general']} in {_format_in_list(tg_ids)}\")\n",
    "\n",
    "    # tissue (fine)\n",
    "    tissue_ids = list(obs_filters.tissue_ontology_term_ids)\n",
    "    if not tissue_ids and obs_filters.tissue_labels:\n",
    "        tissue_ids = resolve_ontology_term_ids(census, \"tissue\", obs_filters.tissue_labels)\n",
    "    if tissue_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['tissue']} in {_format_in_list(tissue_ids)}\")\n",
    "\n",
    "    # cell_type\n",
    "    ct_ids = list(obs_filters.cell_type_ontology_term_ids)\n",
    "    if not ct_ids and obs_filters.cell_type_labels:\n",
    "        ct_ids = resolve_ontology_term_ids(census, \"cell_type\", obs_filters.cell_type_labels)\n",
    "    if ct_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['cell_type']} in {_format_in_list(ct_ids)}\")\n",
    "\n",
    "    # development_stage (optional)\n",
    "    ds_ids = list(obs_filters.development_stage_ontology_term_ids)\n",
    "    if not ds_ids and obs_filters.development_stage_labels:\n",
    "        ds_ids = resolve_ontology_term_ids(census, \"development_stage\", obs_filters.development_stage_labels)\n",
    "    if ds_ids:\n",
    "        parts.append(f\"{OBS_TERM_ID_COLS['development_stage']} in {_format_in_list(ds_ids)}\")\n",
    "\n",
    "    # extra fragment\n",
    "    if obs_filters.extra_value_filter:\n",
    "        parts.append(f\"({obs_filters.extra_value_filter})\")\n",
    "\n",
    "    return \" and \".join(parts) if parts else \"\"\n",
    "\n",
    "\n",
    "def _default_obs_columns() -> List[str]:\n",
    "    # Includes tissue + coarse organ + cell type labels; excludes *_ontology_term_id by default.\n",
    "    return [\n",
    "        \"dataset_id\",\n",
    "        \"donor_id\",\n",
    "        \"assay\",\n",
    "        \"cell_type\",\n",
    "        \"tissue\",\n",
    "        \"tissue_general\",\n",
    "        \"disease\",\n",
    "        \"development_stage\",\n",
    "        \"sex\",\n",
    "        \"self_reported_ethnicity\",\n",
    "        \"is_primary_data\",\n",
    "        \"suspension_type\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_obs_export_columns(obs_keys: Sequence[str], export_all_non_ontology: bool, user_cols: Sequence[str]) -> List[str]:\n",
    "    keys = list(obs_keys)\n",
    "\n",
    "    if export_all_non_ontology:\n",
    "        cols = [c for c in keys if (c != \"soma_joinid\" and \"ontology_term_id\" not in c)]\n",
    "        return cols\n",
    "\n",
    "    cols = list(user_cols) if user_cols else _default_obs_columns()\n",
    "    missing = [c for c in cols if c not in keys]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested obs columns not present in schema: {missing}\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# IO / execution\n",
    "# -----------------------------\n",
    "def _ensure_writable_parent(path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not os.access(path.parent, os.W_OK):\n",
    "        raise PermissionError(f\"Output directory is not writable: {path.parent}\")\n",
    "\n",
    "\n",
    "def _resolve_outpath(outpath: str, overwrite: bool) -> Path:\n",
    "    p = Path(outpath)\n",
    "    _ensure_writable_parent(p)\n",
    "    if p.exists():\n",
    "        if overwrite:\n",
    "            if not os.access(p, os.W_OK):\n",
    "                raise PermissionError(f\"Cannot overwrite existing file: {p}\")\n",
    "            p.unlink()\n",
    "        else:\n",
    "            raise FileExistsError(f\"Output exists and overwrite=False: {p}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def _make_soma_context(tiledb_config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Optional: create a SOMATileDBContext to tune S3 timeouts/retries/parallel ops.\n",
    "    \"\"\"\n",
    "    if not tiledb_config:\n",
    "        return None\n",
    "    if soma is None:\n",
    "        raise RuntimeError(\"tiledbsoma is not importable, but tiledb_config was provided.\")\n",
    "    return soma.SOMATileDBContext(tiledb_config=tiledb_config)\n",
    "\n",
    "\n",
    "def stream_obs_tables(\n",
    "    exp,\n",
    "    value_filter: str,\n",
    "    column_names: Sequence[str],\n",
    ") -> Iterator[pa.Table]:\n",
    "    \"\"\"\n",
    "    Stream obs results as Arrow Tables.\n",
    "    \"\"\"\n",
    "    it = exp.obs.read(value_filter=value_filter, column_names=list(column_names))\n",
    "    for tbl in it:\n",
    "        if isinstance(tbl, pa.RecordBatch):\n",
    "            yield pa.Table.from_batches([tbl])\n",
    "        elif isinstance(tbl, pa.Table):\n",
    "            yield tbl\n",
    "        else:\n",
    "            yield pa.Table.from_batches(tbl.to_batches())\n",
    "\n",
    "\n",
    "def write_parquet_stream(\n",
    "    tables: Iterable[pa.Table],\n",
    "    outpath: Path,\n",
    "    compression: str = \"zstd\",\n",
    ") -> Tuple[int, int]:\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    rows = 0\n",
    "    batches = 0\n",
    "    try:\n",
    "        for tbl in tables:\n",
    "            if tbl.num_rows == 0:\n",
    "                continue\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(outpath), tbl.schema, compression=compression)\n",
    "            writer.write_table(tbl)\n",
    "            rows += tbl.num_rows\n",
    "            batches += 1\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    return rows, batches\n",
    "\n",
    "\n",
    "def run_query(spec: QuerySpec) -> Union[pd.DataFrame, pa.Table, Any, Path]:\n",
    "    \"\"\"\n",
    "    Execute the query.\n",
    "    Returns:\n",
    "      - pandas DataFrame (mode=pandas)\n",
    "      - Arrow Table (mode=arrow; concatenated, so only for moderate result sizes)\n",
    "      - AnnData (mode=anndata) via cellxgene_census.get_anndata :contentReference[oaicite:5]{index=5}\n",
    "      - Path to parquet (mode=parquet)\n",
    "    \"\"\"\n",
    "    context = _make_soma_context(spec.tiledb_config)\n",
    "    with cxc.open_soma(census_version=spec.target.census_version, context=context) as census:\n",
    "        exp = census[\"census_data\"][spec.target.organism]\n",
    "\n",
    "        obs_keys = list(exp.obs.keys())\n",
    "        organ_col = choose_organ_column(obs_keys)  # tissue_general preferred :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "        value_filter = build_obs_value_filter(census, spec.obs_filters)\n",
    "\n",
    "        # Build export columns; ensure organ/tissue + cell_type available\n",
    "        export_cols = build_obs_export_columns(\n",
    "            obs_keys=obs_keys,\n",
    "            export_all_non_ontology=spec.export_all_non_ontology_obs_columns,\n",
    "            user_cols=spec.obs_columns,\n",
    "        )\n",
    "        # Ensure core metadata are present\n",
    "        for required in (\"cell_type\", \"tissue\", organ_col):\n",
    "            if required in obs_keys and required not in export_cols:\n",
    "                export_cols.append(required)\n",
    "\n",
    "        mode = spec.output.mode\n",
    "\n",
    "        if mode == \"anndata\":\n",
    "            # get_anndata is explicitly documented as the convenient API for expression+metadata slices. :contentReference[oaicite:7]{index=7}\n",
    "            column_names = {\"obs\": export_cols}\n",
    "            if spec.var_columns:\n",
    "                column_names[\"var\"] = spec.var_columns\n",
    "\n",
    "            adata = cxc.get_anndata(\n",
    "                census=census,\n",
    "                organism=\"Homo sapiens\" if spec.target.organism == \"homo_sapiens\" else \"Mus musculus\",\n",
    "                obs_value_filter=value_filter or None,\n",
    "                var_value_filter=spec.var_value_filter or None,\n",
    "                column_names=column_names,\n",
    "            )\n",
    "            return adata\n",
    "\n",
    "        if mode == \"pandas\":\n",
    "            df = exp.obs.read(value_filter=value_filter or None, column_names=export_cols).concat().to_pandas()\n",
    "            return df\n",
    "\n",
    "        if mode == \"arrow\":\n",
    "            tables = list(stream_obs_tables(exp, value_filter=value_filter, column_names=export_cols))\n",
    "            if not tables:\n",
    "                return pa.table({c: [] for c in export_cols})\n",
    "            return pa.concat_tables(tables, promote=True)\n",
    "\n",
    "        if mode == \"parquet\":\n",
    "            if not spec.output.outpath:\n",
    "                raise ValueError(\"output.outpath is required for mode='parquet'\")\n",
    "            outpath = _resolve_outpath(spec.output.outpath, spec.output.overwrite)\n",
    "\n",
    "            tables = stream_obs_tables(exp, value_filter=value_filter, column_names=export_cols)\n",
    "            rows, batches = write_parquet_stream(tables, outpath, compression=spec.output.parquet_compression)\n",
    "            # Optionally you could log rows/batches here; returning the path keeps this a clean module API.\n",
    "            return outpath\n",
    "\n",
    "        raise ValueError(f\"Unknown output mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467a736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
